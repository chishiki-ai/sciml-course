{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fStJ5jZC1T_t"
   },
   "source": [
    "# DeepONet: Operator Learning with Neural Networks\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the leap from function approximation to operator learning\n",
    "- Master the Universal Approximation Theorem for Operators\n",
    "- Learn the DeepONet architecture: Branch and Trunk networks\n",
    "- Implement operator learning for the derivative operator\n",
    "- Apply DeepONet to the 1D nonlinear Darcy problem\n",
    "\n",
    "\n",
    "\n",
    "**Slides:** [![View PDF](https://img.shields.io/badge/View-PDF-red?style=flat-square&logo=googledocs&logoColor=white)](https://github.com/chishiki-ai/sciml/raw/main/docs/02-deeponet/deeponet-slides.pdf)\n",
    "---\n",
    "\n",
    "## The Fundamental Question\n",
    "\n",
    "We've learned how neural networks can approximate **functions**: $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$.\n",
    "\n",
    "**But what if we want to learn mappings between infinite-dimensional function spaces?**\n",
    "\n",
    "Enter **operators**: mappings that take functions as input and produce functions as output.\n",
    "\n",
    "$$\\mathcal{G}: \\mathcal{A} \\rightarrow \\mathcal{U}$$\n",
    "\n",
    "where $\\mathcal{A}$ and $\\mathcal{U}$ are function spaces.\n",
    "\n",
    "**Examples of operators:**\n",
    "\n",
    "-   **Derivative operator:** $\\mathcal{G}u = \\frac{du}{dx}$\n",
    "-   **Integration operator:** $\\mathcal{G}f = \\int_0^x f(t) dt$\n",
    "-   **PDE solution operator:** Given boundary conditions or source terms, map to the PDE solution.\n",
    "\n",
    "---\n",
    "\n",
    "## The Challenge with Traditional Neural Networks\n",
    "\n",
    "Standard neural networks learn point-wise mappings: $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$. But operators map functions to functions. How do we represent infinite-dimensional functions with finite data?\n",
    "\n",
    "**Traditional approach limitations:**\n",
    "\n",
    "-   **Fixed discretization:** Networks trained on specific grids can't generalize to different resolutions.\n",
    "-   **Curse of dimensionality:** High-dimensional function spaces are computationally intractable.\n",
    "-   **No theoretical foundation:** No guarantee that standard networks can approximate operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJ5CGv-v1T_x",
    "outputId": "d7d5a763-2f12-4121-e079-f0c03c34d85b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style and random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device selection\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcy3muMJ1T_z"
   },
   "source": [
    "## From Scalars to Functions - The Conceptual Leap\n",
    "\n",
    "Let's start with something familiar and build our intuition.\n",
    "\n",
    "### Traditional Function Approximation\n",
    "\n",
    "We know neural networks can learn mappings like:\n",
    "- **Input:** A number $x = 2.5$\n",
    "- **Output:** A number $f(x) = x^2 = 6.25$\n",
    "\n",
    "This is **point-wise mapping**: each input point maps to an output point.\n",
    "\n",
    "### Operator Learning: The Next Level\n",
    "\n",
    "Now imagine:\n",
    "- **Input:** An entire function $u(x) = \\sin(x)$\n",
    "- **Output:** Another entire function $\\mathcal{G}[u](x) = \\cos(x)$ (the derivative!)\n",
    "\n",
    "This is **function-to-function mapping**: each input function maps to an output function.\n",
    "\n",
    "**Examples of operators we encounter in science:**\n",
    "1. **Derivative operator:** $\\mathcal{D}[u] = \\frac{du}{dx}$\n",
    "2. **Integration operator:** $\\mathcal{I}[f] = \\int_0^x f(t) dt$\n",
    "3. **PDE solution operator:** Given source $f$, return solution $u$ of $\\nabla^2 u = f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4cwW_9x1T_0",
    "outputId": "c8286e29-5741-4f61-d0ee-764a87b99589"
   },
   "outputs": [],
   "source": [
    "def demonstrate_function_approximation():\n",
    "    \"\"\"Show traditional function approximation with clearer visualization\"\"\"\n",
    "\n",
    "    # Example: Learn f(x) = x² + sin(x)\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = x**2 + np.sin(x)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot 1: Traditional function plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(x, y, 'b-', linewidth=2, label='f(x) = x² + sin(x)')\n",
    "    plt.title('Traditional Function Approximation\\nf: R → R')\n",
    "    plt.xlabel('Input x')\n",
    "    plt.ylabel('Output f(x)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot 2: Point-wise mapping visualization (improved)\n",
    "    plt.subplot(1, 3, 2)\n",
    "\n",
    "    # Show specific input-output pairs\n",
    "    sample_x = np.array([-1.0, 0.0, 1.0, 2.0])\n",
    "    sample_y = sample_x**2 + np.sin(sample_x)\n",
    "\n",
    "    # Create a clearer mapping visualization\n",
    "    input_positions = np.arange(len(sample_x))\n",
    "    output_positions = np.arange(len(sample_x)) + 0.5\n",
    "\n",
    "    # Plot input values\n",
    "    for i, (xi, yi) in enumerate(zip(sample_x, sample_y)):\n",
    "        # Input side\n",
    "        plt.scatter(0, input_positions[i], s=100, c='blue', zorder=5)\n",
    "        plt.text(-0.3, input_positions[i], f'x={xi:.1f}', ha='right', va='center')\n",
    "\n",
    "        # Output side\n",
    "        plt.scatter(2, input_positions[i], s=100, c='red', zorder=5)\n",
    "        plt.text(2.3, input_positions[i], f'f(x)={yi:.2f}', ha='left', va='center')\n",
    "\n",
    "        # Arrow showing mapping\n",
    "        plt.arrow(0.1, input_positions[i], 1.8, 0,\n",
    "                 head_width=0.1, head_length=0.1, fc='gray', ec='gray', alpha=0.7)\n",
    "\n",
    "    plt.xlim(-0.5, 2.8)\n",
    "    plt.ylim(-0.5, len(sample_x) - 0.5)\n",
    "    plt.title('Point-wise Mapping\\nScalar Input → Scalar Output')\n",
    "    plt.text(0, -0.8, 'Input Space R', ha='center', fontsize=12, fontweight='bold')\n",
    "    plt.text(2, -0.8, 'Output Space R', ha='center', fontsize=12, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot 3: Characteristics\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.text(0.1, 0.9, 'Traditional Neural Networks:', fontsize=14, fontweight='bold',\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.75, '• Input: Single numbers (scalars)', fontsize=12,\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.65, '• Output: Single numbers (scalars)', fontsize=12,\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.55, '• Learn: Point-wise mappings', fontsize=12,\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.45, '• Architecture: Standard feedforward', fontsize=12,\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.3, 'Examples:', fontsize=12, fontweight='bold',\n",
    "             transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.2, '• f(x) = x²', fontsize=12,\n",
    "             transform=plt.gca().transAxes, style='italic')\n",
    "    plt.text(0.1, 0.1, '• Classification problems', fontsize=12,\n",
    "             transform=plt.gca().transAxes, style='italic')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    plt.title('Characteristics')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Traditional Function Approximation:\")\n",
    "    print(\"• Takes individual points as input\")\n",
    "    print(\"• Produces individual points as output\")\n",
    "    print(\"• Neural network learns: x → f(x)\")\n",
    "    print(\"• This is what we're familiar with!\")\n",
    "\n",
    "demonstrate_function_approximation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkx4Io6F1T_2"
   },
   "source": [
    "### Operator Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TQ8U_ZR1T_2",
    "outputId": "55453f82-0abb-47d7-b3ad-9f2815c47f61"
   },
   "outputs": [],
   "source": [
    "def demonstrate_operator_concept():\n",
    "    \"\"\"Demonstrate the concept of operators\"\"\"\n",
    "\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "\n",
    "    # Three different input functions\n",
    "    u1 = x**2\n",
    "    u2 = np.sin(2*x)\n",
    "    u3 = np.exp(-x**2)\n",
    "\n",
    "    # The derivative operator D applied to each\n",
    "    du1_dx = 2*x\n",
    "    du2_dx = 2*np.cos(2*x)\n",
    "    du3_dx = -2*x*np.exp(-x**2)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Show operator concept\n",
    "    for i, (u, du_dx, title) in enumerate([(u1, du1_dx, 'u(x) = x²'),\n",
    "                                           (u2, du2_dx, 'u(x) = sin(2x)'),\n",
    "                                           (u3, du3_dx, 'u(x) = e^(-x²)')]):\n",
    "\n",
    "        plt.subplot(3, 3, 3*i + 1)\n",
    "        plt.plot(x, u, 'b-', linewidth=2)\n",
    "        plt.title(f'Input Function\\n{title}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('u(x)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 3, 3*i + 2)\n",
    "        plt.arrow(0.3, 0.5, 0.4, 0, head_width=0.1, head_length=0.05, fc='green', ec='green', linewidth=3)\n",
    "        plt.text(0.5, 0.7, 'Derivative\\nOperator $\\mathcal{D}$', ha='center', fontsize=12, fontweight='bold')\n",
    "        plt.text(0.5, 0.3, '$\\mathcal{D}$[u] = du/dx', ha='center', fontsize=10, style='italic')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.axis('off')\n",
    "        plt.title('Apply Operator')\n",
    "\n",
    "        plt.subplot(3, 3, 3*i + 3)\n",
    "        plt.plot(x, du_dx, 'r-', linewidth=2)\n",
    "        derivative_titles = ['2x', '2cos(2x)', '-2xe^(-x²)']\n",
    "        plt.title(f'Output Function\\ndu/dx = {derivative_titles[i]}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('du/dx')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Key Insight: An operator takes a FUNCTION as input and produces a FUNCTION as output!\")\n",
    "    print(\"This is fundamentally different from traditional function approximation.\")\n",
    "\n",
    "demonstrate_operator_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMG3sr_D1T_3"
   },
   "source": [
    "## The Universal Approximation Theorem for Operators\n",
    "\n",
    "### The Breakthrough Theorem\n",
    "\n",
    "Just as the Universal Approximation Theorem tells us neural networks can approximate functions, there's a remarkable extension:\n",
    "\n",
    "**Theorem (Chen & Chen, 1995):** Neural networks can approximate **operators** that map functions to functions!\n",
    "\n",
    "**Mathematical Statement:** For any continuous operator $\\mathcal{G}: V \\subset C(K_1) \\rightarrow C(K_2)$ and $\\epsilon > 0$, there exist constants such that:\n",
    "\n",
    "$$\\left|\\mathcal{G}(u)(y) - \\sum_{k=1}^p \\underbrace{\\sum_{i=1}^n c_i^k \\sigma\\left(\\sum_{j=1}^m \\xi_{ij}^k u(x_j) + \\theta_i^k\\right)}_{\\text{Branch Network}} \\underbrace{\\sigma(w_k \\cdot y + \\zeta_k)}_{\\text{Trunk Network}}\\right| < \\epsilon$$\n",
    "\n",
    "### Decoding the Theorem\n",
    "\n",
    "This looks complex, but the insight is beautiful:\n",
    "\n",
    "1. **Branch Network:** Processes function $u$ sampled at sensor points $\\{x_j\\}$\n",
    "2. **Trunk Network:** Processes output coordinates $y$\n",
    "3. **Combination:** Multiply branch and trunk outputs, then sum\n",
    "\n",
    "**Key insight:** Any operator can be written as:\n",
    "$$\\mathcal{G}(u)(y) \\approx \\sum_{k=1}^p b_k(u) \\cdot t_k(y)$$\n",
    "\n",
    "where $b_k$ depends only on the input function and $t_k$ depends only on the output location!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMANWwo91T_4"
   },
   "source": [
    "### Visualizing the Operator UAT Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKfVcJqc1T_4"
   },
   "source": [
    "## DeepONet Architecture\n",
    "\n",
    "**DeepONet** (Deep Operator Network) is the practical implementation of the Operator Universal Approximation Theorem.\n",
    "\n",
    "### Core Architecture\n",
    "\n",
    "$$\\mathcal{G}_\\theta(u)(y) = \\sum_{k=1}^p b_k(u) \\cdot t_k(y) + b_0$$\n",
    "\n",
    "where:\n",
    "- **Branch network:** $b_k(u) = \\mathcal{B}_k([u(x_1), u(x_2), \\ldots, u(x_m)])$\n",
    "- **Trunk network:** $t_k(y) = \\mathcal{T}_k(y)$\n",
    "- **$p$:** Number of basis functions (typically 50-200)\n",
    "- **$b_0$:** Bias term\n",
    "\n",
    "![DeepONet](https://github.com/chishiki-ai/sciml/blob/main/docs/02-deeponet/figs/deeponet-arch.png?raw=1)\n",
    "### Training Data Structure\n",
    "\n",
    "**Input-output pairs:** $(u^{(i)}, y^{(j)}, \\mathcal{G}(u^{(i)})(y^{(j)}))$\n",
    "\n",
    "- **$N$** input functions: $\\{u^{(i)}\\}_{i=1}^N$\n",
    "- Each function sampled at **$m$** sensors: $\\{u^{(i)}(x_j)\\}_{j=1}^m$\n",
    "- Corresponding outputs at **query points**: $\\{\\mathcal{G}(u^{(i)})(y_k)\\}$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{N \\cdot P} \\sum_{i=1}^N \\sum_{k=1}^P \\left|\\mathcal{G}_\\theta(u^{(i)})(y_k) - \\mathcal{G}(u^{(i)})(y_k)\\right|^2$$\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "1. **Resolution independence:** Train on one grid, evaluate on any grid\n",
    "2. **Fast evaluation:** Once trained, instant prediction (no iterative solving)\n",
    "3. **Generalization:** Works for new functions not seen during training\n",
    "4. **Physical consistency:** Learns the underlying operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI_ewuvI1T_4"
   },
   "source": [
    "## Concrete Example - The Derivative Operator\n",
    "\n",
    "**Perfect starting point:** Learn the derivative operator $\\mathcal{D}[u] = \\frac{du}{dx}$\n",
    "\n",
    "**Why this example:**\n",
    "- Simple and intuitive\n",
    "- Exact analytical solution for verification\n",
    "- Shows how DeepONet learns basis decompositions\n",
    "- Bridges function approximation → operator learning\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "**Input functions:** Cubic polynomials $u(x) = ax^3 + bx^2 + cx + d$\n",
    "\n",
    "**Target operator:** $\\mathcal{D}[u](x) = \\frac{du}{dx} = 3ax^2 + 2bx + c$\n",
    "\n",
    "**Key insight:** The derivative of a cubic is always quadratic, so it can be written as:\n",
    "$$\\frac{du}{dx} = w_1 \\cdot 1 + w_2 \\cdot x + w_3 \\cdot x^2$$\n",
    "\n",
    "where $w_1 = c$, $w_2 = 2b$, $w_3 = 3a$.\n",
    "\n",
    "**The DeepONet challenge:** Can it learn this mapping automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owjc4Nmh1T_4"
   },
   "source": [
    "### Data Generation for DeepONet\n",
    "\n",
    "Key Concept: We're learning the derivative operator $D[u] = du/dx$\n",
    "\n",
    "- For cubic polynomials $u(x) = ax^3 + bx^2 + cx + d$\n",
    "\n",
    "- The derivative is $u^\\prime(x) = 3ax^2 + 2bx + c$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmcwWxug1T_4",
    "outputId": "009fa796-e675-44b4-fbe4-bbe444acc846"
   },
   "outputs": [],
   "source": [
    "# ========================= CELL 1: Data Generation =========================\n",
    "def generate_polynomial_data(num_functions=2000, num_points=100, x_range=(-2, 2)):\n",
    "    \"\"\"Generate cubic polynomial functions and their derivatives\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Generate random coefficients for cubic polynomials: ax^3 + bx^2 + cx + d\n",
    "    coeffs = np.random.randn(num_functions, 4) * 0.5\n",
    "\n",
    "    # Generate spatial points\n",
    "    x = np.linspace(x_range[0], x_range[1], num_points)\n",
    "\n",
    "    # Initialize arrays\n",
    "    functions = np.zeros((num_functions, num_points))\n",
    "    derivatives = np.zeros((num_functions, num_points))\n",
    "\n",
    "    for i in range(num_functions):\n",
    "        a, b, c, d = coeffs[i]\n",
    "        # f(x) = ax^3 + bx^2 + cx + d\n",
    "        functions[i] = a * x**3 + b * x**2 + c * x + d\n",
    "        # f'(x) = 3ax^2 + 2bx + c\n",
    "        derivatives[i] = 3 * a * x**2 + 2 * b * x + c\n",
    "\n",
    "    return coeffs, x, functions, derivatives\n",
    "\n",
    "\n",
    "print(\"=== POLYNOMIAL DATA GENERATION ===\")\n",
    "coeffs, x, functions, derivatives = generate_polynomial_data(num_functions=2000, num_points=100)\n",
    "\n",
    "print(f\"Generated {len(coeffs)} polynomial functions\")\n",
    "print(f\"Coefficient shape: {coeffs.shape} (a, b, c, d for ax³+bx²+cx+d)\")\n",
    "print(f\"Spatial domain: x ∈ [{x[0]:.1f}, {x[-1]:.1f}] with {len(x)} points\")\n",
    "print(f\"Functions shape: {functions.shape}\")\n",
    "print(f\"Derivatives shape: {derivatives.shape}\")\n",
    "\n",
    "# Visualize sample polynomials and their derivatives\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i in range(6):\n",
    "    ax = axes[i//3, i%3]\n",
    "\n",
    "    # Plot function and derivative\n",
    "    ax.plot(x, functions[i], 'b-', linewidth=2, label=f'f(x)', alpha=0.8)\n",
    "    ax.plot(x, derivatives[i], 'r-', linewidth=2, label=f\"f'(x)\", alpha=0.8)\n",
    "\n",
    "    # Show coefficients\n",
    "    a, b, c, d = coeffs[i]\n",
    "    ax.set_title(f'Sample {i+1}: [{a:.2f}, {b:.2f}, {c:.2f}, {d:.2f}]')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('polynomial_samples.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print some mathematical relationships\n",
    "print(\"\\nMathematical relationships:\")\n",
    "print(\"f(x) = ax³ + bx² + cx + d\")\n",
    "print(\"f'(x) = 3ax² + 2bx + c\")\n",
    "print(\"Goal: Learn operator mapping [a,b,c,d] → f'(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDjL9Wg91T_5"
   },
   "source": [
    "### DeepONet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HR7fioO-1T_5",
    "outputId": "6d9a247f-8e5c-452b-80eb-2e1ad13742de"
   },
   "outputs": [],
   "source": [
    "class DeepONet(nn.Module):\n",
    "    def __init__(self, branch_input_dim, trunk_input_dim, hidden_dim=32, num_basis=3):\n",
    "        super(DeepONet, self).__init__()\n",
    "        self.num_basis = num_basis\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Branch network - processes function coefficients\n",
    "        self.branch_net = nn.Sequential(\n",
    "            nn.Linear(branch_input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, num_basis)\n",
    "        )\n",
    "\n",
    "        # Trunk network - processes spatial coordinates\n",
    "        self.trunk_net = nn.Sequential(\n",
    "            nn.Linear(trunk_input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, num_basis)\n",
    "        )\n",
    "\n",
    "        # Bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, branch_input, trunk_input):\n",
    "        # Branch network output\n",
    "        branch_out = self.branch_net(branch_input)  # [batch_size, num_basis]\n",
    "\n",
    "        # Trunk network output - handle batch dimension\n",
    "        if trunk_input.dim() == 3:  # [batch_size, num_points, 1]\n",
    "            batch_size, num_points, _ = trunk_input.shape\n",
    "            trunk_input_flat = trunk_input.view(-1, 1)  # [batch_size * num_points, 1]\n",
    "            trunk_out = self.trunk_net(trunk_input_flat)  # [batch_size * num_points, num_basis]\n",
    "            trunk_out = trunk_out.view(batch_size, num_points, self.num_basis)  # [batch_size, num_points, num_basis]\n",
    "        else:  # [num_points, 1] - single evaluation\n",
    "            trunk_out = self.trunk_net(trunk_input)  # [num_points, num_basis]\n",
    "            trunk_out = trunk_out.unsqueeze(0)  # [1, num_points, num_basis]\n",
    "\n",
    "        # Compute inner product\n",
    "        output = torch.einsum('bi,bpi->bp', branch_out, trunk_out)\n",
    "\n",
    "        return output + self.bias\n",
    "\n",
    "\n",
    "print(\"\\n=== DEEPONET ARCHITECTURE ===\")\n",
    "model = DeepONet(branch_input_dim=4, trunk_input_dim=1, hidden_dim=32, num_basis=3)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"DeepONet Architecture:\")\n",
    "print(f\"- Branch input: 4 (polynomial coefficients [a,b,c,d])\")\n",
    "print(f\"- Trunk input: 1 (spatial coordinate x)\")\n",
    "print(f\"- Hidden dimension: {model.hidden_dim}\")\n",
    "print(f\"- Number of basis functions: {model.num_basis}\")\n",
    "print(f\"- Total parameters: {total_params:,}\")\n",
    "print(f\"- Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GLeAiZk1T_5"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VH-fIJRQ1T_5",
    "outputId": "330c2ea0-d487-4a05-ece2-7f3b32bbe00d"
   },
   "outputs": [],
   "source": [
    "def setup_data_loaders(coeffs, x, derivatives, batch_size=128):\n",
    "    \"\"\"Setup train/val/test data loaders\"\"\"\n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(coeffs))\n",
    "    val_size = int(0.1 * len(coeffs))\n",
    "\n",
    "    train_coeffs = coeffs[:train_size]\n",
    "    train_derivatives = derivatives[:train_size]\n",
    "\n",
    "    val_coeffs = coeffs[train_size:train_size+val_size]\n",
    "    val_derivatives = derivatives[train_size:train_size+val_size]\n",
    "\n",
    "    test_coeffs = coeffs[train_size+val_size:]\n",
    "    test_derivatives = derivatives[train_size+val_size:]\n",
    "\n",
    "    # Convert to tensors\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Create datasets - expand x_tensor for each function\n",
    "    train_x = x_tensor.unsqueeze(0).repeat(len(train_coeffs), 1, 1)\n",
    "    val_x = x_tensor.unsqueeze(0).repeat(len(val_coeffs), 1, 1)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_coeffs, dtype=torch.float32),\n",
    "        train_x,\n",
    "        torch.tensor(train_derivatives, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(val_coeffs, dtype=torch.float32),\n",
    "        val_x,\n",
    "        torch.tensor(val_derivatives, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, (test_coeffs, test_derivatives)\n",
    "\n",
    "def train_deeponet(model, train_loader, val_loader, num_epochs=500, lr=0.01):\n",
    "    \"\"\"Train the DeepONet model\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 100\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Progress bar\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training DeepONet\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_coeffs, batch_x, batch_derivatives in train_loader:\n",
    "            batch_coeffs = batch_coeffs.to(device)\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_derivatives = batch_derivatives.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_coeffs, batch_x)\n",
    "            loss = criterion(output, batch_derivatives)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_coeffs, batch_x, batch_derivatives in val_loader:\n",
    "                batch_coeffs = batch_coeffs.to(device)\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_derivatives = batch_derivatives.to(device)\n",
    "\n",
    "                output = model(batch_coeffs, batch_x)\n",
    "                loss = criterion(output, batch_derivatives)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Train': f'{avg_train_loss:.6f}',\n",
    "            'Val': f'{avg_val_loss:.6f}',\n",
    "            'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'\\nEarly stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    pbar.close()\n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"\\n=== TRAINING ===\")\n",
    "train_loader, val_loader, test_data = setup_data_loaders(coeffs, x, derivatives, batch_size=128)\n",
    "\n",
    "print(f\"Data splits:\")\n",
    "print(f\"- Training batches: {len(train_loader)} (batch size: 128)\")\n",
    "print(f\"- Validation batches: {len(val_loader)}\")\n",
    "print(f\"- Test samples: {len(test_data[0])}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses, val_losses = train_deeponet(model, train_loader, val_loader, num_epochs=500, lr=0.01)\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', alpha=0.8)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Final training loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYAsV7Y91T_6"
   },
   "source": [
    "### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTDRh8dA1T_6",
    "outputId": "d509f2f3-610d-41ac-d402-6e8ef2ce7881"
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(model, test_data, x):\n",
    "    \"\"\"Evaluate model predictions and create visualizations\"\"\"\n",
    "    test_coeffs, test_derivatives = test_data\n",
    "    model.eval()\n",
    "\n",
    "    # Compute test errors\n",
    "    test_errors = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_coeffs)):\n",
    "            coeffs_tensor = torch.tensor(test_coeffs[i:i+1], dtype=torch.float32).to(device)\n",
    "            x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            predicted = model(coeffs_tensor, x_tensor).cpu().numpy().flatten()\n",
    "            true_derivative = test_derivatives[i]\n",
    "\n",
    "            mse = np.mean((predicted - true_derivative)**2)\n",
    "            test_errors.append(mse)\n",
    "            predictions.append(predicted)\n",
    "\n",
    "    avg_test_mse = np.mean(test_errors)\n",
    "    print(f\"Average Test MSE: {avg_test_mse:.6f}\")\n",
    "    print(f\"Test MSE std: {np.std(test_errors):.6f}\")\n",
    "\n",
    "    return predictions, test_errors\n",
    "\n",
    "print(\"\\n=== PREDICTION & EVALUATION ===\")\n",
    "predictions, test_errors = evaluate_predictions(model, test_data, x)\n",
    "test_coeffs, test_derivatives = test_data\n",
    "\n",
    "# Plot prediction results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot sample predictions\n",
    "for i in range(6):\n",
    "    ax = axes[i//3, i%3]\n",
    "\n",
    "    ax.plot(x, test_derivatives[i], 'b-', linewidth=2, label='True f\\'(x)', alpha=0.8)\n",
    "    ax.plot(x, predictions[i], 'r--', linewidth=2, label='Predicted f\\'(x)', alpha=0.8)\n",
    "\n",
    "    # Show coefficients and error\n",
    "    coeffs_str = ', '.join([f'{c:.2f}' for c in test_coeffs[i]])\n",
    "    error = test_errors[i]\n",
    "    ax.set_title(f'Test {i+1}: [{coeffs_str}]\\nMSE: {error:.6f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel(\"f'(x)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_results.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFQyplRi1T_6"
   },
   "source": [
    "### Understanding What DeepONet Learned\n",
    "\n",
    "**Critical question:** Did DeepONet truly learn the derivative operator, or just memorize patterns?\n",
    "\n",
    "Let's test it on completely new types of functions it has never seen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPh80v1P1T_6",
    "outputId": "bbb13457-0696-4f10-87fd-b8de27b47681"
   },
   "outputs": [],
   "source": [
    "def extract_basis_equations(model, x_range=(-2, 2), num_points=1000):\n",
    "    \"\"\"Extract basis functions as polynomial equations by fitting\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Generate fine-grained x values for fitting\n",
    "    x_fine = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    x_tensor = torch.tensor(x_fine, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get trunk network outputs (basis functions)\n",
    "        basis_outputs = model.trunk_net(x_tensor).cpu().numpy()  # [num_points, num_basis]\n",
    "\n",
    "    # Fit polynomials to each basis function\n",
    "    basis_equations = []\n",
    "\n",
    "    for i in range(model.num_basis):\n",
    "        y_basis = basis_outputs[:, i]\n",
    "\n",
    "        # Try different polynomial degrees and pick the best fit\n",
    "        best_coeffs = None\n",
    "        best_degree = 0\n",
    "        best_r2 = -np.inf\n",
    "\n",
    "        for degree in range(1, 5):  # Try degrees 1-4\n",
    "            try:\n",
    "                coeffs = np.polyfit(x_fine, y_basis, degree)\n",
    "                y_pred = np.polyval(coeffs, x_fine)\n",
    "\n",
    "                # Calculate R² score\n",
    "                ss_res = np.sum((y_basis - y_pred) ** 2)\n",
    "                ss_tot = np.sum((y_basis - np.mean(y_basis)) ** 2)\n",
    "                r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "\n",
    "                if r2 > best_r2:\n",
    "                    best_r2 = r2\n",
    "                    best_coeffs = coeffs\n",
    "                    best_degree = degree\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Format equation string\n",
    "        equation = format_polynomial_equation(best_coeffs, best_degree)\n",
    "        basis_equations.append({\n",
    "            'coefficients': best_coeffs,\n",
    "            'degree': best_degree,\n",
    "            'equation': equation,\n",
    "            'r2_score': best_r2,\n",
    "            'function_values': y_basis,\n",
    "            'x_values': x_fine\n",
    "        })\n",
    "\n",
    "    return basis_equations\n",
    "\n",
    "def format_polynomial_equation(coeffs, degree):\n",
    "    \"\"\"Format polynomial coefficients as readable equation\"\"\"\n",
    "    if coeffs is None:\n",
    "        return \"Unable to fit\"\n",
    "\n",
    "    terms = []\n",
    "\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        power = degree - i\n",
    "\n",
    "        if abs(coeff) < 1e-6:  # Skip negligible terms\n",
    "            continue\n",
    "\n",
    "        # Format coefficient\n",
    "        if len(terms) == 0:  # First term\n",
    "            coeff_str = f\"{coeff:.4f}\"\n",
    "        else:  # Subsequent terms\n",
    "            coeff_str = f\"{coeff:+.4f}\"\n",
    "\n",
    "        # Format power\n",
    "        if power == 0:\n",
    "            term = coeff_str\n",
    "        elif power == 1:\n",
    "            if abs(coeff - 1) < 1e-6:\n",
    "                term = \"x\" if len(terms) == 0 else \"+x\"\n",
    "            elif abs(coeff + 1) < 1e-6:\n",
    "                term = \"-x\"\n",
    "            else:\n",
    "                term = f\"{coeff_str}x\"\n",
    "        else:\n",
    "            if abs(coeff - 1) < 1e-6:\n",
    "                term = f\"x^{power}\" if len(terms) == 0 else f\"+x^{power}\"\n",
    "            elif abs(coeff + 1) < 1e-6:\n",
    "                term = f\"-x^{power}\"\n",
    "            else:\n",
    "                term = f\"{coeff_str}x^{power}\"\n",
    "\n",
    "        terms.append(term)\n",
    "\n",
    "    return \" \".join(terms) if terms else \"0\"\n",
    "\n",
    "def analyze_branch_behavior(model):\n",
    "    \"\"\"Analyze how branch network maps different polynomial types\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Test specific polynomial types\n",
    "    sample_coeffs = np.array([\n",
    "        [1, 0, 0, 0],  # Pure cubic: ax³ → 3ax²\n",
    "        [0, 1, 0, 0],  # Pure quadratic: bx² → 2bx\n",
    "        [0, 0, 1, 0],  # Pure linear: cx → c\n",
    "        [0, 0, 0, 1]   # Pure constant: d → 0\n",
    "    ])\n",
    "\n",
    "    coeffs_labels = ['ax³→3ax²', 'bx²→2bx', 'cx→c', 'd→0']\n",
    "\n",
    "    print(\"Branch Network Analysis:\")\n",
    "    print(\"Input polynomial term → Basis function weights\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for coeffs, label in zip(sample_coeffs, coeffs_labels):\n",
    "            coeffs_tensor = torch.tensor(coeffs.reshape(1, -1), dtype=torch.float32).to(device)\n",
    "            branch_out = model.branch_net(coeffs_tensor).detach().cpu().numpy().flatten()\n",
    "\n",
    "            weights_str = ', '.join([f'{w:.4f}' for w in branch_out])\n",
    "            print(f\"  {label}: [{weights_str}]\")\n",
    "\n",
    "    return sample_coeffs, coeffs_labels\n",
    "\n",
    "\n",
    "print(\"\\n=== BASIS FUNCTION ANALYSIS ===\")\n",
    "\n",
    "# Extract basis functions\n",
    "print(\"Extracting learned basis functions...\")\n",
    "basis_equations = extract_basis_equations(model, x_range=(-2, 2))\n",
    "\n",
    "# Visualize basis functions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Basis functions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    trunk_features = model.trunk_net(x_tensor).cpu().numpy()\n",
    "\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    for i in range(model.num_basis):\n",
    "        axes[0,0].plot(x, trunk_features[:, i], linewidth=2,\n",
    "                label=f'φ_{i+1}(x)', color=colors[i])\n",
    "\n",
    "    axes[0,0].set_xlabel('x')\n",
    "    axes[0,0].set_ylabel('Basis Function Value')\n",
    "    axes[0,0].set_title('Learned Basis Functions φᵢ(x)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Branch network behavior\n",
    "sample_coeffs, coeffs_labels = analyze_branch_behavior(model)\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "x_pos = np.arange(model.num_basis)\n",
    "width = 0.2\n",
    "\n",
    "for i, (coeffs, label) in enumerate(zip(sample_coeffs, coeffs_labels)):\n",
    "    coeffs_tensor = torch.tensor(coeffs.reshape(1, -1), dtype=torch.float32).to(device)\n",
    "    branch_out = model.branch_net(coeffs_tensor).detach().cpu().numpy().flatten()\n",
    "\n",
    "    axes[0,1].bar(x_pos + i*width, branch_out, width, alpha=0.7,\n",
    "           label=label, color=colors[i])\n",
    "\n",
    "axes[0,1].set_xlabel('Basis Function Index')\n",
    "axes[0,1].set_ylabel('Branch Weight')\n",
    "axes[0,1].set_title('Branch Network: Polynomial Terms → Weights')\n",
    "axes[0,1].set_xticks(x_pos + 1.5*width)\n",
    "axes[0,1].set_xticklabels([f'φ_{i+1}' for i in range(model.num_basis)])\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3-4: Individual basis functions with fitted equations\n",
    "for i in range(min(2, model.num_basis)):\n",
    "    ax = axes[1, i]\n",
    "    basis_info = basis_equations[i]\n",
    "\n",
    "    ax.plot(x, trunk_features[:, i], 'b-', linewidth=2, label='Learned φ(x)')\n",
    "\n",
    "    # Plot fitted polynomial\n",
    "    if basis_info['coefficients'] is not None:\n",
    "        x_fine = np.linspace(x[0], x[-1], 200)\n",
    "        y_fit = np.polyval(basis_info['coefficients'], x_fine)\n",
    "        ax.plot(x_fine, y_fit, 'r--', linewidth=2, label='Fitted polynomial')\n",
    "\n",
    "    ax.set_title(f'φ_{i+1}(x) = {basis_info[\"equation\"]}\\nR² = {basis_info[\"r2_score\"]:.4f}')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('φ(x)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('basis_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print mathematical formulation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEPONET MATHEMATICAL DECOMPOSITION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Original problem: Learn operator T: [a,b,c,d] → f'(x)\")\n",
    "print(\"where f(x) = ax³ + bx² + cx + d\")\n",
    "print(\"and f'(x) = 3ax² + 2bx + c\")\n",
    "print()\n",
    "print(\"DeepONet solution: f'(x) ≈ Σᵢ₌₁³ wᵢ(a,b,c,d) × φᵢ(x) + bias\")\n",
    "print()\n",
    "print(\"Learned basis functions:\")\n",
    "for i, basis_info in enumerate(basis_equations):\n",
    "    print(f\"  φ_{i+1}(x) = {basis_info['equation']} (R² = {basis_info['r2_score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOJAQ3zY1T_6"
   },
   "source": [
    "## The 1D Nonlinear Darcy Problem\n",
    "\n",
    "**Now for something more challenging:** A real PDE with nonlinear physics!\n",
    "\n",
    "### Problem Formulation\n",
    "\n",
    "The 1D nonlinear Darcy equation models groundwater flow with solution-dependent permeability:\n",
    "\n",
    "$$\\frac{d}{dx}\\left(-\\kappa(u(x))\\frac{du}{dx}\\right) = f(x), \\quad x \\in [0,1]$$\n",
    "\n",
    "where:\n",
    "- **`u(x)`** is the **solution field** (e.g., pressure or hydraulic head).\n",
    "- The **$\\kappa(u)$:** is non-linear solution-dependent permeability is `κ(u(x)) = 0.2 + u^2(x)`.\n",
    "- The **input term `f(x)`** is a Gaussian random field defined as `f(x) ~ GP(0, k(x, x'))` such that `k(x, x') = σ^2 exp(-||x - x'||^2 / (2ℓ_x^2))`, where `ℓ_x = 0.04` and `σ^2 = 1.0`.\n",
    "- Homogeneous Dirichlet boundary conditions **`u(0) = 0`** and **`u(1) = 0`** are considered at the domain boundaries.\n",
    "\n",
    "### The Operator Learning Challenge\n",
    "\n",
    "**Goal:** Learn the solution operator $\\mathcal{G}$ such that:\n",
    "$$\\mathcal{G}[f] = u$$\n",
    "\n",
    "where $u$ is the solution to the nonlinear Darcy equation for source $f$.\n",
    "\n",
    "**Key insight:** This is much harder than the derivative operator because:\n",
    "1. **Nonlinear PDE:** No analytical solution\n",
    "2. **Random sources:** Infinite variety of input functions\n",
    "3. **Complex physics:** Solution depends on entire source profile\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Traditional approach:** For each new source $f$, solve the PDE numerically (expensive!)\n",
    "\n",
    "**DeepONet approach:** Learn the operator once, then instant evaluation for any new source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTLauUo11T_6"
   },
   "source": [
    "### Let's examine the existing Darcy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULyZJ9nJ1T_6",
    "outputId": "dea746cc-208c-41e2-8f22-e368ef04af17"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========================= CELL 1: Data Generation =========================\n",
    "def generate_darcy_data(n_funcs=1000, n_points=40):\n",
    "    \"\"\"Generate 1D nonlinear Darcy flow data\"\"\"\n",
    "\n",
    "    def permeability(s):\n",
    "        return 0.2 + s**2\n",
    "\n",
    "    # Gaussian process for source function\n",
    "    x = np.linspace(0, 1, n_points)\n",
    "    l, sigma = 0.04, 1.0\n",
    "    K = sigma**2 * np.exp(-0.5 * (x[:, None] - x[None, :])**2 / l**2)\n",
    "    K += 1e-6 * np.eye(n_points)\n",
    "\n",
    "    def solve_darcy(u_func):\n",
    "        dx = x[1] - x[0]\n",
    "        s = np.zeros(n_points)\n",
    "\n",
    "        for _ in range(100):  # Fixed point iteration\n",
    "            kappa = permeability(s)\n",
    "            main_diag = (kappa[1:] + kappa[:-1]) / dx**2\n",
    "            upper_diag = -kappa[1:-1] / dx**2\n",
    "            lower_diag = -kappa[1:-1] / dx**2\n",
    "\n",
    "            A = diags([lower_diag, main_diag, upper_diag], [-1, 0, 1],\n",
    "                     shape=(n_points-2, n_points-2))\n",
    "\n",
    "            s_interior = spsolve(A, u_func[1:-1])\n",
    "            s_new = np.zeros(n_points)\n",
    "            s_new[1:-1] = s_interior\n",
    "            s = 0.5 * s_new + 0.5 * s\n",
    "\n",
    "        return s\n",
    "\n",
    "    # Generate dataset\n",
    "    np.random.seed(42)\n",
    "    X, U, S = [], [], []\n",
    "\n",
    "    print(\"Generating Darcy flow dataset...\")\n",
    "    for i in tqdm(range(n_funcs), desc=\"Solving PDEs\"):\n",
    "        u = multivariate_normal.rvs(mean=np.zeros(n_points), cov=K)\n",
    "        s = solve_darcy(u)\n",
    "        X.append(x)\n",
    "        U.append(u)\n",
    "        S.append(s)\n",
    "\n",
    "    X = torch.tensor(np.array(X), dtype=torch.float32)\n",
    "    U = torch.tensor(np.array(U), dtype=torch.float32)\n",
    "    S = torch.tensor(np.array(S), dtype=torch.float32)\n",
    "\n",
    "    print(f\"Dataset generated: {n_funcs} functions, {n_points} points each\")\n",
    "    print(f\"Input shape: {U.shape}, Output shape: {S.shape}\")\n",
    "\n",
    "    return X, U, S\n",
    "\n",
    "\n",
    "print(\"=== DATA GENERATION ===\")\n",
    "X, U, S = generate_darcy_data(n_funcs=1000, n_points=40)\n",
    "\n",
    "# Plot samples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "for i in range(6):\n",
    "    ax = axes[i//3, i%3]\n",
    "    ax.plot(X[i], U[i], 'g-', label='Source f(x)', linewidth=2)\n",
    "    ax.plot(X[i], S[i], 'b-', label='Solution u(x)', linewidth=2)\n",
    "    ax.set_title(f'Sample {i+1}')\n",
    "    ax.grid(True)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('darcy_samples.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx3HkMJd1T_6"
   },
   "source": [
    "### DeepONet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPESnIId1T_6",
    "outputId": "19a7797b-dc5f-4950-c7ac-212995d1ec14"
   },
   "outputs": [],
   "source": [
    "class DeepONet(nn.Module):\n",
    "    def __init__(self, branch_dim, trunk_dim, p_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Branch network (processes input function)\n",
    "        self.branch = nn.Sequential(\n",
    "            nn.Linear(branch_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, p_dim)\n",
    "        )\n",
    "\n",
    "        # Trunk network (processes query points)\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(trunk_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, p_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, u_vals, y_locs):\n",
    "        branch_out = self.branch(u_vals)  # [batch, p_dim]\n",
    "        trunk_out = self.trunk(y_locs)    # [batch, n_points, p_dim]\n",
    "\n",
    "        # Dot product: sum over p_dim\n",
    "        return torch.einsum('bp,bnp->bn', branch_out, trunk_out)\n",
    "\n",
    "\n",
    "print(\"\\n=== DEEPONET ARCHITECTURE ===\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = DeepONet(branch_dim=U.shape[1], trunk_dim=1, p_dim=128).to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbriOV_T1T_7"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF3ECq2I1T_7",
    "outputId": "5efa0965-b985-46a6-b8c6-899d63fc3fa8"
   },
   "outputs": [],
   "source": [
    "def setup_training(X, U, S, model, device):\n",
    "    \"\"\"Prepare data and optimizer for training\"\"\"\n",
    "    # Split data\n",
    "    n_train = int(0.8 * len(U))\n",
    "    U_train, S_train = U[:n_train].to(device), S[:n_train].to(device)\n",
    "    U_test, S_test = U[n_train:].to(device), S[n_train:].to(device)\n",
    "    X_train, X_test = X[:n_train].to(device), X[n_train:].to(device)\n",
    "\n",
    "    # Normalize targets\n",
    "    S_mean, S_std = S_train.mean(), S_train.std()\n",
    "    S_train_norm = (S_train - S_mean) / S_std\n",
    "    S_test_norm = (S_test - S_mean) / S_std\n",
    "\n",
    "    print(f\"Training samples: {len(U_train)}\")\n",
    "    print(f\"Test samples: {len(U_test)}\")\n",
    "    print(f\"Target normalization - Mean: {S_mean:.4f}, Std: {S_std:.4f}\")\n",
    "\n",
    "    return (U_train, S_train_norm, X_train, U_test, S_test_norm, X_test, S_mean, S_std)\n",
    "\n",
    "def train_model(model, train_data, epochs=5000, batch_size=64, lr=1e-3):\n",
    "    \"\"\"Train the DeepONet model\"\"\"\n",
    "    U_train, S_train_norm, X_train, _, _, _, _, _ = train_data\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    print(f\"Starting training: {epochs} epochs, batch size {batch_size}, lr {lr}\")\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "    for epoch in pbar:\n",
    "        # Random batch\n",
    "        idx = torch.randperm(len(U_train))[:batch_size]\n",
    "        u_batch = U_train[idx]\n",
    "        s_batch = S_train_norm[idx]\n",
    "        x_batch = X_train[idx].unsqueeze(-1)  # [batch, n_points, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        s_pred = model(u_batch, x_batch)\n",
    "        loss = F.mse_loss(s_pred, s_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.6f}',\n",
    "                'LR': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    return losses\n",
    "\n",
    "\n",
    "print(\"\\n=== TRAINING ===\")\n",
    "train_data = setup_training(X, U, S, model, device)\n",
    "losses = train_model(model, train_data, epochs=5000)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.savefig('training_loss.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpfEFz531T_7"
   },
   "source": [
    "### DeepONet Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhiILDvE1T_7",
    "outputId": "34f7c6dd-500a-487b-cf90-623b508b4fc0"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_data):\n",
    "    \"\"\"Evaluate model and create visualizations\"\"\"\n",
    "    U_train, S_train_norm, X_train, U_test, S_test_norm, X_test, S_mean, S_std = train_data\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Test on first few samples\n",
    "        n_test = min(5, len(U_test))\n",
    "        u_test = U_test[:n_test]\n",
    "        s_test_norm = S_test_norm[:n_test]\n",
    "        x_test = X_test[:n_test].unsqueeze(-1)\n",
    "\n",
    "        # Predict\n",
    "        s_pred_norm = model(u_test, x_test)\n",
    "\n",
    "        # Denormalize\n",
    "        s_pred = s_pred_norm * S_std + S_mean\n",
    "        s_test = s_test_norm * S_std + S_mean\n",
    "\n",
    "        # Compute error\n",
    "        l2_error = torch.sqrt(torch.mean((s_pred - s_test)**2) / torch.mean(s_test**2))\n",
    "        print(f\"Relative L2 error: {l2_error.item():.4f}\")\n",
    "\n",
    "        return s_pred, s_test, u_test, X_test\n",
    "\n",
    "print(\"\\n=== PREDICTION & EVALUATION ===\")\n",
    "s_pred, s_test, u_test, X_test = evaluate_model(model, train_data)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i in range(min(6, len(s_pred))):\n",
    "    ax = axes[i//3, i%3]\n",
    "    x_np = X_test[i].cpu().numpy()\n",
    "\n",
    "    ax.plot(x_np, s_test[i].cpu().numpy(), 'k-', label='True s(x)', linewidth=2)\n",
    "    ax.plot(x_np, s_pred[i].cpu().numpy(), 'r--', label='Pred s(x)', linewidth=2)\n",
    "\n",
    "    ax.set_title(f'Test Sample {i+1}')\n",
    "    ax.grid(True)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('deeponet_results.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6A0mb5o1T_7"
   },
   "source": [
    "### Understanding basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMB16Fxl1T_7",
    "outputId": "11b716ab-b2d0-4cc1-a841-e6134103259a"
   },
   "outputs": [],
   "source": [
    "def extract_basis_functions(model, X_test):\n",
    "    \"\"\"Extract and visualize learned basis functions\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Use test locations\n",
    "        x_points = X_test[0].unsqueeze(0).unsqueeze(-1).to(device)  # [1, n_points, 1]\n",
    "\n",
    "        # Get trunk network output (basis functions)\n",
    "        trunk_out = model.trunk(x_points)  # [1, n_points, p_dim]\n",
    "        basis_functions = trunk_out.squeeze(0).cpu().numpy()  # [n_points, p_dim]\n",
    "\n",
    "        print(f\"Extracted {basis_functions.shape[1]} basis functions\")\n",
    "        print(f\"Each basis function has {basis_functions.shape[0]} points\")\n",
    "\n",
    "        return basis_functions\n",
    "\n",
    "print(\"\\n=== BASIS FUNCTION ANALYSIS ===\")\n",
    "basis_functions = extract_basis_functions(model, X_test)\n",
    "\n",
    "# Plot first 8 basis functions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "x_np = X_test[0].cpu().numpy()\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i//4, i%4]\n",
    "    ax.plot(x_np, basis_functions[:, i], 'b-', linewidth=2)\n",
    "    ax.set_title(f'Basis Function φ_{i+1}(y)')\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('basis_functions.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKI8WUwI1T_7"
   },
   "source": [
    "## Summary:\n",
    "### What We've Learned\n",
    "\n",
    "1. **Conceptual Leap:** From function approximation to operator learning\n",
    "   - Functions: $\\mathbb{R}^d \\rightarrow \\mathbb{R}^m$ (point to point)\n",
    "   - Operators: $\\mathcal{F}_1 \\rightarrow \\mathcal{F}_2$ (function to function)\n",
    "\n",
    "2. **Theoretical Foundation:** Universal Approximation Theorem for Operators\n",
    "   - Neural networks can approximate operators!\n",
    "   - Branch-trunk architecture emerges naturally\n",
    "   - Basis function decomposition: $\\mathcal{G}(u)(y) = \\sum_k b_k(u) \\cdot t_k(y)$\n",
    "\n",
    "3. **Practical Implementation:** DeepONet architecture\n",
    "   - **Branch network:** Encodes input functions into coefficients\n",
    "   - **Trunk network:** Generates basis functions at query points\n",
    "   - **Training:** Learn from input-output function pairs\n",
    "\n",
    "4. **Real Applications:** From derivatives to nonlinear PDEs\n",
    "   - **Derivative operator:** Perfect pedagogical example\n",
    "   - **Darcy flow:** Real-world nonlinear PDE\n",
    "   - **Generalization:** Works on unseen function types\n",
    "\n",
    "### Key Advantages of DeepONet\n",
    "\n",
    "✅ **Resolution independence:** Train on one grid, evaluate on any grid\n",
    "\n",
    "✅ **Fast evaluation:** Once trained, instant prediction\n",
    "\n",
    "✅ **Generalization:** Works for new functions not seen during training\n",
    "\n",
    "✅ **Physical consistency:** Learns the underlying operator, not just patterns\n",
    "\n",
    "### When to Use DeepONet\n",
    "\n",
    "**Ideal scenarios:**\n",
    "- **Parametric PDEs:** Need solutions for many different source terms/boundary conditions\n",
    "- **Real-time applications:** Require instant evaluation\n",
    "- **Complex geometries:** Traditional methods struggle\n",
    "- **Multi-query problems:** Same operator, many evaluations\n",
    "\n",
    "**Limitations:**\n",
    "- **Training data:** Need many solved examples\n",
    "- **Complex operators:** Very nonlinear mappings may be challenging\n",
    "- **High dimensions:** Curse of dimensionality still applies\n",
    "\n",
    "### The Bigger Picture\n",
    "\n",
    "**DeepONet represents a paradigm shift:**\n",
    "- Traditional numerical methods: Solve each problem instance\n",
    "- **Operator learning:** Learn the solution pattern once, apply everywhere\n",
    "\n",
    "This opens new possibilities for:\n",
    "- **Inverse problems:** Learn parameter-to-solution mappings\n",
    "- **Control applications:** Real-time system response\n",
    "- **Multi-physics:** Coupled operator learning\n",
    "- **Scientific discovery:** Understanding operator structure\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Combine with PINNs for physics-informed operator learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tZKBWqO1T_7"
   },
   "source": [
    "## Further Reading and Extensions\n",
    "\n",
    "### Key Papers\n",
    "1. **[Lu et al. (2019)](https://arxiv.org/abs/1910.03193)** - Original DeepONet paper\n",
    "2. **[Goswami et al. (2023)](https://arxiv.org/pdf/2207.05748)** - Physics-informed DeepONets\n",
    "3. **[Chen & Chen (1995)](https://link.springer.com/article/10.1007/BF02551274)** - Universal approximation theorem for operators\n",
    "\n",
    "### Extensions to Explore\n",
    "- **Multi-output operators:** Vector-valued mappings\n",
    "- **Higher dimensions:** 2D/3D PDEs\n",
    "- **Physics-informed training:** Incorporate governing equations\n",
    "- **Fourier Neural Operators:** Alternative operator learning architecture\n",
    "\n",
    "### Exercises\n",
    "1. **Modify the derivative example** to learn the second derivative operator\n",
    "2. **Extend to 2D** by implementing the Laplacian operator $\\nabla^2 u$\n",
    "3. **Add physics constraints** by incorporating the differential equation into the loss\n",
    "4. **Compare with traditional methods** on computational efficiency\n",
    "\n",
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](https://cvw.cac.cornell.edu/SciML/deeponet/demo-deeponet-derivative)\n",
    "\n",
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](https://cvw.cac.cornell.edu/SciML/deeponet/demo-deeponet-einsum)\n",
    "\n",
    "The journey from function approximation to operator learning represents one of the most exciting frontiers in scientific machine learning!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
