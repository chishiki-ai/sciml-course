{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24541bdf",
   "metadata": {
    "id": "24541bdf"
   },
   "source": [
    "## Introduction: The 1D Poisson Problem\n",
    "\n",
    "\n",
    "In this notebook, we tackle the one-dimensional Poisson equation, a fundamental elliptic partial differential equation that describes many steady-state physical phenomena, such as heat conduction, electrostatics, and ideal fluid flow.\n",
    "\n",
    "Our specific problem is to find the solution $u(x)$ that satisfies:\n",
    "\n",
    "**Governing Equation:**\n",
    "$$\\frac{d^2u}{dx^2} + \\pi \\sin(\\pi x) = 0, \\quad \\text{for } x \\in [0, 1]$$\n",
    "\n",
    "**Boundary Conditions (BCs):**\n",
    "$$u(0) = 0 \\quad \\text{and} \\quad u(1) = 0$$\n",
    "\n",
    "This is a boundary value problem with Dirichlet conditions (i.e., the value of the solution is specified at the boundaries).\n",
    "\n",
    "**Analytical Solution:**\n",
    "For validation purposes, this problem has a known exact solution, which can be found by integrating the equation twice and applying the boundary conditions:\n",
    "$$u_{\\text{exact}}(x) = \\frac{1}{\\pi} \\sin(\\pi x)$$\n",
    "\n",
    "Our goal is to train a Physics-Informed Neural Network (PINN) to discover this solution without being given the analytical form, using only the governing equation and its boundary conditions. We will explore two common methods for enforcing the boundary conditions:\n",
    "\n",
    "1.  **Soft Constraints:** Adding a penalty term to the loss function for boundary violations.\n",
    "2.  **Hard Constraints:** Modifying the network architecture to satisfy the boundary conditions by construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3d10f",
   "metadata": {
    "id": "62c3d10f",
    "outputId": "3e0c3df4-c7f0-420e-dc78-733a3e96a982"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up plot styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14, 'figure.figsize': (10, 6)})\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for available device (CUDA, MPS, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Analytical solution for comparison\n",
    "def analytical_solution(x):\n",
    "    \"\"\"Analytical solution: u(x) = (1/π)sin(πx)\"\"\"\n",
    "    return (1 / np.pi) * np.sin(np.pi * x)\n",
    "\n",
    "# Generate test points for plotting\n",
    "x_test_np = np.linspace(0, 1, 200)\n",
    "u_true_np = analytical_solution(x_test_np)\n",
    "x_test = torch.tensor(x_test_np.reshape(-1, 1), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32669f98",
   "metadata": {
    "id": "32669f98"
   },
   "source": [
    "\n",
    "### **Stage 1: The Soft Constraint Approach (Weak Enforcement)**\n",
    "\n",
    "The most common way to enforce boundary conditions in a PINN is the \"soft\" or \"weak\" method. The core idea is to treat the boundary conditions just like the PDE itself: as a component of the total loss function that we want to minimize.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1.  We define a **boundary loss** term, $\\mathcal{L}_{BC}$, which measures the mean squared error between the network's predictions at the boundary points and the true boundary values.\n",
    "2.  This boundary loss is added to the PDE residual loss, $\\mathcal{L}*{PDE}$, often with a weighting factor $\\lambda*{BC}$.\n",
    "3.  The total loss is: $\\mathcal{L}*{\\text{total}} = \\mathcal{L}*{PDE} + \\lambda\\_{BC} \\mathcal{L}_{BC}$.\n",
    "\n",
    "By minimizing $\\mathcal{L}*{\\text{total}}$, the optimizer is encouraged to find a solution that *approximately* satisfies the boundary conditions. Satisfaction is not guaranteed by the network's structure but is instead a goal of the optimization. The weight $\\lambda*{BC}$ is a hyperparameter that balances how strictly the boundary conditions are enforced relative to the PDE.\n",
    "\n",
    "#### **PINN Definition and Loss Functions**\n",
    "\n",
    "First, we define our neural network and the two loss components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98dbe8",
   "metadata": {
    "id": "ad98dbe8"
   },
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \"\"\"Standard Feedforward Neural Network for PINN\"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=32, output_size=1, num_layers=3):\n",
    "        super(PINN, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.Tanh())\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def pde_loss(model, x_interior):\n",
    "    \"\"\"Computes the PDE residual loss: d²u/dx² + πsin(πx) = 0\"\"\"\n",
    "    x_interior.requires_grad_(True)\n",
    "    u = model(x_interior)\n",
    "\n",
    "    # First derivative\n",
    "    du_dx = torch.autograd.grad(u, x_interior, torch.ones_like(u), create_graph=True)[0]\n",
    "    # Second derivative\n",
    "    d2u_dx2 = torch.autograd.grad(du_dx, x_interior, torch.ones_like(du_dx), create_graph=True)[0]\n",
    "\n",
    "    forcing_term = torch.pi * torch.sin(torch.pi * x_interior)\n",
    "    pde_residual = d2u_dx2 + forcing_term\n",
    "\n",
    "    return torch.mean(pde_residual**2)\n",
    "\n",
    "def boundary_loss(model, x_boundary):\n",
    "    \"\"\"Computes boundary loss for u(0)=0 and u(1)=0\"\"\"\n",
    "    u_boundary = model(x_boundary)\n",
    "    # The target values are 0 for both boundaries\n",
    "    return torch.mean(u_boundary**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b909762",
   "metadata": {
    "id": "4b909762"
   },
   "source": [
    "\n",
    "#### **Training the Soft-Constraint PINN**\n",
    "\n",
    "Now, we set up the training loop. We sample points inside the domain (collocation points) to enforce the PDE and points on the boundary to enforce the BCs. We choose a large weight `lambda_bc` to strongly encourage the network to satisfy the boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d635a",
   "metadata": {
    "id": "c76d635a",
    "outputId": "4dd0a2cf-23cb-495d-f8f8-2ae808b15a12"
   },
   "outputs": [],
   "source": [
    "def train_pinn_soft(model, epochs=15000, lr=0.001, lambda_bc=1000.0):\n",
    "    \"\"\"Trains the PINN using the soft constraint method.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Collocation points\n",
    "    n_interior = 500\n",
    "    x_interior = torch.rand(n_interior, 1, device=device)\n",
    "\n",
    "    # Boundary points\n",
    "    x_boundary = torch.tensor([[0.0], [1.0]], device=device)\n",
    "\n",
    "    print(f\"\\n--- Training Soft-Constraint PINN (lambda_bc = {lambda_bc}) ---\")\n",
    "    losses = {'total': [], 'pde': [], 'boundary': []}\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Training Soft-Constraint PINN\", ncols=100)\n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute losses\n",
    "        loss_pde = pde_loss(model, x_interior)\n",
    "        loss_bc = boundary_loss(model, x_boundary)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = loss_pde + lambda_bc * loss_bc\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store and display losses\n",
    "        losses['total'].append(total_loss.item())\n",
    "        losses['pde'].append(loss_pde.item())\n",
    "        losses['boundary'].append(loss_bc.item())\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{total_loss.item():.2e}',\n",
    "            'PDE': f'{loss_pde.item():.2e}',\n",
    "            'BC': f'{loss_bc.item():.2e}'\n",
    "        })\n",
    "\n",
    "    pbar.close()\n",
    "    return model, losses\n",
    "\n",
    "# Instantiate and train the soft-constraint model\n",
    "pinn_soft = PINN().to(device)\n",
    "pinn_soft_trained, losses_soft = train_pinn_soft(pinn_soft)\n",
    "\n",
    "# Get predictions\n",
    "pinn_soft_trained.eval()\n",
    "with torch.no_grad():\n",
    "    u_pred_soft = pinn_soft_trained(x_test).cpu().numpy()\n",
    "\n",
    "# Plot results for the soft-constraint model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_test_np, u_true_np, 'k-', label='Analytical Solution', linewidth=3)\n",
    "plt.plot(x_test_np, u_pred_soft, 'r--', label='PINN (Soft Constraint)', linewidth=2.5)\n",
    "plt.title('Soft Constraint PINN vs. Analytical Solution')\n",
    "plt.xlabel('Position (x)')\n",
    "plt.ylabel('Displacement (u)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Check boundary values\n",
    "print(f\"Predicted u(0): {u_pred_soft[0][0]:.4e}\")\n",
    "print(f\"Predicted u(1): {u_pred_soft[-1][0]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb8a3e",
   "metadata": {
    "id": "f3cb8a3e"
   },
   "source": [
    "\n",
    "As we can see, the soft constraint approach produces a very accurate solution. However, the predicted values at the boundaries are not exactly zero, but very small numbers. This is the nature of soft enforcement—it gets close, but not perfect, depending on the loss weight and training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0e881",
   "metadata": {
    "id": "12c0e881"
   },
   "source": [
    "\n",
    "### **Stage 2: The Hard Constraint Approach (Strong Enforcement)**\n",
    "\n",
    "An alternative to penalty terms is to enforce the boundary conditions *by construction*. This is known as the \"hard\" or \"strong\" method.\n",
    "\n",
    "**How it works:**\n",
    "We redefine the network's output to create a \"trial solution\" $\\tilde{u}(x)$ that is mathematically guaranteed to satisfy the Dirichlet boundary conditions, regardless of the neural network's raw output.\n",
    "\n",
    "For our problem with BCs $u(0)=0$ and $u(1)=0$, a suitable trial solution is:\n",
    "$$\\tilde{u}(x) = D(x) \\cdot \\text{NN}(x; \\theta)$$\n",
    "where $\\text{NN}(x; \\theta)$ is the output of a standard neural network and $D(x)$ is a function that is zero at the boundaries. A simple and effective choice is:\n",
    "$$D(x) = x(1-x)$$\n",
    "Our new model's output is $\\tilde{u}(x) = x(1-x)\\text{NN}(x; \\theta)$.\n",
    "\n",
    "  * At $x=0$, $\\tilde{u}(0) = 0 \\cdot (1-0) \\cdot \\text{NN}(0) = 0$.\n",
    "  * At $x=1$, $\\tilde{u}(1) = 1 \\cdot (1-1) \\cdot \\text{NN}(1) = 0$.\n",
    "\n",
    "The BCs are now satisfied exactly. The training process simplifies significantly, as the **loss function now only contains the PDE residual term**. We no longer need a boundary loss or a weighting hyperparameter.\n",
    "\n",
    "#### **PINN Definition for Hard Constraints**\n",
    "\n",
    "We create a new model that wraps our original `PINN` and applies the hard constraint transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67faa0",
   "metadata": {
    "id": "cb67faa0"
   },
   "outputs": [],
   "source": [
    "class PINNHardConstraint(nn.Module):\n",
    "    \"\"\"PINN that enforces u(0)=0 and u(1)=0 by construction.\"\"\"\n",
    "    def __init__(self, base_network):\n",
    "        super(PINNHardConstraint, self).__init__()\n",
    "        self.base_network = base_network\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Raw output from the base network\n",
    "        nn_output = self.base_network(x)\n",
    "\n",
    "        # Enforce u(0)=0 and u(1)=0\n",
    "        # D(x) = x * (1 - x)\n",
    "        # u_trial(x) = D(x) * NN(x)\n",
    "        u_trial = x * (1.0 - x) * nn_output\n",
    "\n",
    "        return u_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b233b9",
   "metadata": {
    "id": "83b233b9"
   },
   "source": [
    "#### **Training the Hard-Constraint PINN**\n",
    "\n",
    "The training loop is now simpler, with only the PDE loss to minimize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5e7bc",
   "metadata": {
    "id": "d5e5e7bc",
    "outputId": "4584b437-8d69-47c3-c2f2-1ad89fb11bb5"
   },
   "outputs": [],
   "source": [
    "def train_pinn_hard(model, epochs=15000, lr=0.001):\n",
    "    \"\"\"Trains the PINN using the hard constraint method.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Collocation points\n",
    "    n_interior = 500\n",
    "    x_interior = torch.rand(n_interior, 1, device=device)\n",
    "\n",
    "    print(\"\\n--- Training Hard-Constraint PINN ---\")\n",
    "    losses = {'pde': []}\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Training Hard-Constraint PINN\", ncols=100)\n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss (only PDE loss is needed)\n",
    "        loss_pde = pde_loss(model, x_interior)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_pde.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store and display loss\n",
    "        losses['pde'].append(loss_pde.item())\n",
    "        pbar.set_postfix({'PDE Loss': f'{loss_pde.item():.2e}'})\n",
    "\n",
    "    pbar.close()\n",
    "    return model, losses\n",
    "\n",
    "# Instantiate the base network and the hard-constraint wrapper\n",
    "base_pinn = PINN().to(device)\n",
    "pinn_hard = PINNHardConstraint(base_pinn).to(device)\n",
    "\n",
    "# Train the hard-constraint model\n",
    "pinn_hard_trained, losses_hard = train_pinn_hard(pinn_hard)\n",
    "\n",
    "# Get predictions\n",
    "pinn_hard_trained.eval()\n",
    "with torch.no_grad():\n",
    "    u_pred_hard = pinn_hard_trained(x_test).cpu().numpy()\n",
    "\n",
    "# Plot results for the hard-constraint model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_test_np, u_true_np, 'k-', label='Analytical Solution', linewidth=3)\n",
    "plt.plot(x_test_np, u_pred_hard, 'b--', label='PINN (Hard Constraint)', linewidth=2.5)\n",
    "plt.title('Hard Constraint PINN vs. Analytical Solution')\n",
    "plt.xlabel('Position (x)')\n",
    "plt.ylabel('Displacement (u)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Check boundary values\n",
    "print(f\"Predicted u(0): {u_pred_hard[0][0]:.4e}\")\n",
    "print(f\"Predicted u(1): {u_pred_hard[-1][0]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e68a72",
   "metadata": {
    "id": "b9e68a72"
   },
   "source": [
    "With the hard constraint, the predicted values at the boundaries are exactly zero, as expected from the design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a89be",
   "metadata": {
    "id": "156a89be"
   },
   "source": [
    "\n",
    "### **Stage 3: Comparison and Analysis**\n",
    "\n",
    "Now let's directly compare the performance of the soft and hard constraint methods.\n",
    "\n",
    "#### **Solution and Error Comparison**\n",
    "\n",
    "We plot both PINN solutions against the analytical solution and also visualize their absolute errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81589920",
   "metadata": {
    "id": "81589920",
    "outputId": "88f1b707-914b-44fe-a7b3-5ffac7ec8c9d"
   },
   "outputs": [],
   "source": [
    "# Calculate absolute errors\n",
    "error_soft = np.abs(u_pred_soft - u_true_np.reshape(-1, 1))\n",
    "error_hard = np.abs(u_pred_hard - u_true_np.reshape(-1, 1))\n",
    "\n",
    "# Create comparison plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Solution Comparison\n",
    "ax1.plot(x_test_np, u_true_np, 'k-', label='Analytical Solution', linewidth=3, alpha=0.8)\n",
    "ax1.plot(x_test_np, u_pred_soft, 'r--', label='Soft Constraint', linewidth=2)\n",
    "ax1.plot(x_test_np, u_pred_hard, 'b:', label='Hard Constraint', linewidth=3)\n",
    "ax1.set_title('Solution Comparison')\n",
    "ax1.set_xlabel('Position (x)')\n",
    "ax1.set_ylabel('Displacement (u)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.5)\n",
    "\n",
    "# Plot 2: Absolute Error Comparison\n",
    "ax2.plot(x_test_np, error_soft, 'r-', label='Soft Constraint Error', linewidth=2)\n",
    "ax2.plot(x_test_np, error_hard, 'b-', label='Hard Constraint Error', linewidth=2)\n",
    "ax2.set_title('Absolute Error Comparison')\n",
    "ax2.set_xlabel('Position (x)')\n",
    "ax2.set_ylabel('Absolute Error')\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantitative Error Metrics\n",
    "rmse_soft = np.sqrt(np.mean(error_soft**2))\n",
    "rmse_hard = np.sqrt(np.mean(error_hard**2))\n",
    "u0_soft = u_pred_soft[0][0]\n",
    "u1_soft = u_pred_soft[-1][0]\n",
    "u0_hard = u_pred_hard[0][0]\n",
    "u1_hard = u_pred_hard[-1][0]\n",
    "\n",
    "print(\"--- Quantitative Comparison ---\")\n",
    "print(f\"Method             | RMSE      | u(0) Value | u(1) Value\")\n",
    "print(\"-------------------|-----------|------------|-----------\")\n",
    "print(f\"Soft Constraint    | {rmse_soft:.3e} | {u0_soft:.3e}  | {u1_soft:.3e}\")\n",
    "print(f\"Hard Constraint    | {rmse_hard:.3e} | {u0_hard:.3e} | {u1_hard:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f6028",
   "metadata": {
    "id": "0f8f6028"
   },
   "source": [
    "\n",
    "#### **Discussion of Results**\n",
    "\n",
    "Both methods achieve a high degree of accuracy for this problem. The hard constraint method shows a slightly lower overall error (RMSE) and, by design, has zero error at the boundaries.\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "  * **Soft Constraints (Weak Enforcement):**\n",
    "\n",
    "      * **Pros:** Very flexible. The same approach can be used for different types of boundary conditions (Dirichlet, Neumann, Robin) without changing the network architecture. It's easy to implement.\n",
    "      * **Cons:** Requires careful tuning of the loss weight ($\\lambda\\_{BC}$). If the weight is too small, the BCs are not well-enforced. If it's too large, it can dominate the optimization and hinder convergence of the PDE part. Satisfaction of BCs is only approximate.\n",
    "\n",
    "  * **Hard Constraints (Strong Enforcement):**\n",
    "\n",
    "      * **Pros:** Guarantees exact satisfaction of Dirichlet BCs. Simplifies the loss function, removing the need to balance competing PDE and BC loss terms, which can lead to more stable and faster training.\n",
    "      * **Cons:** Less flexible. It requires designing a specific trial function $\\tilde{u}(x)$ for the exact geometry and boundary conditions of the problem. This can become very difficult or impossible for complex domains or more complicated BCs.\n",
    "\n",
    "For problems with simple geometries and Dirichlet boundary conditions, the **hard constraint method is often the superior choice** due to its robustness and simplified training dynamics. For problems with complex geometries or other types of boundary conditions, the **soft constraint method provides a more versatile and straightforward framework**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
