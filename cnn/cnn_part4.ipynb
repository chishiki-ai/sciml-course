{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e805597",
   "metadata": {},
   "source": [
    "# CNN Part 4: Single Node MultiGPU Training with Torchrun "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470907f",
   "metadata": {},
   "source": [
    "## Using Torchrun \n",
    "Code below will modify the MNIST example to be run by torchrun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af9480",
   "metadata": {},
   "source": [
    "### Modify code for environment variables set by torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# A. Remove code that sets environment variables as this done for you automatically with torchrun.\n",
    "def init_distributed():\n",
    "\n",
    "    # B. Instead, use these environment variables set by pytorch and instead of explicitly defining them.\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    dist.init_process_group(\"nccl\",\n",
    "                            rank=local_rank,\n",
    "                            world_size=world_size)\n",
    "\n",
    "def main():\n",
    "    #####################################################################\n",
    "    # B. We also create the variable local_rank in our main function as well as call the new init_distributed()\n",
    "    # this will be used to assign the gpu where our model should reside as highlighted below \n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    init_distributed()\n",
    "    ################################################\n",
    "    # .....\n",
    "    # instantiate network and set to local_rank device\n",
    "    net = Net().to(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334192",
   "metadata": {},
   "source": [
    "### Add code for writing checkpoints and resuming training after failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee27c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    init_distributed()\n",
    "\n",
    "    train_dataloader = prepare_data()\n",
    "\n",
    "    ################################################                                                 \n",
    "    # A. Create location to store checkpoints\n",
    "\n",
    "    # Create directory for storing checkpointed model\n",
    "    model_folder_path = os.path.join(os.environ['SCRATCH'], \"cnn4_mnist_output_model\") # create variable for path to folder for checkpoints\n",
    "    os.makedirs(model_folder_path,exist_ok=True)                         # create directory for models if they do not exist\n",
    "    \n",
    "    # create file name for checkpoint \n",
    "    checkpoint_file = os.path.join(model_folder_path, \"best_model.pt\")   # create filename for model checkpoint\n",
    "    ################################################\n",
    "\n",
    "    net = Net().to(local_rank)\n",
    "\n",
    "    #################################################\n",
    "    # 2B. Read checkpoints if they exist \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)  # load previous checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # set model weights to be that of the last checkpoint\n",
    "        epoch_start = checkpoint['epoch']                      # set epoch where training should resume\n",
    "   \n",
    "    # otherwise we are starting training from the beginning at epoch 0\n",
    "    else:\n",
    "        epoch_start = 0\n",
    "    ################################################\n",
    "\n",
    "    model = DDP(net,\n",
    "            device_ids=[local_rank],                  # list of gpu that model lives on \n",
    "            output_device=local_rank,                 # where to output model\n",
    "        )\n",
    "\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    save_every = 1\n",
    "    epochs = 10\n",
    "    ###########################################################\n",
    "    # 2C. Resume training at epoch last checkpoint was written\n",
    "    for epoch in range(epoch_start, epochs):                  # note we start loop at epoch_start defined in code above\n",
    "    ###########################################################\n",
    "        train_loop(rank, train_dataloader, model, loss_fn, optimizer)\n",
    "        ###########################################################\n",
    "        # 2D. Write checkpoints periodically during training\n",
    "        if rank == 0 and epoch%save_every==0:\n",
    "            print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "            torch.save({                                     # save model's state_dict and current epoch periodically\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict':model.module.state_dict(),\n",
    "            }, checkpoint_file)\n",
    "            print(\"Finished saving model\\n\")\n",
    "        ###########################################################\n",
    "\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6887b5",
   "metadata": {},
   "source": [
    "## Launching jobs with Torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-nebraska",
   "metadata": {},
   "source": [
    "The two cells below help display Python files `.py` as HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8fc9299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9f37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f44c33",
   "metadata": {},
   "source": [
    "Remove the model from $SCRATCH if you would like to start from epoch 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17930e8",
   "metadata": {},
   "source": [
    "`display.Code` displays `cnn_part4/mnist_torchrun.py` with Python syntax highlighting as the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c22fedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch.distributed</span> <span class=\"k\">as</span> <span class=\"nn\">dist</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span> \n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.multiprocessing</span> <span class=\"k\">as</span> <span class=\"nn\">mp</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">DistributedSampler</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">flatten</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Flatten</span><span class=\"p\">()</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear_relu_stack</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"o\">*</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear_relu_stack</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">prob</span>\n",
       "\n",
       "<span class=\"c1\"># Remove rank and world_size -- use what&#39;s in </span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">prepare_data</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">):</span>\n",
       "\n",
       "    <span class=\"n\">trainset</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">MNIST</span><span class=\"p\">(</span>\n",
       "                            <span class=\"n\">root</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;SCRATCH&#39;</span><span class=\"p\">],</span> <span class=\"s2\">&quot;data&quot;</span><span class=\"p\">),</span>      <span class=\"c1\"># path to where data is stored</span>\n",
       "                            <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>                                         <span class=\"c1\"># specifies if data is train or test</span>\n",
       "                            <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>                                      <span class=\"c1\"># downloads data if not available at root</span>\n",
       "                            <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">()</span>         <span class=\"c1\"># trasforms both features and targets accordingly</span>\n",
       "                            <span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># pass data to the distributed sampler and dataloader </span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">trainset</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"n\">trainset</span><span class=\"p\">),</span><span class=\"c1\"># num_replicas=world_size, rank=rank),</span>\n",
       "                                  <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataloader</span>\n",
       "\n",
       "\n",
       "<span class=\"c1\">##################################################################################</span>\n",
       "<span class=\"c1\"># 1A. Remove code that sets environment variables as this done for you automatically with torchrun.</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">init_distributed</span><span class=\"p\">():</span>   <span class=\"c1\">#rank, world_size):</span>\n",
       "    <span class=\"c1\"># 1B. Instead, use these environment variables set by pytorch and instead of explicitly defining them.</span>\n",
       "    <span class=\"n\">world_size</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;WORLD_SIZE&#39;</span><span class=\"p\">])</span>\n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">set_device</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>                       <span class=\"c1\">#</span>\n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"s2\">&quot;nccl&quot;</span><span class=\"p\">,</span>                   <span class=\"c1\"># backend being used; nccl typically used with distributed GPU training</span>\n",
       "                            <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>                <span class=\"c1\"># rank of the current process being used</span>\n",
       "                            <span class=\"n\">world_size</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>    <span class=\"c1\"># total number of processors being used</span>\n",
       "<span class=\"c1\">#############################################################</span>\n",
       "\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.nn.parallel</span> <span class=\"kn\">import</span> <span class=\"n\">DistributedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">DDP</span>\n",
       "\n",
       "<span class=\"c1\"># training loop for one epoch</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">train_loop</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\"># transfer data to GPU if available</span>\n",
       "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Compute prediction and loss</span>\n",
       "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"c1\"># Backpropagation</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "        <span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"k\">if</span> <span class=\"n\">batch</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "                <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">(),</span> <span class=\"n\">batch</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"si\">:</span><span class=\"s2\">&gt;7f</span><span class=\"si\">}</span><span class=\"s2\">  [</span><span class=\"si\">{</span><span class=\"n\">current</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">size</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">]&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n",
       "    <span class=\"c1\">#####################################################################</span>\n",
       "    <span class=\"c1\"># 1.B We also create the variable local_rank in our main function as well as call the new init_distributed()</span>\n",
       "    <span class=\"c1\"># this will be used to assign the gpu where our model should reside</span>\n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "\n",
       "    <span class=\"n\">init_distributed</span><span class=\"p\">()</span>\n",
       "    <span class=\"c1\">################################################</span>\n",
       "\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">prepare_data</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"c1\">################################################                                                 </span>\n",
       "    <span class=\"c1\"># A. Create location to store checkpoints</span>\n",
       "\n",
       "    <span class=\"c1\"># Create directory for storing checkpointed model</span>\n",
       "    <span class=\"n\">model_folder_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;SCRATCH&#39;</span><span class=\"p\">],</span> <span class=\"s2\">&quot;cnn4_mnist_output_model&quot;</span><span class=\"p\">)</span> <span class=\"c1\"># create variable for path to folder for checkpoints</span>\n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">makedirs</span><span class=\"p\">(</span><span class=\"n\">model_folder_path</span><span class=\"p\">,</span><span class=\"n\">exist_ok</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>                              <span class=\"c1\"># create directory for models if they do not exist</span>\n",
       "    <span class=\"c1\"># create file name for checkpoint </span>\n",
       "    <span class=\"n\">checkpoint_file</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_folder_path</span><span class=\"p\">,</span> <span class=\"s2\">&quot;best_model.pt&quot;</span><span class=\"p\">)</span>        <span class=\"c1\"># create filename for model checkpoint</span>\n",
       "    <span class=\"c1\">################################################</span>\n",
       "\n",
       "    <span class=\"c1\"># instantiate network and set to local_rank device</span>\n",
       "    <span class=\"n\">net</span> <span class=\"o\">=</span> <span class=\"n\">Net</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"c1\">#################################################</span>\n",
       "    <span class=\"c1\"># 2B. Read checkpoints if they exist </span>\n",
       "    <span class=\"k\">if</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">checkpoint_file</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">checkpoint_file</span><span class=\"p\">,</span> <span class=\"n\">map_location</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">))</span>\n",
       "        <span class=\"n\">net</span><span class=\"o\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s1\">&#39;model_state_dict&#39;</span><span class=\"p\">])</span>\n",
       "        <span class=\"n\">epoch_start</span> <span class=\"o\">=</span> <span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s1\">&#39;epoch&#39;</span><span class=\"p\">]</span>\n",
       "    \n",
       "    <span class=\"c1\"># otherwise we are starting training from the beginning at epoch 0</span>\n",
       "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">epoch_start</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
       "    <span class=\"c1\">################################################</span>\n",
       "    \n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DDP</span><span class=\"p\">(</span><span class=\"n\">net</span><span class=\"p\">,</span>\n",
       "            <span class=\"n\">device_ids</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">local_rank</span><span class=\"p\">],</span>                  <span class=\"c1\"># list of gpu that model lives on </span>\n",
       "            <span class=\"n\">output_device</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>                 <span class=\"c1\"># where to output model</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "\n",
       "    <span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">()</span> \n",
       "    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"n\">save_every</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n",
       "    <span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n",
       "    <span class=\"c1\">###########################################################</span>\n",
       "    <span class=\"c1\"># 2C. Resume training at epoch last checkpoint was written</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epoch_start</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">):</span> <span class=\"c1\"># note we start loop at epoch_start defined in code above</span>\n",
       "    <span class=\"c1\">###########################################################</span>\n",
       "        <span class=\"n\">train_loop</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\">###########################################################</span>\n",
       "        <span class=\"c1\"># 2D. Write checkpoints periodically during training</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span><span class=\"o\">%</span><span class=\"n\">save_every</span><span class=\"o\">==</span><span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">-------------------------------&quot;</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">({</span>\n",
       "                <span class=\"s1\">&#39;epoch&#39;</span><span class=\"p\">:</span><span class=\"n\">epoch</span><span class=\"p\">,</span>\n",
       "                <span class=\"s1\">&#39;model_state_dict&#39;</span><span class=\"p\">:</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">state_dict</span><span class=\"p\">(),</span>\n",
       "            <span class=\"p\">},</span> <span class=\"n\">checkpoint_file</span><span class=\"p\">)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Finished saving model</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"c1\">############################################################</span>\n",
       "\n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">destroy_process_group</span><span class=\"p\">()</span>\n",
       "    \n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Done!&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">model</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"c1\">############################################################</span>\n",
       "    <span class=\"c1\"># 4. Remove using the mp.spawn to parallelize code and replace this with a function call, as this is done automatically by torchrun</span>\n",
       "    <span class=\"c1\">############################################################  </span>\n",
       "    <span class=\"n\">main</span><span class=\"p\">()</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k}{as} \\PY{n+nn}{dist}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch} \n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data} \\PY{k+kn}{import} \\PY{n}{DataLoader}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{matplotlib}\\PY{n+nn}{.}\\PY{n+nn}{pyplot} \\PY{k}{as} \\PY{n+nn}{plt}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{multiprocessing} \\PY{k}{as} \\PY{n+nn}{mp}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k+kn}{import} \\PY{n}{DistributedSampler}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torchvision}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn} \\PY{k}{as} \\PY{n+nn}{nn}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional} \\PY{k}{as} \\PY{n+nn}{F}\n",
       "\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{Net}\\PY{p}{(}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Module}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb}{super}\\PY{p}{(}\\PY{n}{Net}\\PY{p}{,} \\PY{n+nb+bp}{self}\\PY{p}{)}\\PY{o}{.}\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{flatten} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Flatten}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{linear\\PYZus{}relu\\PYZus{}stack} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{28}\\PY{o}{*}\\PY{l+m+mi}{28}\\PY{p}{,} \\PY{l+m+mi}{128}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{ReLU}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Dropout}\\PY{p}{(}\\PY{l+m+mf}{0.2}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{10}\\PY{p}{)}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{forward}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{x} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{flatten}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{n}{prob} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{linear\\PYZus{}relu\\PYZus{}stack}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{prob}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Remove rank and world\\PYZus{}size \\PYZhy{}\\PYZhy{} use what\\PYZsq{}s in }\n",
       "\\PY{k}{def} \\PY{n+nf}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{32}\\PY{p}{)}\\PY{p}{:}\n",
       "\n",
       "    \\PY{n}{trainset} \\PY{o}{=} \\PY{n}{torchvision}\\PY{o}{.}\\PY{n}{datasets}\\PY{o}{.}\\PY{n}{MNIST}\\PY{p}{(}\n",
       "                            \\PY{n}{root}\\PY{o}{=}\\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SCRATCH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{data}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,}      \\PY{c+c1}{\\PYZsh{} path to where data is stored}\n",
       "                            \\PY{n}{train}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{,}                                         \\PY{c+c1}{\\PYZsh{} specifies if data is train or test}\n",
       "                            \\PY{n}{download}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{,}                                      \\PY{c+c1}{\\PYZsh{} downloads data if not available at root}\n",
       "                            \\PY{n}{transform}\\PY{o}{=}\\PY{n}{torchvision}\\PY{o}{.}\\PY{n}{transforms}\\PY{o}{.}\\PY{n}{ToTensor}\\PY{p}{(}\\PY{p}{)}         \\PY{c+c1}{\\PYZsh{} trasforms both features and targets accordingly}\n",
       "                            \\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} pass data to the distributed sampler and dataloader }\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{DataLoader}\\PY{p}{(}\\PY{n}{trainset}\\PY{p}{,}\n",
       "                                  \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{False}\\PY{p}{,}\n",
       "                                  \\PY{n}{sampler}\\PY{o}{=}\\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n}{trainset}\\PY{p}{)}\\PY{p}{,}\\PY{c+c1}{\\PYZsh{} num\\PYZus{}replicas=world\\PYZus{}size, rank=rank),}\n",
       "                                  \\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataloader}\n",
       "\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{} 1A. Remove code that sets environment variables as this done for you automatically with torchrun.}\n",
       "\\PY{k}{def} \\PY{n+nf}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}   \\PY{c+c1}{\\PYZsh{}rank, world\\PYZus{}size):}\n",
       "    \\PY{c+c1}{\\PYZsh{} 1B. Instead, use these environment variables set by pytorch and instead of explicitly defining them.}\n",
       "    \\PY{n}{world\\PYZus{}size} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{WORLD\\PYZus{}SIZE}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{set\\PYZus{}device}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}                       \\PY{c+c1}{\\PYZsh{}}\n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{init\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{nccl}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}                   \\PY{c+c1}{\\PYZsh{} backend being used; nccl typically used with distributed GPU training}\n",
       "                            \\PY{n}{rank}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}                \\PY{c+c1}{\\PYZsh{} rank of the current process being used}\n",
       "                            \\PY{n}{world\\PYZus{}size}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}    \\PY{c+c1}{\\PYZsh{} total number of processors being used}\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{parallel} \\PY{k+kn}{import} \\PY{n}{DistributedDataParallel} \\PY{k}{as} \\PY{n}{DDP}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} training loop for one epoch}\n",
       "\\PY{k}{def} \\PY{n+nf}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{size} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\n",
       "    \\PY{k}{for} \\PY{n}{batch}\\PY{p}{,} \\PY{p}{(}\\PY{n}{X}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)} \\PY{o+ow}{in} \\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{dataloader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} transfer data to GPU if available}\n",
       "        \\PY{n}{X} \\PY{o}{=} \\PY{n}{X}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \\PY{n}{y} \\PY{o}{=} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Compute prediction and loss}\n",
       "        \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} Backpropagation}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "        \\PY{k}{if} \\PY{n}{rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{k}{if} \\PY{n}{batch} \\PY{o}{\\PYZpc{}} \\PY{l+m+mi}{100} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "                \\PY{n}{loss}\\PY{p}{,} \\PY{n}{current} \\PY{o}{=} \\PY{n}{loss}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{batch} \\PY{o}{*} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "                \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{loss}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}7f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  [}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{size}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{]}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{main}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 1.B We also create the variable local\\PYZus{}rank in our main function as well as call the new init\\PYZus{}distributed()}\n",
       "    \\PY{c+c1}{\\PYZsh{} this will be used to assign the gpu where our model should reside}\n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}                                                 }\n",
       "    \\PY{c+c1}{\\PYZsh{} A. Create location to store checkpoints}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Create directory for storing checkpointed model}\n",
       "    \\PY{n}{model\\PYZus{}folder\\PYZus{}path} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SCRATCH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cnn4\\PYZus{}mnist\\PYZus{}output\\PYZus{}model}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)} \\PY{c+c1}{\\PYZsh{} create variable for path to folder for checkpoints}\n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{makedirs}\\PY{p}{(}\\PY{n}{model\\PYZus{}folder\\PYZus{}path}\\PY{p}{,}\\PY{n}{exist\\PYZus{}ok}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}                              \\PY{c+c1}{\\PYZsh{} create directory for models if they do not exist}\n",
       "    \\PY{c+c1}{\\PYZsh{} create file name for checkpoint }\n",
       "    \\PY{n}{checkpoint\\PYZus{}file} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{model\\PYZus{}folder\\PYZus{}path}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{best\\PYZus{}model.pt}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}        \\PY{c+c1}{\\PYZsh{} create filename for model checkpoint}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} instantiate network and set to local\\PYZus{}rank device}\n",
       "    \\PY{n}{net} \\PY{o}{=} \\PY{n}{Net}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 2B. Read checkpoints if they exist }\n",
       "    \\PY{k}{if} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{exists}\\PY{p}{(}\\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{checkpoint} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{load}\\PY{p}{(}\\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{,} \\PY{n}{map\\PYZus{}location}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\\PY{p}{)}\n",
       "        \\PY{n}{net}\\PY{o}{.}\\PY{n}{load\\PYZus{}state\\PYZus{}dict}\\PY{p}{(}\\PY{n}{checkpoint}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{model\\PYZus{}state\\PYZus{}dict}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "        \\PY{n}{epoch\\PYZus{}start} \\PY{o}{=} \\PY{n}{checkpoint}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{epoch}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} otherwise we are starting training from the beginning at epoch 0}\n",
       "    \\PY{k}{else}\\PY{p}{:}\n",
       "        \\PY{n}{epoch\\PYZus{}start} \\PY{o}{=} \\PY{l+m+mi}{0}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{DDP}\\PY{p}{(}\\PY{n}{net}\\PY{p}{,}\n",
       "            \\PY{n}{device\\PYZus{}ids}\\PY{o}{=}\\PY{p}{[}\\PY{n}{local\\PYZus{}rank}\\PY{p}{]}\\PY{p}{,}                  \\PY{c+c1}{\\PYZsh{} list of gpu that model lives on }\n",
       "            \\PY{n}{output\\PYZus{}device}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}                 \\PY{c+c1}{\\PYZsh{} where to output model}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "\n",
       "    \\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{CrossEntropyLoss}\\PY{p}{(}\\PY{p}{)} \n",
       "    \\PY{n}{optimizer} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{Adam}\\PY{p}{(}\\PY{n}{model}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{lr}\\PY{o}{=}\\PY{l+m+mf}{0.001}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{save\\PYZus{}every} \\PY{o}{=} \\PY{l+m+mi}{1}\n",
       "    \\PY{n}{epochs} \\PY{o}{=} \\PY{l+m+mi}{10}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 2C. Resume training at epoch last checkpoint was written}\n",
       "    \\PY{k}{for} \\PY{n}{epoch} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epoch\\PYZus{}start}\\PY{p}{,} \\PY{n}{epochs}\\PY{p}{)}\\PY{p}{:} \\PY{c+c1}{\\PYZsh{} note we start loop at epoch\\PYZus{}start defined in code above}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{n}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} 2D. Write checkpoints periodically during training}\n",
       "        \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0} \\PY{o+ow}{and} \\PY{n}{epoch}\\PY{o}{\\PYZpc{}}\\PY{n}{save\\PYZus{}every}\\PY{o}{==}\\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epoch}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{save}\\PY{p}{(}\\PY{p}{\\PYZob{}}\n",
       "                \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{epoch}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\\PY{n}{epoch}\\PY{p}{,}\n",
       "                \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{model\\PYZus{}state\\PYZus{}dict}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\\PY{n}{model}\\PY{o}{.}\\PY{n}{module}\\PY{o}{.}\\PY{n}{state\\PYZus{}dict}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{p}{\\PYZcb{}}\\PY{p}{,} \\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Finished saving model}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{destroy\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Done!}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{model}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 4. Remove using the mp.spawn to parallelize code and replace this with a function call, as this is done automatically by torchrun}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}  }\n",
       "    \\PY{n}{main}\\PY{p}{(}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "import torchvision\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "\n",
       "\n",
       "class Net(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(Net, self).__init__()\n",
       "        self.flatten = torch.nn.Flatten()\n",
       "        self.linear_relu_stack = torch.nn.Sequential(\n",
       "            torch.nn.Linear(28*28, 128),\n",
       "            torch.nn.ReLU(),\n",
       "            torch.nn.Dropout(0.2),\n",
       "            torch.nn.Linear(128, 10),\n",
       "        )\n",
       "\n",
       "    def forward(self, x):\n",
       "        x = self.flatten(x)\n",
       "        prob = self.linear_relu_stack(x)\n",
       "        return prob\n",
       "\n",
       "# Remove rank and world_size -- use what's in \n",
       "def prepare_data(batch_size=32):\n",
       "\n",
       "    trainset = torchvision.datasets.MNIST(\n",
       "                            root=os.path.join(os.environ['SCRATCH'], \"data\"),      # path to where data is stored\n",
       "                            train=True,                                         # specifies if data is train or test\n",
       "                            download=True,                                      # downloads data if not available at root\n",
       "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
       "                            )\n",
       "\n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(trainset,\n",
       "                                  shuffle=False,\n",
       "                                  sampler=DistributedSampler(trainset),# num_replicas=world_size, rank=rank),\n",
       "                                  batch_size=batch_size)\n",
       "\n",
       "    return train_dataloader\n",
       "\n",
       "\n",
       "##################################################################################\n",
       "# 1A. Remove code that sets environment variables as this done for you automatically with torchrun.\n",
       "def init_distributed():   #rank, world_size):\n",
       "    # 1B. Instead, use these environment variables set by pytorch and instead of explicitly defining them.\n",
       "    world_size = int(os.environ['WORLD_SIZE'])\n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "    torch.cuda.set_device(local_rank)                       #\n",
       "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
       "                            rank=local_rank,                # rank of the current process being used\n",
       "                            world_size=world_size)    # total number of processors being used\n",
       "#############################################################\n",
       "\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(rank)\n",
       "        y = y.to(rank)\n",
       "\n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "        \n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        if rank == 0:\n",
       "            if batch % 100 == 0:\n",
       "                loss, current = loss.item(), batch * len(X)\n",
       "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "\n",
       "def main():\n",
       "    #####################################################################\n",
       "    # 1.B We also create the variable local_rank in our main function as well as call the new init_distributed()\n",
       "    # this will be used to assign the gpu where our model should reside\n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "\n",
       "    init_distributed()\n",
       "    ################################################\n",
       "\n",
       "    train_dataloader = prepare_data()\n",
       "\n",
       "    ################################################                                                 \n",
       "    # A. Create location to store checkpoints\n",
       "\n",
       "    # Create directory for storing checkpointed model\n",
       "    model_folder_path = os.path.join(os.environ['SCRATCH'], \"cnn4_mnist_output_model\") # create variable for path to folder for checkpoints\n",
       "    os.makedirs(model_folder_path,exist_ok=True)                              # create directory for models if they do not exist\n",
       "    # create file name for checkpoint \n",
       "    checkpoint_file = os.path.join(model_folder_path, \"best_model.pt\")        # create filename for model checkpoint\n",
       "    ################################################\n",
       "\n",
       "    # instantiate network and set to local_rank device\n",
       "    net = Net().to(local_rank)\n",
       "    \n",
       "    #################################################\n",
       "    # 2B. Read checkpoints if they exist \n",
       "    if os.path.exists(checkpoint_file):\n",
       "        checkpoint = torch.load(checkpoint_file, map_location=torch.device(local_rank))\n",
       "        net.load_state_dict(checkpoint['model_state_dict'])\n",
       "        epoch_start = checkpoint['epoch']\n",
       "    \n",
       "    # otherwise we are starting training from the beginning at epoch 0\n",
       "    else:\n",
       "        epoch_start = 0\n",
       "    ################################################\n",
       "    \n",
       "    model = DDP(net,\n",
       "            device_ids=[local_rank],                  # list of gpu that model lives on \n",
       "            output_device=local_rank,                 # where to output model\n",
       "        )\n",
       "\n",
       "\n",
       "    loss_fn = torch.nn.CrossEntropyLoss() \n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
       "\n",
       "    save_every = 1\n",
       "    epochs = 10\n",
       "    ###########################################################\n",
       "    # 2C. Resume training at epoch last checkpoint was written\n",
       "    for epoch in range(epoch_start, epochs): # note we start loop at epoch_start defined in code above\n",
       "    ###########################################################\n",
       "        train_loop(local_rank, train_dataloader, model, loss_fn, optimizer)\n",
       "        ###########################################################\n",
       "        # 2D. Write checkpoints periodically during training\n",
       "        if local_rank == 0 and epoch%save_every==0:\n",
       "            print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
       "            torch.save({\n",
       "                'epoch':epoch,\n",
       "                'model_state_dict':model.module.state_dict(),\n",
       "            }, checkpoint_file)\n",
       "            print(\"Finished saving model\\n\")\n",
       "    ############################################################\n",
       "\n",
       "    dist.destroy_process_group()\n",
       "    \n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    ############################################################\n",
       "    # 4. Remove using the mp.spawn to parallelize code and replace this with a function call, as this is done automatically by torchrun\n",
       "    ############################################################  \n",
       "    main()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Code(\"cnn_part4/mnist_torchrun.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc-per-node=4 cnn_part4/mnist_torchrun.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e315261",
   "metadata": {},
   "source": [
    "## Additional Exercise\n",
    "\n",
    "Modify `cnn_part4/simple_linear_regression_parallel.py` to be able to use torchrun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fb59c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch.distributed</span> <span class=\"k\">as</span> <span class=\"nn\">dist</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span> \n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span> \n",
       "<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.multiprocessing</span> <span class=\"k\">as</span> <span class=\"nn\">mp</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">DistributedSampler</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.nn.parallel</span> <span class=\"kn\">import</span> <span class=\"n\">DistributedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">DDP</span>\n",
       "\n",
       "\n",
       "<span class=\"c1\">##############################################</span>\n",
       "<span class=\"c1\"># 1. Create a process group</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">init_distributed</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">    </span><span class=\"sd\">&#39;&#39;&#39;</span>\n",
       "<span class=\"sd\">    local_rank: identifier for pariticular GPU on one node</span>\n",
       "<span class=\"sd\">    world: total number of process in a the group</span>\n",
       "<span class=\"sd\">    &#39;&#39;&#39;</span>\n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;MASTER_ADDR&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;localhost&#39;</span>           <span class=\"c1\"># IP address of rank 0 process</span>\n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;MASTER_PORT&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;12355&#39;</span>               <span class=\"c1\"># a free port used to communicate amongst processors</span>\n",
       "    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">set_device</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>                 \n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"s2\">&quot;nccl&quot;</span><span class=\"p\">,</span>                   <span class=\"c1\"># backend being used; nccl typically used with distributed GPU training</span>\n",
       "                            <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>          <span class=\"c1\"># rank of the current process being used</span>\n",
       "                            <span class=\"n\">world_size</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>    <span class=\"c1\"># total number of processors being used</span>\n",
       "<span class=\"c1\">##############################################</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">get_model</span><span class=\"p\">():</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>     <span class=\"c1\"># first number specifies input dimension; second number specifies output dimension</span>\n",
       "            <span class=\"p\">)</span> \n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">prepare_data</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">):</span>\n",
       "    <span class=\"c1\"># Generate random data centered around 10 with noise</span>\n",
       "    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>\n",
       "    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">3</span>\n",
       "    \n",
       "    <span class=\"c1\"># pass data to the distributed sampler and dataloader </span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)),</span>\n",
       "                                  <span class=\"c1\">##############################################</span>\n",
       "                                  <span class=\"c1\"># 2. Use Pytorch&#39;s DistributedSampler to ensure that data passed to each GPU is different</span>\n",
       "                                  <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">))),</span>\n",
       "                                  <span class=\"c1\">##############################################</span>\n",
       "                                  <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataloader</span>\n",
       "\n",
       "<span class=\"c1\"># training loop for one epoch</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">train_loop</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\"># transfer data to GPU if available</span>\n",
       "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"c1\"># Compute prediction and loss</span>\n",
       "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Backpropagation</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "        <span class=\"k\">if</span> <span class=\"n\">batch</span> <span class=\"o\">%</span> <span class=\"mi\">2</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">(),</span> <span class=\"n\">batch</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"si\">:</span><span class=\"s2\">&gt;7f</span><span class=\"si\">}</span><span class=\"s2\">  [</span><span class=\"si\">{</span><span class=\"n\">current</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">size</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">]&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">init_distributed</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">prepare_data</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"c1\">##############################################</span>\n",
       "    <span class=\"c1\"># 3. Wrap Model with Pytorch&#39;s DistributedDataParallel</span>\n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DDP</span><span class=\"p\">(</span><span class=\"n\">get_model</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">),</span> <span class=\"n\">device_ids</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">rank</span><span class=\"p\">],</span> <span class=\"n\">output_device</span><span class=\"o\">=</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "    <span class=\"c1\">##############################################</span>\n",
       "    \n",
       "    <span class=\"c1\"># instantiate loss and optimizer </span>\n",
       "    <span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MSELoss</span><span class=\"p\">(</span><span class=\"n\">reduction</span><span class=\"o\">=</span><span class=\"s1\">&#39;mean&#39;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># Train Model </span>\n",
       "    <span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"c1\"># 4. Only write/print model information on one GPU</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Epoch </span><span class=\"si\">{</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">-------------------------------&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"n\">train_loop</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "    <span class=\"c1\"># 5. Close Process Group</span>\n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">destroy_process_group</span><span class=\"p\">()</span>\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Done!&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">model</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">world_size</span><span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n",
       "    <span class=\"n\">mp</span><span class=\"o\">.</span><span class=\"n\">spawn</span><span class=\"p\">(</span><span class=\"n\">main</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">world_size</span><span class=\"p\">,)</span> <span class=\"p\">,</span> <span class=\"n\">nprocs</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k}{as} \\PY{n+nn}{dist}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch} \n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data} \\PY{k+kn}{import} \\PY{n}{DataLoader}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np} \n",
       "\\PY{k+kn}{import} \\PY{n+nn}{matplotlib}\\PY{n+nn}{.}\\PY{n+nn}{pyplot} \\PY{k}{as} \\PY{n+nn}{plt}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{multiprocessing} \\PY{k}{as} \\PY{n+nn}{mp}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k+kn}{import} \\PY{n}{DistributedSampler}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{parallel} \\PY{k+kn}{import} \\PY{n}{DistributedDataParallel} \\PY{k}{as} \\PY{n}{DDP}\n",
       "\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{} 1. Create a process group}\n",
       "\\PY{k}{def} \\PY{n+nf}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{    }\\PY{l+s+sd}{\\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "\\PY{l+s+sd}{    local\\PYZus{}rank: identifier for pariticular GPU on one node}\n",
       "\\PY{l+s+sd}{    world: total number of process in a the group}\n",
       "\\PY{l+s+sd}{    \\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MASTER\\PYZus{}ADDR}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{localhost}\\PY{l+s+s1}{\\PYZsq{}}           \\PY{c+c1}{\\PYZsh{} IP address of rank 0 process}\n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MASTER\\PYZus{}PORT}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{12355}\\PY{l+s+s1}{\\PYZsq{}}               \\PY{c+c1}{\\PYZsh{} a free port used to communicate amongst processors}\n",
       "    \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{set\\PYZus{}device}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}                 \n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{init\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{nccl}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}                   \\PY{c+c1}{\\PYZsh{} backend being used; nccl typically used with distributed GPU training}\n",
       "                            \\PY{n}{rank}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}          \\PY{c+c1}{\\PYZsh{} rank of the current process being used}\n",
       "                            \\PY{n}{world\\PYZus{}size}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}    \\PY{c+c1}{\\PYZsh{} total number of processors being used}\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{return} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)}\\PY{p}{,}     \\PY{c+c1}{\\PYZsh{} first number specifies input dimension; second number specifies output dimension}\n",
       "            \\PY{p}{)} \n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{32}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{} Generate random data centered around 10 with noise}\n",
       "    \\PY{n}{X} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{10}\n",
       "    \\PY{n}{y} \\PY{o}{=} \\PY{n}{X} \\PY{o}{+} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{3}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} pass data to the distributed sampler and dataloader }\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{DataLoader}\\PY{p}{(}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{zip}\\PY{p}{(}\\PY{n}{X}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "                                  \\PY{c+c1}{\\PYZsh{} 2. Use Pytorch\\PYZsq{}s DistributedSampler to ensure that data passed to each GPU is different}\n",
       "                                  \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{False}\\PY{p}{,}\n",
       "                                  \\PY{n}{sampler}\\PY{o}{=}\\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{zip}\\PY{p}{(}\\PY{n}{X}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "                                  \\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataloader}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} training loop for one epoch}\n",
       "\\PY{k}{def} \\PY{n+nf}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{size} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\n",
       "    \\PY{k}{for} \\PY{n}{batch}\\PY{p}{,} \\PY{p}{(}\\PY{n}{X}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)} \\PY{o+ow}{in} \\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{dataloader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} transfer data to GPU if available}\n",
       "        \\PY{n}{X} \\PY{o}{=} \\PY{n}{X}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \\PY{n}{y} \\PY{o}{=} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} Compute prediction and loss}\n",
       "        \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Backpropagation}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "        \\PY{k}{if} \\PY{n}{batch} \\PY{o}{\\PYZpc{}} \\PY{l+m+mi}{2} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n}{loss}\\PY{p}{,} \\PY{n}{current} \\PY{o}{=} \\PY{n}{loss}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{batch} \\PY{o}{*} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{loss}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}7f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  [}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{size}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{]}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{main}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 3. Wrap Model with Pytorch\\PYZsq{}s DistributedDataParallel}\n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{DDP}\\PY{p}{(}\\PY{n}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\\PY{p}{,} \\PY{n}{device\\PYZus{}ids}\\PY{o}{=}\\PY{p}{[}\\PY{n}{rank}\\PY{p}{]}\\PY{p}{,} \\PY{n}{output\\PYZus{}device}\\PY{o}{=}\\PY{n}{rank}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} instantiate loss and optimizer }\n",
       "    \\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{MSELoss}\\PY{p}{(}\\PY{n}{reduction}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{mean}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "    \\PY{n}{optimizer} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{Adam}\\PY{p}{(}\\PY{n}{model}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{lr}\\PY{o}{=}\\PY{l+m+mf}{0.1}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Train Model }\n",
       "    \\PY{n}{epochs} \\PY{o}{=} \\PY{l+m+mi}{20}\n",
       "    \\PY{k}{for} \\PY{n}{t} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epochs}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} 4. Only write/print model information on one GPU}\n",
       "        \\PY{k}{if} \\PY{n}{rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{t}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{n}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 5. Close Process Group}\n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{destroy\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Done!}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{model}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{n}{world\\PYZus{}size}\\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{device\\PYZus{}count}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{n}{mp}\\PY{o}{.}\\PY{n}{spawn}\\PY{p}{(}\\PY{n}{main}\\PY{p}{,} \\PY{n}{args}\\PY{o}{=}\\PY{p}{(}\\PY{n}{world\\PYZus{}size}\\PY{p}{,}\\PY{p}{)} \\PY{p}{,} \\PY{n}{nprocs}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "\n",
       "##############################################\n",
       "# 1. Create a process group\n",
       "def init_distributed(local_rank, world_size):\n",
       "    '''\n",
       "    local_rank: identifier for pariticular GPU on one node\n",
       "    world: total number of process in a the group\n",
       "    '''\n",
       "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
       "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
       "    torch.cuda.set_device(local_rank)                 \n",
       "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
       "                            rank=local_rank,          # rank of the current process being used\n",
       "                            world_size=world_size)    # total number of processors being used\n",
       "##############################################\n",
       "\n",
       "def get_model():\n",
       "    return torch.nn.Sequential(\n",
       "            torch.nn.Linear(1, 1),     # first number specifies input dimension; second number specifies output dimension\n",
       "            ) \n",
       "\n",
       "def prepare_data(batch_size=32):\n",
       "    # Generate random data centered around 10 with noise\n",
       "    X = torch.randn(32*4, 1) * 10\n",
       "    y = X + torch.randn(32*4, 1) * 3\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(list(zip(X,y)),\n",
       "                                  ##############################################\n",
       "                                  # 2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
       "                                  shuffle=False,\n",
       "                                  sampler=DistributedSampler(list(zip(X,y))),\n",
       "                                  ##############################################\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(rank)\n",
       "        y = y.to(rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "\n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        if batch % 2 == 0:\n",
       "            loss, current = loss.item(), batch * len(X)\n",
       "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "\n",
       "def main(rank, world_size):\n",
       "    init_distributed(rank, world_size)\n",
       "\n",
       "    train_dataloader = prepare_data()\n",
       "\n",
       "    ##############################################\n",
       "    # 3. Wrap Model with Pytorch's DistributedDataParallel\n",
       "    model = DDP(get_model().to(rank), device_ids=[rank], output_device=rank)\n",
       "    ##############################################\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 20\n",
       "    for t in range(epochs):\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if rank == 0:\n",
       "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        ################################################\n",
       "        train_loop(rank, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    #################################################\n",
       "    # 5. Close Process Group\n",
       "    dist.destroy_process_group()\n",
       "    #################################################\n",
       "\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    world_size= torch.cuda.device_count()\n",
       "    mp.spawn(main, args=(world_size,) , nprocs=world_size)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Code(\"cnn_part4/simple_linear_regression_parallel.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be9158",
   "metadata": {},
   "source": [
    "Below is the modified version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24de4c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch.distributed</span> <span class=\"k\">as</span> <span class=\"nn\">dist</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span> \n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data</span> <span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span> \n",
       "<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.multiprocessing</span> <span class=\"k\">as</span> <span class=\"nn\">mp</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">DistributedSampler</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.nn.parallel</span> <span class=\"kn\">import</span> <span class=\"n\">DistributedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">DDP</span>\n",
       "\n",
       "\n",
       "<span class=\"c1\">##############################################</span>\n",
       "<span class=\"c1\"># 1. Create a process group</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">init_distributed</span><span class=\"p\">():</span>\n",
       "<span class=\"w\">    </span><span class=\"sd\">&#39;&#39;&#39;</span>\n",
       "<span class=\"sd\">    local_rank: identifier for pariticular GPU on one node</span>\n",
       "<span class=\"sd\">    world: total number of process in a the group</span>\n",
       "<span class=\"sd\">    &#39;&#39;&#39;</span>\n",
       "    <span class=\"n\">world_size</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;WORLD_SIZE&#39;</span><span class=\"p\">])</span>\n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">set_device</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>                 \n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"s2\">&quot;nccl&quot;</span><span class=\"p\">,</span>                   <span class=\"c1\"># backend being used; nccl typically used with distributed GPU training</span>\n",
       "                            <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>          <span class=\"c1\"># rank of the current process being used</span>\n",
       "                            <span class=\"n\">world_size</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>    <span class=\"c1\"># total number of processors being used</span>\n",
       "<span class=\"c1\">##############################################</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">get_model</span><span class=\"p\">():</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>     <span class=\"c1\"># first number specifies input dimension; second number specifies output dimension</span>\n",
       "            <span class=\"p\">)</span> \n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">prepare_data</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">):</span>\n",
       "    <span class=\"c1\"># Generate random data centered around 10 with noise</span>\n",
       "    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>\n",
       "    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">3</span>\n",
       "    \n",
       "    <span class=\"c1\"># pass data to the distributed sampler and dataloader </span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)),</span>\n",
       "                                  <span class=\"c1\">##############################################</span>\n",
       "                                  <span class=\"c1\"># 2. Use Pytorch&#39;s DistributedSampler to ensure that data passed to each GPU is different</span>\n",
       "                                  <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">))),</span>\n",
       "                                  <span class=\"c1\">##############################################</span>\n",
       "                                  <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataloader</span>\n",
       "\n",
       "<span class=\"c1\"># training loop for one epoch</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">train_loop</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\"># transfer data to GPU if available</span>\n",
       "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"c1\"># Compute prediction and loss</span>\n",
       "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Backpropagation</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "        <span class=\"k\">if</span> <span class=\"n\">batch</span> <span class=\"o\">%</span> <span class=\"mi\">2</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">(),</span> <span class=\"n\">batch</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"si\">:</span><span class=\"s2\">&gt;7f</span><span class=\"si\">}</span><span class=\"s2\">  [</span><span class=\"si\">{</span><span class=\"n\">current</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">size</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">]&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "\n",
       "    <span class=\"n\">init_distributed</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">prepare_data</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"c1\">##############################################</span>\n",
       "    <span class=\"c1\"># 3. Wrap Model with Pytorch&#39;s DistributedDataParallel</span>\n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DDP</span><span class=\"p\">(</span><span class=\"n\">get_model</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">),</span> <span class=\"n\">device_ids</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">local_rank</span><span class=\"p\">],</span> <span class=\"n\">output_device</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n",
       "    <span class=\"c1\">##############################################</span>\n",
       "    \n",
       "    <span class=\"c1\"># instantiate loss and optimizer </span>\n",
       "    <span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MSELoss</span><span class=\"p\">(</span><span class=\"n\">reduction</span><span class=\"o\">=</span><span class=\"s1\">&#39;mean&#39;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># Train Model </span>\n",
       "    <span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"c1\"># 4. Only write/print model information on one GPU</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Epoch </span><span class=\"si\">{</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">-------------------------------&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"n\">train_loop</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "    <span class=\"c1\"># 5. Close Process Group</span>\n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">destroy_process_group</span><span class=\"p\">()</span>\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Done!&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">model</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">main</span><span class=\"p\">()</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k}{as} \\PY{n+nn}{dist}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch} \n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data} \\PY{k+kn}{import} \\PY{n}{DataLoader}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np} \n",
       "\\PY{k+kn}{import} \\PY{n+nn}{matplotlib}\\PY{n+nn}{.}\\PY{n+nn}{pyplot} \\PY{k}{as} \\PY{n+nn}{plt}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{multiprocessing} \\PY{k}{as} \\PY{n+nn}{mp}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k+kn}{import} \\PY{n}{DistributedSampler}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{parallel} \\PY{k+kn}{import} \\PY{n}{DistributedDataParallel} \\PY{k}{as} \\PY{n}{DDP}\n",
       "\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{} 1. Create a process group}\n",
       "\\PY{k}{def} \\PY{n+nf}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{    }\\PY{l+s+sd}{\\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "\\PY{l+s+sd}{    local\\PYZus{}rank: identifier for pariticular GPU on one node}\n",
       "\\PY{l+s+sd}{    world: total number of process in a the group}\n",
       "\\PY{l+s+sd}{    \\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "    \\PY{n}{world\\PYZus{}size} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{WORLD\\PYZus{}SIZE}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{set\\PYZus{}device}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}                 \n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{init\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{nccl}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}                   \\PY{c+c1}{\\PYZsh{} backend being used; nccl typically used with distributed GPU training}\n",
       "                            \\PY{n}{rank}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}          \\PY{c+c1}{\\PYZsh{} rank of the current process being used}\n",
       "                            \\PY{n}{world\\PYZus{}size}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}    \\PY{c+c1}{\\PYZsh{} total number of processors being used}\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{return} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)}\\PY{p}{,}     \\PY{c+c1}{\\PYZsh{} first number specifies input dimension; second number specifies output dimension}\n",
       "            \\PY{p}{)} \n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{32}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{} Generate random data centered around 10 with noise}\n",
       "    \\PY{n}{X} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{10}\n",
       "    \\PY{n}{y} \\PY{o}{=} \\PY{n}{X} \\PY{o}{+} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{3}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} pass data to the distributed sampler and dataloader }\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{DataLoader}\\PY{p}{(}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{zip}\\PY{p}{(}\\PY{n}{X}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "                                  \\PY{c+c1}{\\PYZsh{} 2. Use Pytorch\\PYZsq{}s DistributedSampler to ensure that data passed to each GPU is different}\n",
       "                                  \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{False}\\PY{p}{,}\n",
       "                                  \\PY{n}{sampler}\\PY{o}{=}\\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{zip}\\PY{p}{(}\\PY{n}{X}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "                                  \\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataloader}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} training loop for one epoch}\n",
       "\\PY{k}{def} \\PY{n+nf}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{size} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\n",
       "    \\PY{k}{for} \\PY{n}{batch}\\PY{p}{,} \\PY{p}{(}\\PY{n}{X}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)} \\PY{o+ow}{in} \\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{dataloader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} transfer data to GPU if available}\n",
       "        \\PY{n}{X} \\PY{o}{=} \\PY{n}{X}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \\PY{n}{y} \\PY{o}{=} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} Compute prediction and loss}\n",
       "        \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Backpropagation}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "        \\PY{k}{if} \\PY{n}{batch} \\PY{o}{\\PYZpc{}} \\PY{l+m+mi}{2} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n}{loss}\\PY{p}{,} \\PY{n}{current} \\PY{o}{=} \\PY{n}{loss}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{batch} \\PY{o}{*} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{loss}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}7f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  [}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{size}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{]}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{main}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 3. Wrap Model with Pytorch\\PYZsq{}s DistributedDataParallel}\n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{DDP}\\PY{p}{(}\\PY{n}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\\PY{p}{,} \\PY{n}{device\\PYZus{}ids}\\PY{o}{=}\\PY{p}{[}\\PY{n}{local\\PYZus{}rank}\\PY{p}{]}\\PY{p}{,} \\PY{n}{output\\PYZus{}device}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} instantiate loss and optimizer }\n",
       "    \\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{MSELoss}\\PY{p}{(}\\PY{n}{reduction}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{mean}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "    \\PY{n}{optimizer} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{Adam}\\PY{p}{(}\\PY{n}{model}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{lr}\\PY{o}{=}\\PY{l+m+mf}{0.1}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Train Model }\n",
       "    \\PY{n}{epochs} \\PY{o}{=} \\PY{l+m+mi}{20}\n",
       "    \\PY{k}{for} \\PY{n}{t} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epochs}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} 4. Only write/print model information on one GPU}\n",
       "        \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{t}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{n}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 5. Close Process Group}\n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{destroy\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Done!}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{model}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{n}{main}\\PY{p}{(}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "\n",
       "##############################################\n",
       "# 1. Create a process group\n",
       "def init_distributed():\n",
       "    '''\n",
       "    local_rank: identifier for pariticular GPU on one node\n",
       "    world: total number of process in a the group\n",
       "    '''\n",
       "    world_size = int(os.environ['WORLD_SIZE'])\n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "    torch.cuda.set_device(local_rank)                 \n",
       "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
       "                            rank=local_rank,          # rank of the current process being used\n",
       "                            world_size=world_size)    # total number of processors being used\n",
       "##############################################\n",
       "\n",
       "def get_model():\n",
       "    return torch.nn.Sequential(\n",
       "            torch.nn.Linear(1, 1),     # first number specifies input dimension; second number specifies output dimension\n",
       "            ) \n",
       "\n",
       "def prepare_data(batch_size=32):\n",
       "    # Generate random data centered around 10 with noise\n",
       "    X = torch.randn(32*4, 1) * 10\n",
       "    y = X + torch.randn(32*4, 1) * 3\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(list(zip(X,y)),\n",
       "                                  ##############################################\n",
       "                                  # 2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
       "                                  shuffle=False,\n",
       "                                  sampler=DistributedSampler(list(zip(X,y))),\n",
       "                                  ##############################################\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(rank)\n",
       "        y = y.to(rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "\n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        if batch % 2 == 0:\n",
       "            loss, current = loss.item(), batch * len(X)\n",
       "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "\n",
       "def main():\n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "\n",
       "    init_distributed()\n",
       "\n",
       "    train_dataloader = prepare_data()\n",
       "\n",
       "    ##############################################\n",
       "    # 3. Wrap Model with Pytorch's DistributedDataParallel\n",
       "    model = DDP(get_model().to(local_rank), device_ids=[local_rank], output_device=local_rank)\n",
       "    ##############################################\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 20\n",
       "    for t in range(epochs):\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if local_rank == 0:\n",
       "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        ################################################\n",
       "        train_loop(local_rank, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    #################################################\n",
       "    # 5. Close Process Group\n",
       "    dist.destroy_process_group()\n",
       "    #################################################\n",
       "\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Code(\"cnn_part4/simple_linear_regression_parallel_torchrun.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc-per-node=4 cnn_part4/simple_linear_regression_parallel_torchrun.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea86fe9",
   "metadata": {},
   "source": [
    "## DesignSafe Classifier \n",
    "### Reused code from Part 1 and 2 \n",
    "Below are a set of functions and import statements that can be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to our data.\n",
    "# The datasets transformations are the same as the ones from part 2 of this tutorial.\n",
    "def load_datasets(train_path, val_path, test_path):\n",
    "    val_img_transform = transforms.Compose([transforms.Resize((244,244)),\n",
    "                                             transforms.ToTensor()])\n",
    "    train_img_transform = transforms.Compose([transforms.AutoAugment(),\n",
    "                                               transforms.Resize((244,244)),\n",
    "                                               transforms.ToTensor()])\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform=train_img_transform)\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=val_img_transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform=val_img_transform) if test_path is not None else None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Building the Neural Network\n",
    "def getResNet():\n",
    "    resnet = models.resnet34(weights='IMAGENET1K_V1')\n",
    "\n",
    "    # Fix the conv layers parameters\n",
    "    for conv_param in resnet.parameters():\n",
    "        conv_param.require_grad = False\n",
    "\n",
    "    # get the input dimension for this layer\n",
    "    num_ftrs = resnet.fc.in_features\n",
    "\n",
    "    # build the new final mlp layers of network\n",
    "    fc = nn.Sequential(\n",
    "          nn.Linear(num_ftrs, num_ftrs),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(num_ftrs, 3)\n",
    "        )\n",
    "   \n",
    "    # replace final fully connected layer\n",
    "    resnet.fc = fc\n",
    "    return resnet\n",
    "\n",
    "# Model evaluation.\n",
    "@torch.no_grad()\n",
    "def eval_model(data_loader, model, loss_fn, DEVICE):\n",
    "    model.train(False)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0.0, 0.0\n",
    "    n = len(data_loader)\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        x,y = data\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        pred = model(x)\n",
    "        loss += loss_fn(pred, y)/len(x)\n",
    "        pred_label = torch.argmax(pred, axis = 1)\n",
    "        accuracy += torch.sum(pred_label == y)/len(x)\n",
    "\n",
    "    return loss/n, accuracy/n\n",
    "\n",
    "# loading checkpoint\n",
    "def load_checkpoint(checkpoint_path, DEVICE):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    return checkpoint\n",
    "\n",
    "def load_model_fm_checkpoint(checkpoint, primitive_model):\n",
    "    primitive_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return primitive_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d665d11",
   "metadata": {},
   "source": [
    "### Setup Process Group (1 and 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f64ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_distributed():\n",
    "    '''\n",
    "    set up process group with torchrun's environment variables\n",
    "    '''\n",
    "    # 1, 6 use os to get rank and world size\n",
    "    dist_url = \"env://\"\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    dist.init_process_group(backend=\"nccl\", # \"nccl\" for using GPUs, \"gloo\" for using CPUs\n",
    "                          init_method=dist_url,\n",
    "                          world_size=world_size,\n",
    "                          rank=local_rank)\n",
    "    torch.cuda.set_device(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc61024",
   "metadata": {},
   "source": [
    "### Create Data DistributedSampler (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12044b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataloaders(train_set, val_set, test_set, batch_size, shuffle=True):\n",
    "    ##########################################################################################\n",
    "    # 2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
    "\n",
    "    # create distributedsampler for train, validation and test sets\n",
    "    train_sampler = DistributedSampler(dataset=train_set,shuffle=shuffle)\n",
    "    val_sampler = DistributedSampler(dataset=val_set, shuffle=False)\n",
    "    test_sampler = DistributedSampler(dataset=test_set, shuffle=False) if test_set is not None else None\n",
    "\n",
    "    # pass distributedsampler for train, validation and test sets into DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,sampler=train_sampler,num_workers=4,pin_memory=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set,batch_size=batch_size,sampler=val_sampler,num_workers=4)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_set, batch_size, sampler=test_sampler,num_workers=4) if test_set is not None else None\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518ab26",
   "metadata": {},
   "source": [
    "### Write Checkpoints periodically during training and only from one device (4, 7C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, opt, scheduler, loss_fn, epochs, DEVICE, checkpoint_file, prev_best_val_acc):\n",
    "    n = len(train_loader)\n",
    "\n",
    "    best_val_acc = torch.tensor(0.0).to(DEVICE) if prev_best_val_acc is None else prev_best_val_acc\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train(True)\n",
    "\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        avg_loss, val_loss, val_acc, avg_acc  = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred,y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            avg_loss += loss.item()/len(x)\n",
    "            pred_label = torch.argmax(pred, axis=1)\n",
    "            avg_acc += torch.sum(pred_label == y)/len(x)\n",
    "\n",
    "        val_loss, val_acc = eval_model(val_loader, model, loss_fn, DEVICE)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        total_time = torch.tensor((end_time-start_time).seconds).to(DEVICE)\n",
    "\n",
    "        # Learning rate reducer takes action\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        avg_loss, avg_acc = avg_loss/n, avg_acc/n\n",
    "\n",
    "        ###############################################################################\n",
    "        # 4. Modify Training Loop to write model from one GPU     #####################\n",
    "        # 7C. Write checkpoints periodically throughout training. #####################\n",
    "        local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        # Only machine rank==0 (master machine) saves the model and prints the metrics    \n",
    "        if local_rank == 0:\n",
    "\n",
    "          # Save the best model that has the highest val accuracy\n",
    "            if val_acc.item() > best_val_acc.item():\n",
    "                print(f'lr for this epoch is {scheduler.get_last_lr()}')\n",
    "                print(f\"\\nPrev Best Val Acc: {best_val_acc} < Cur Val Acc: {val_acc}\")\n",
    "                print(\"Saving the new best model...\")\n",
    "                torch.save({\n",
    "                    'epoch':epoch,\n",
    "                    'machine':local_rank,\n",
    "                    'model_state_dict':model.module.state_dict(),\n",
    "                    'accuracy':val_acc,\n",
    "                    'loss':val_loss\n",
    "                }, checkpoint_file)\n",
    "                best_val_acc = val_acc\n",
    "                print(\"Finished saving model\\n\")\n",
    "\n",
    "            # Print the metrics (should be same on all machines)\n",
    "            print(f\"\\n(Epoch {epoch+1}/{epochs}) Time: {total_time}s\")\n",
    "            print(f\"(Epoch {epoch+1}/{epochs}) Average train loss: {avg_loss}, Average train accuracy: {avg_acc}\")\n",
    "            print(f\"(Epoch {epoch+1}/{epochs}) Val loss: {val_loss}, Val accuracy: {val_acc}\")\n",
    "            print(f\"(Epoch {epoch+1}/{epochs}) Current best val acc: {best_val_acc}\\n\")\n",
    "        ###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f95fb",
   "metadata": {},
   "source": [
    "### Create Clean Up Function (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    print(\"Cleaning up the distributed environment...\")\n",
    "    dist.destroy_process_group()\n",
    "    print(\"Distributed environment has been properly closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4a96c",
   "metadata": {},
   "source": [
    "### Wrap Model with DDP and put everything together in main function (3, 6B, 7A, 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    hp = {\"lr\":1e-4, \"batch_size\":16, \"epochs\":5}\n",
    "    train_path = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Train/\")\n",
    "    val_path   = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Validation/\")\n",
    "    test_path  = None\n",
    "\n",
    "    #################################################\n",
    "    # 6B. Use pytorch's enviornment variables.  #####\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    #################################################\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "    ###########################################################\n",
    "    # 7A. create location to store checkpoints if they did not exist. ##\n",
    "    \n",
    "    model_folder_path = os.path.join(os.environ['SCRATCH'], \"cnn4_damagelevel_output_model\") \n",
    "    os.makedirs(model_folder_path,exist_ok=True)\n",
    "    ###########################################################\n",
    "   \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(DEVICE)\n",
    "    train_set, val_set, test_set = load_datasets(train_path, val_path, test_path)\n",
    "    train_dataloader, val_dataloader, test_dataloader = construct_dataloaders(train_set, val_set, test_set, hp[\"batch_size\"], True)\n",
    "\n",
    "    model = getResNet().to(DEVICE)\n",
    "    \n",
    "    \n",
    "    ######################################################################################\n",
    "    # 7B, Read check point if it exists and pass to the train function to resume training##\n",
    "    prev_best_val_acc = None\n",
    "    checkpoint_file = os.path.join(model_folder_path, \"best_model.pt\")\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        prev_best_val_acc = checkpoint['accuracy']\n",
    "        model = load_model_fm_checkpoint(checkpoint,model)\n",
    "        epoch_start = checkpoint['epoch']\n",
    "        if rank == 0:\n",
    "            print(f\"resuming training from epoch {epoch_start}\")\n",
    "        else:\n",
    "            epoch_start = 0\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    ##########################################################################\n",
    "    # 3. Wrap model with DDP #################################################\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "    ##########################################################################\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=hp[\"lr\"])\n",
    "\n",
    "\n",
    "\n",
    "    # same learning rate scheduler as part 2\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',factor=0.1, patience=5, min_lr=1e-8)\n",
    "\n",
    "    train(train_dataloader, val_dataloader, model, opt, scheduler, loss_fn, hp[\"epochs\"], DEVICE, checkpoint_file, prev_best_val_acc)\n",
    "\n",
    "    # only the node with rank 0 does the loading, evaluation and printing to avoild duplicate \n",
    "    if local_rank == 0:\n",
    "        # store and print info on the best model at the end of training\n",
    "        primitive_model = getResNet().to(DEVICE)\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        best_model = load_model_fm_checkpoint(checkpoint,primitive_model)\n",
    "        loss, acc = eval_model(val_dataloader,best_model,loss_fn,DEVICE)\n",
    "        print(f\"\\nBest model (val loss: {loss}, val accuracy: {acc}) has been saved to {checkpoint_file}\\n\")\n",
    "        ###############################\n",
    "        # 5. close process group ######\n",
    "        cleanup()\n",
    "        ###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5846857",
   "metadata": {},
   "source": [
    "Copy the DesignSafe dataset to your `$SCRATCH`. If you had already copied the Dataset into your `$SCRATCH` folder (`$SCRATCH/Dataset_2`), you do not need to execute the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861757c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /scratch1/07980/sli4/training/cnn_course/data/data.tar.gz $SCRATCH\n",
    "! tar zxf $SCRATCH/data.tar.gz -C $SCRATCH\n",
    "! ls $SCRATCH/Dataset_2\n",
    "! rm $SCRATCH/data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66a08a",
   "metadata": {},
   "source": [
    "Launch the job with torchrun to train the designsafe classifier on a single node and 3 GPUs. Remove the previous model from $SCRATCH if you would like to start from epoch 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5df788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># NOTE: This is the main script of using Distributed Data Parallel to train ResNet</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">gc</span>\n",
       "\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torchvision</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torchvision</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span><span class=\"p\">,</span> <span class=\"n\">models</span><span class=\"p\">,</span> <span class=\"n\">transforms</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"nn\">nn</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">torch.utils.data.distributed</span> <span class=\"kn\">import</span> <span class=\"n\">DistributedSampler</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.distributed</span> <span class=\"k\">as</span> <span class=\"nn\">dist</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">datetime</span> <span class=\"kn\">import</span> <span class=\"n\">datetime</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">warnings</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">shutil</span>\n",
       "\n",
       "<span class=\"n\">warnings</span><span class=\"o\">.</span><span class=\"n\">filterwarnings</span><span class=\"p\">(</span><span class=\"s2\">&quot;ignore&quot;</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"o\">=</span><span class=\"s2\">&quot;torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"c1\"># Define the GPUs that will be used in this script</span>\n",
       "<span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;,&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">())))</span>\n",
       "\n",
       "<span class=\"c1\"># Apply transformations to our data.</span>\n",
       "<span class=\"c1\"># The datasets transformations are the same as the ones from part 2 of this tutorial.</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">load_datasets</span><span class=\"p\">(</span><span class=\"n\">train_path</span><span class=\"p\">,</span> <span class=\"n\">val_path</span><span class=\"p\">,</span> <span class=\"n\">test_path</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">val_img_transform</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Resize</span><span class=\"p\">((</span><span class=\"mi\">244</span><span class=\"p\">,</span><span class=\"mi\">244</span><span class=\"p\">)),</span>\n",
       "                                         <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">()])</span>\n",
       "    <span class=\"n\">train_img_transform</span> <span class=\"o\">=</span> <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">([</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">AutoAugment</span><span class=\"p\">(),</span>\n",
       "                                           <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Resize</span><span class=\"p\">((</span><span class=\"mi\">244</span><span class=\"p\">,</span><span class=\"mi\">244</span><span class=\"p\">)),</span>\n",
       "                                           <span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">()])</span>\n",
       "    <span class=\"n\">train_dataset</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span><span class=\"p\">(</span><span class=\"n\">train_path</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">train_img_transform</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">val_dataset</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span><span class=\"p\">(</span><span class=\"n\">val_path</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">val_img_transform</span><span class=\"p\">)</span> \n",
       "    <span class=\"n\">test_dataset</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span><span class=\"p\">(</span><span class=\"n\">test_path</span><span class=\"p\">,</span> <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">val_img_transform</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">test_path</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span> <span class=\"k\">else</span> <span class=\"kc\">None</span>\n",
       "  \n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "    <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Train set size: </span><span class=\"si\">{</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_dataset</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">, Validation set size: </span><span class=\"si\">{</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">val_dataset</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataset</span><span class=\"p\">,</span> <span class=\"n\">val_dataset</span><span class=\"p\">,</span> <span class=\"n\">test_dataset</span>\n",
       "\n",
       "<span class=\"c1\"># Construct Dataloaders</span>\n",
       "<span class=\"c1\"># The DistributedSampler we use here restricts data loading to a subset of the dataset.</span>\n",
       "<span class=\"c1\"># In conjunction with DistributedDataParallel (shows up later in this tutorial), each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">construct_dataloaders</span><span class=\"p\">(</span><span class=\"n\">train_set</span><span class=\"p\">,</span> <span class=\"n\">val_set</span><span class=\"p\">,</span> <span class=\"n\">test_set</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>  \n",
       "    <span class=\"n\">train_sampler</span> <span class=\"o\">=</span> <span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">train_set</span><span class=\"p\">,</span><span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"n\">shuffle</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">val_sampler</span> <span class=\"o\">=</span> <span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">val_set</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">test_sampler</span> <span class=\"o\">=</span> <span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">test_set</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">test_set</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span> <span class=\"k\">else</span> <span class=\"kc\">None</span>\n",
       "  \n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">train_set</span><span class=\"p\">,</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span><span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">train_sampler</span><span class=\"p\">,</span><span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"n\">pin_memory</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">val_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">val_set</span><span class=\"p\">,</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">,</span><span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">val_sampler</span><span class=\"p\">,</span><span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">test_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">test_aset</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">test_sampler</span><span class=\"p\">,</span><span class=\"n\">num_workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">test_set</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span> <span class=\"k\">else</span> <span class=\"kc\">None</span>\n",
       "    \n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">val_dataloader</span><span class=\"p\">,</span> <span class=\"n\">test_dataloader</span>\n",
       "\n",
       "\n",
       "<span class=\"c1\"># Building the Neural Network</span>\n",
       "<span class=\"c1\"># This is the same from part 2 of this tutorial</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">getResNet</span><span class=\"p\">():</span>\n",
       "    <span class=\"n\">resnet</span> <span class=\"o\">=</span> <span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">resnet34</span><span class=\"p\">(</span><span class=\"n\">weights</span><span class=\"o\">=</span><span class=\"s1\">&#39;IMAGENET1K_V1&#39;</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># Fix the conv layers parameters</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">conv_param</span> <span class=\"ow\">in</span> <span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">():</span>\n",
       "        <span class=\"n\">conv_param</span><span class=\"o\">.</span><span class=\"n\">require_grad</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n",
       "\n",
       "    <span class=\"c1\"># get the input dimension for this layer</span>\n",
       "    <span class=\"n\">num_ftrs</span> <span class=\"o\">=</span> <span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">fc</span><span class=\"o\">.</span><span class=\"n\">in_features</span>\n",
       "    \n",
       "    <span class=\"c1\"># build the new final mlp layers of network</span>\n",
       "    <span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "          <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">num_ftrs</span><span class=\"p\">,</span> <span class=\"n\">num_ftrs</span><span class=\"p\">),</span>\n",
       "          <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n",
       "          <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">num_ftrs</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>\n",
       "        <span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"c1\"># replace final fully connected layer</span>\n",
       "    <span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">fc</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">resnet</span>\n",
       "\n",
       "\n",
       "<span class=\"c1\"># Model evaluation.</span>\n",
       "<span class=\"c1\"># This is implemented in the same way as part 2 of this tutorial.</span>\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">eval_model</span><span class=\"p\">(</span><span class=\"n\">data_loader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">DEVICE</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"kc\">False</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"p\">()</span>\n",
       "    <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span>\n",
       "    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">data_loader</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "\n",
       "    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">data</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">data_loader</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">data</span>\n",
       "        <span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">),</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">pred_label</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">accuracy</span> <span class=\"o\">+=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">pred_label</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"o\">/</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"o\">/</span><span class=\"n\">n</span>\n",
       "\n",
       "\n",
       "<span class=\"c1\"># Model training.</span>\n",
       "<span class=\"c1\"># This is implemented in the same way as part 2 of this tutorial.</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">,</span> <span class=\"n\">val_loader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">opt</span><span class=\"p\">,</span> <span class=\"n\">scheduler</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">epoch_start</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">,</span> <span class=\"n\">DEVICE</span><span class=\"p\">,</span> <span class=\"n\">checkpoint_file</span><span class=\"p\">,</span> <span class=\"n\">prev_best_val_acc</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">train_loader</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "  \n",
       "    <span class=\"n\">best_val_acc</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"mf\">0.0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">prev_best_val_acc</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span> <span class=\"k\">else</span> <span class=\"n\">prev_best_val_acc</span>\n",
       "    \n",
       "    <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epoch_start</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "    \n",
       "        <span class=\"n\">train_loader</span><span class=\"o\">.</span><span class=\"n\">sampler</span><span class=\"o\">.</span><span class=\"n\">set_epoch</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">)</span>\n",
       "    \n",
       "        <span class=\"n\">avg_loss</span><span class=\"p\">,</span> <span class=\"n\">val_loss</span><span class=\"p\">,</span> <span class=\"n\">val_acc</span><span class=\"p\">,</span> <span class=\"n\">avg_acc</span>  <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span>\n",
       "    \n",
       "        <span class=\"n\">start_time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">()</span>\n",
       "    \n",
       "        <span class=\"k\">for</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"ow\">in</span> <span class=\"n\">train_loader</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">),</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "            \n",
       "            <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "            <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "            <span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "            <span class=\"n\">avg_loss</span> <span class=\"o\">+=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span><span class=\"o\">/</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">pred_label</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">avg_acc</span> <span class=\"o\">+=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">pred_label</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "\n",
       "\n",
       "        <span class=\"n\">val_loss</span><span class=\"p\">,</span> <span class=\"n\">val_acc</span> <span class=\"o\">=</span> <span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">val_loader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "    \n",
       "        <span class=\"n\">end_time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">()</span>\n",
       "    \n",
       "        <span class=\"n\">total_time</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">((</span><span class=\"n\">end_time</span><span class=\"o\">-</span><span class=\"n\">start_time</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">seconds</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "    \n",
       "        <span class=\"c1\"># Learning rate reducer takes action</span>\n",
       "        <span class=\"n\">scheduler</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">val_loss</span><span class=\"p\">)</span>\n",
       "    \n",
       "        <span class=\"n\">avg_loss</span><span class=\"p\">,</span> <span class=\"n\">avg_acc</span> <span class=\"o\">=</span> <span class=\"n\">avg_loss</span><span class=\"o\">/</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">avg_acc</span><span class=\"o\">/</span><span class=\"n\">n</span>\n",
       "    \n",
       "        <span class=\"c1\"># Only machine rank==0 (master machine) saves the model and prints the metrics    </span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "        \n",
       "            <span class=\"c1\"># Save the best model that has the highest val accuracy</span>\n",
       "            <span class=\"k\">if</span> <span class=\"n\">val_acc</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span> <span class=\"o\">&gt;</span> <span class=\"n\">best_val_acc</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">():</span>\n",
       "                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">&#39;</span><span class=\"se\">\\n</span><span class=\"s1\">lr for this epoch is </span><span class=\"si\">{</span><span class=\"n\">scheduler</span><span class=\"o\">.</span><span class=\"n\">get_last_lr</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span><span class=\"p\">)</span>\n",
       "                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Prev Best Val Acc: </span><span class=\"si\">{</span><span class=\"n\">best_val_acc</span><span class=\"si\">}</span><span class=\"s2\"> &lt; Cur Val Acc: </span><span class=\"si\">{</span><span class=\"n\">val_acc</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Saving the new best model...&quot;</span><span class=\"p\">)</span>\n",
       "                <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">({</span>\n",
       "                        <span class=\"s1\">&#39;epoch&#39;</span><span class=\"p\">:</span><span class=\"n\">epoch</span><span class=\"p\">,</span>\n",
       "                        <span class=\"s1\">&#39;machine&#39;</span><span class=\"p\">:</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>\n",
       "                        <span class=\"s1\">&#39;model_state_dict&#39;</span><span class=\"p\">:</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">state_dict</span><span class=\"p\">(),</span>\n",
       "                        <span class=\"s1\">&#39;accuracy&#39;</span><span class=\"p\">:</span><span class=\"n\">val_acc</span><span class=\"p\">,</span>\n",
       "                        <span class=\"s1\">&#39;loss&#39;</span><span class=\"p\">:</span><span class=\"n\">val_loss</span>\n",
       "                <span class=\"p\">},</span> <span class=\"n\">checkpoint_file</span><span class=\"p\">)</span>\n",
       "                <span class=\"n\">best_val_acc</span> <span class=\"o\">=</span> <span class=\"n\">val_acc</span>\n",
       "                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Finished saving model</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "        \n",
       "            <span class=\"c1\"># Print the metrics (should be same on all machines)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">(Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">epochs</span><span class=\"si\">}</span><span class=\"s2\">) Time: </span><span class=\"si\">{</span><span class=\"n\">total_time</span><span class=\"si\">}</span><span class=\"s2\">s&quot;</span><span class=\"p\">)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;(Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">epochs</span><span class=\"si\">}</span><span class=\"s2\">) Average train loss: </span><span class=\"si\">{</span><span class=\"n\">avg_loss</span><span class=\"si\">}</span><span class=\"s2\">, Average train accuracy: </span><span class=\"si\">{</span><span class=\"n\">avg_acc</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;(Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">epochs</span><span class=\"si\">}</span><span class=\"s2\">) Val loss: </span><span class=\"si\">{</span><span class=\"n\">val_loss</span><span class=\"si\">}</span><span class=\"s2\">, Val accuracy: </span><span class=\"si\">{</span><span class=\"n\">val_acc</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>  \n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;(Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">epochs</span><span class=\"si\">}</span><span class=\"s2\">) Current best val acc: </span><span class=\"si\">{</span><span class=\"n\">best_val_acc</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>  \n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">load_checkpoint</span><span class=\"p\">(</span><span class=\"n\">checkpoint_path</span><span class=\"p\">,</span> <span class=\"n\">DEVICE</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">checkpoint_path</span><span class=\"p\">,</span> <span class=\"n\">map_location</span><span class=\"o\">=</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">checkpoint</span>\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">load_model_fm_checkpoint</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">,</span> <span class=\"n\">primitive_model</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">primitive_model</span><span class=\"o\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s1\">&#39;model_state_dict&#39;</span><span class=\"p\">])</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">primitive_model</span> \n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">init_distributed</span><span class=\"p\">():</span>\n",
       "    \n",
       "    <span class=\"n\">dist_url</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;env://&quot;</span>\n",
       "  \n",
       "    <span class=\"n\">world_size</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;WORLD_SIZE&#39;</span><span class=\"p\">])</span> \n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span> \n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"n\">backend</span><span class=\"o\">=</span><span class=\"s2\">&quot;nccl&quot;</span><span class=\"p\">,</span> <span class=\"c1\">#&quot;nccl&quot; for using GPUs, &quot;gloo&quot; for using CPUs</span>\n",
       "                            <span class=\"n\">init_method</span><span class=\"o\">=</span><span class=\"n\">dist_url</span><span class=\"p\">,</span>\n",
       "                            <span class=\"n\">world_size</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">,</span>\n",
       "                            <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">set_device</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">def</span> <span class=\"nf\">cleanup</span><span class=\"p\">():</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Cleaning up the distributed environment...&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">destroy_process_group</span><span class=\"p\">()</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Distributed environment has been properly closed&quot;</span><span class=\"p\">)</span>\n",
       "    \n",
       "    \n",
       "<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n",
       "    <span class=\"n\">hp</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;lr&quot;</span><span class=\"p\">:</span><span class=\"mf\">1e-4</span><span class=\"p\">,</span> <span class=\"s2\">&quot;batch_size&quot;</span><span class=\"p\">:</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"s2\">&quot;epochs&quot;</span><span class=\"p\">:</span><span class=\"mi\">5</span><span class=\"p\">}</span>\n",
       "    \n",
       "    <span class=\"c1\"># Please specify the path to train, cross_validation, and test images below:</span>\n",
       "    <span class=\"n\">train_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;SCRATCH&#39;</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Dataset_2/Train/&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">val_path</span>   <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;SCRATCH&#39;</span><span class=\"p\">],</span> <span class=\"s2\">&quot;Dataset_2/Validation/&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">test_path</span>  <span class=\"o\">=</span> <span class=\"kc\">None</span>\n",
       "    \n",
       "    <span class=\"n\">local_rank</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;LOCAL_RANK&#39;</span><span class=\"p\">])</span>\n",
       "    <span class=\"n\">DEVICE</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda&quot;</span><span class=\"p\">,</span> <span class=\"n\">local_rank</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"n\">model_folder_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;SCRATCH&#39;</span><span class=\"p\">],</span> <span class=\"s2\">&quot;cnn4_damagelevel_output_model&quot;</span><span class=\"p\">)</span> \n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">makedirs</span><span class=\"p\">(</span><span class=\"n\">model_folder_path</span><span class=\"p\">,</span><span class=\"n\">exist_ok</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"c1\"># same loss function as part 2 </span>\n",
       "    <span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">(</span><span class=\"n\">label_smoothing</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">train_set</span><span class=\"p\">,</span> <span class=\"n\">val_set</span><span class=\"p\">,</span> <span class=\"n\">test_set</span> <span class=\"o\">=</span> <span class=\"n\">load_datasets</span><span class=\"p\">(</span><span class=\"n\">train_path</span><span class=\"p\">,</span> <span class=\"n\">val_path</span><span class=\"p\">,</span> <span class=\"n\">test_path</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">val_dataloader</span><span class=\"p\">,</span> <span class=\"n\">test_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">construct_dataloaders</span><span class=\"p\">(</span><span class=\"n\">train_set</span><span class=\"p\">,</span> <span class=\"n\">val_set</span><span class=\"p\">,</span> <span class=\"n\">test_set</span><span class=\"p\">,</span> <span class=\"n\">hp</span><span class=\"p\">[</span><span class=\"s2\">&quot;batch_size&quot;</span><span class=\"p\">],</span> <span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "                          \n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">getResNet</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "  \n",
       "    <span class=\"c1\"># load the checkpoint that has the best performance in previous experiments</span>\n",
       "    <span class=\"n\">prev_best_val_acc</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n",
       "    <span class=\"n\">checkpoint_file</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">model_folder_path</span><span class=\"p\">,</span> <span class=\"s2\">&quot;best_model.pt&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">if</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">(</span><span class=\"n\">checkpoint_file</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">load_checkpoint</span><span class=\"p\">(</span><span class=\"n\">checkpoint_file</span><span class=\"p\">,</span> <span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">prev_best_val_acc</span> <span class=\"o\">=</span> <span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s1\">&#39;accuracy&#39;</span><span class=\"p\">]</span>\n",
       "        <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">load_model_fm_checkpoint</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">,</span><span class=\"n\">model</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">epoch_start</span> <span class=\"o\">=</span> <span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s1\">&#39;epoch&#39;</span><span class=\"p\">]</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;resuming training from epoch </span><span class=\"si\">{</span><span class=\"n\">epoch_start</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">else</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">epoch_start</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
       "  \n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">SyncBatchNorm</span><span class=\"o\">.</span><span class=\"n\">convert_sync_batchnorm</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">parallel</span><span class=\"o\">.</span><span class=\"n\">DistributedDataParallel</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">device_ids</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">local_rank</span><span class=\"p\">])</span>\n",
       "    <span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span><span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"n\">hp</span><span class=\"p\">[</span><span class=\"s2\">&quot;lr&quot;</span><span class=\"p\">])</span>\n",
       "    \n",
       "    <span class=\"c1\"># same learning rate scheduler as part 2</span>\n",
       "    <span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">ReduceLROnPlateau</span><span class=\"p\">(</span><span class=\"n\">opt</span><span class=\"p\">,</span> <span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;min&#39;</span><span class=\"p\">,</span><span class=\"n\">factor</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">min_lr</span><span class=\"o\">=</span><span class=\"mf\">1e-8</span><span class=\"p\">)</span>\n",
       "  \n",
       "    <span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">val_dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">opt</span><span class=\"p\">,</span> <span class=\"n\">scheduler</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">epoch_start</span><span class=\"p\">,</span> <span class=\"n\">hp</span><span class=\"p\">[</span><span class=\"s2\">&quot;epochs&quot;</span><span class=\"p\">],</span> <span class=\"n\">DEVICE</span><span class=\"p\">,</span> <span class=\"n\">checkpoint_file</span><span class=\"p\">,</span> <span class=\"n\">prev_best_val_acc</span><span class=\"p\">)</span>\n",
       "   \n",
       "\n",
       "  <span class=\"c1\"># only the node with rank 0 does the loading, evaluation and printing to avoild duplicate </span>\n",
       "    <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "        <span class=\"n\">primitive_model</span> <span class=\"o\">=</span> <span class=\"n\">getResNet</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">load_checkpoint</span><span class=\"p\">(</span><span class=\"n\">checkpoint_file</span><span class=\"p\">,</span> <span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">best_model</span> <span class=\"o\">=</span> <span class=\"n\">load_model_fm_checkpoint</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">,</span><span class=\"n\">primitive_model</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">acc</span> <span class=\"o\">=</span> <span class=\"n\">eval_model</span><span class=\"p\">(</span><span class=\"n\">val_dataloader</span><span class=\"p\">,</span><span class=\"n\">best_model</span><span class=\"p\">,</span><span class=\"n\">loss_fn</span><span class=\"p\">,</span><span class=\"n\">DEVICE</span><span class=\"p\">)</span>\n",
       "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">Best model (val loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"si\">}</span><span class=\"s2\">, val accuracy: </span><span class=\"si\">{</span><span class=\"n\">acc</span><span class=\"si\">}</span><span class=\"s2\">) has been saved to </span><span class=\"si\">{</span><span class=\"n\">checkpoint_file</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">cleanup</span><span class=\"p\">()</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">init_distributed</span><span class=\"p\">()</span>\n",
       "    \n",
       "    <span class=\"n\">gc</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()):</span>\n",
       "        <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;cuda:</span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">):</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">empty_cache</span><span class=\"p\">()</span>\n",
       "  \n",
       "    <span class=\"n\">main</span><span class=\"p\">()</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} NOTE: This is the main script of using Distributed Data Parallel to train ResNet}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{sys}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{os}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{gc}\n",
       "\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torchvision}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torchvision} \\PY{k+kn}{import} \\PY{n}{datasets}\\PY{p}{,} \\PY{n}{models}\\PY{p}{,} \\PY{n}{transforms}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn} \\PY{k}{as} \\PY{n+nn}{nn}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k+kn}{import} \\PY{n}{DistributedSampler}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{distributed} \\PY{k}{as} \\PY{n+nn}{dist}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{datetime} \\PY{k+kn}{import} \\PY{n}{datetime}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{warnings}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{shutil}\n",
       "\n",
       "\\PY{n}{warnings}\\PY{o}{.}\\PY{n}{filterwarnings}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{ignore}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{n}{message}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{torch.distributed.\\PYZus{}all\\PYZus{}gather\\PYZus{}base is a private function and will be deprecated. Please use torch.distributed.all\\PYZus{}gather\\PYZus{}into\\PYZus{}tensor instead.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Define the GPUs that will be used in this script}\n",
       "\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{CUDA\\PYZus{}VISIBLE\\PYZus{}DEVICES}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{,}\\PY{l+s+s2}{\\PYZdq{}}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n+nb}{str}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)} \\PY{k}{for} \\PY{n}{x} \\PY{o+ow}{in} \\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{device\\PYZus{}count}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Apply transformations to our data.}\n",
       "\\PY{c+c1}{\\PYZsh{} The datasets transformations are the same as the ones from part 2 of this tutorial.}\n",
       "\\PY{k}{def} \\PY{n+nf}{load\\PYZus{}datasets}\\PY{p}{(}\\PY{n}{train\\PYZus{}path}\\PY{p}{,} \\PY{n}{val\\PYZus{}path}\\PY{p}{,} \\PY{n}{test\\PYZus{}path}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{val\\PYZus{}img\\PYZus{}transform} \\PY{o}{=} \\PY{n}{transforms}\\PY{o}{.}\\PY{n}{Compose}\\PY{p}{(}\\PY{p}{[}\\PY{n}{transforms}\\PY{o}{.}\\PY{n}{Resize}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{244}\\PY{p}{,}\\PY{l+m+mi}{244}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                         \\PY{n}{transforms}\\PY{o}{.}\\PY{n}{ToTensor}\\PY{p}{(}\\PY{p}{)}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{train\\PYZus{}img\\PYZus{}transform} \\PY{o}{=} \\PY{n}{transforms}\\PY{o}{.}\\PY{n}{Compose}\\PY{p}{(}\\PY{p}{[}\\PY{n}{transforms}\\PY{o}{.}\\PY{n}{AutoAugment}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "                                           \\PY{n}{transforms}\\PY{o}{.}\\PY{n}{Resize}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{244}\\PY{p}{,}\\PY{l+m+mi}{244}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                           \\PY{n}{transforms}\\PY{o}{.}\\PY{n}{ToTensor}\\PY{p}{(}\\PY{p}{)}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{train\\PYZus{}dataset} \\PY{o}{=} \\PY{n}{datasets}\\PY{o}{.}\\PY{n}{ImageFolder}\\PY{p}{(}\\PY{n}{train\\PYZus{}path}\\PY{p}{,} \\PY{n}{transform}\\PY{o}{=}\\PY{n}{train\\PYZus{}img\\PYZus{}transform}\\PY{p}{)}\n",
       "    \\PY{n}{val\\PYZus{}dataset} \\PY{o}{=} \\PY{n}{datasets}\\PY{o}{.}\\PY{n}{ImageFolder}\\PY{p}{(}\\PY{n}{val\\PYZus{}path}\\PY{p}{,} \\PY{n}{transform}\\PY{o}{=}\\PY{n}{val\\PYZus{}img\\PYZus{}transform}\\PY{p}{)} \n",
       "    \\PY{n}{test\\PYZus{}dataset} \\PY{o}{=} \\PY{n}{datasets}\\PY{o}{.}\\PY{n}{ImageFolder}\\PY{p}{(}\\PY{n}{test\\PYZus{}path}\\PY{p}{,} \\PY{n}{transform}\\PY{o}{=}\\PY{n}{val\\PYZus{}img\\PYZus{}transform}\\PY{p}{)} \\PY{k}{if} \\PY{n}{test\\PYZus{}path} \\PY{o+ow}{is} \\PY{o+ow}{not} \\PY{k+kc}{None} \\PY{k}{else} \\PY{k+kc}{None}\n",
       "  \n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Train set size: }\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{train\\PYZus{}dataset}\\PY{p}{)}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{, Validation set size: }\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{val\\PYZus{}dataset}\\PY{p}{)}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataset}\\PY{p}{,} \\PY{n}{val\\PYZus{}dataset}\\PY{p}{,} \\PY{n}{test\\PYZus{}dataset}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Construct Dataloaders}\n",
       "\\PY{c+c1}{\\PYZsh{} The DistributedSampler we use here restricts data loading to a subset of the dataset.}\n",
       "\\PY{c+c1}{\\PYZsh{} In conjunction with DistributedDataParallel (shows up later in this tutorial), each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.}\n",
       "\\PY{k}{def} \\PY{n+nf}{construct\\PYZus{}dataloaders}\\PY{p}{(}\\PY{n}{train\\PYZus{}set}\\PY{p}{,} \\PY{n}{val\\PYZus{}set}\\PY{p}{,} \\PY{n}{test\\PYZus{}set}\\PY{p}{,} \\PY{n}{batch\\PYZus{}size}\\PY{p}{,} \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\\PY{p}{:}  \n",
       "    \\PY{n}{train\\PYZus{}sampler} \\PY{o}{=} \\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n}{dataset}\\PY{o}{=}\\PY{n}{train\\PYZus{}set}\\PY{p}{,}\\PY{n}{shuffle}\\PY{o}{=}\\PY{n}{shuffle}\\PY{p}{)}\n",
       "    \\PY{n}{val\\PYZus{}sampler} \\PY{o}{=} \\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n}{dataset}\\PY{o}{=}\\PY{n}{val\\PYZus{}set}\\PY{p}{,} \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{False}\\PY{p}{)}\n",
       "    \\PY{n}{test\\PYZus{}sampler} \\PY{o}{=} \\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n}{dataset}\\PY{o}{=}\\PY{n}{test\\PYZus{}set}\\PY{p}{,} \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{False}\\PY{p}{)} \\PY{k}{if} \\PY{n}{test\\PYZus{}set} \\PY{o+ow}{is} \\PY{o+ow}{not} \\PY{k+kc}{None} \\PY{k}{else} \\PY{k+kc}{None}\n",
       "  \n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{utils}\\PY{o}{.}\\PY{n}{data}\\PY{o}{.}\\PY{n}{DataLoader}\\PY{p}{(}\\PY{n}{train\\PYZus{}set}\\PY{p}{,}\\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{,}\\PY{n}{sampler}\\PY{o}{=}\\PY{n}{train\\PYZus{}sampler}\\PY{p}{,}\\PY{n}{num\\PYZus{}workers}\\PY{o}{=}\\PY{l+m+mi}{4}\\PY{p}{,}\\PY{n}{pin\\PYZus{}memory}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "    \\PY{n}{val\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{utils}\\PY{o}{.}\\PY{n}{data}\\PY{o}{.}\\PY{n}{DataLoader}\\PY{p}{(}\\PY{n}{val\\PYZus{}set}\\PY{p}{,}\\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{,}\\PY{n}{sampler}\\PY{o}{=}\\PY{n}{val\\PYZus{}sampler}\\PY{p}{,}\\PY{n}{num\\PYZus{}workers}\\PY{o}{=}\\PY{l+m+mi}{4}\\PY{p}{)}\n",
       "    \\PY{n}{test\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{utils}\\PY{o}{.}\\PY{n}{data}\\PY{o}{.}\\PY{n}{DataLoader}\\PY{p}{(}\\PY{n}{test\\PYZus{}aset}\\PY{p}{,} \\PY{n}{batch\\PYZus{}size}\\PY{p}{,} \\PY{n}{sampler}\\PY{o}{=}\\PY{n}{test\\PYZus{}sampler}\\PY{p}{,}\\PY{n}{num\\PYZus{}workers}\\PY{o}{=}\\PY{l+m+mi}{4}\\PY{p}{)} \\PY{k}{if} \\PY{n}{test\\PYZus{}set} \\PY{o+ow}{is} \\PY{o+ow}{not} \\PY{k+kc}{None} \\PY{k}{else} \\PY{k+kc}{None}\n",
       "    \n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{val\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{test\\PYZus{}dataloader}\n",
       "\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Building the Neural Network}\n",
       "\\PY{c+c1}{\\PYZsh{} This is the same from part 2 of this tutorial}\n",
       "\\PY{k}{def} \\PY{n+nf}{getResNet}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{resnet} \\PY{o}{=} \\PY{n}{models}\\PY{o}{.}\\PY{n}{resnet34}\\PY{p}{(}\\PY{n}{weights}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{IMAGENET1K\\PYZus{}V1}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Fix the conv layers parameters}\n",
       "    \\PY{k}{for} \\PY{n}{conv\\PYZus{}param} \\PY{o+ow}{in} \\PY{n}{resnet}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{conv\\PYZus{}param}\\PY{o}{.}\\PY{n}{require\\PYZus{}grad} \\PY{o}{=} \\PY{k+kc}{False}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} get the input dimension for this layer}\n",
       "    \\PY{n}{num\\PYZus{}ftrs} \\PY{o}{=} \\PY{n}{resnet}\\PY{o}{.}\\PY{n}{fc}\\PY{o}{.}\\PY{n}{in\\PYZus{}features}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} build the new final mlp layers of network}\n",
       "    \\PY{n}{fc} \\PY{o}{=} \\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "          \\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{n}{num\\PYZus{}ftrs}\\PY{p}{,} \\PY{n}{num\\PYZus{}ftrs}\\PY{p}{)}\\PY{p}{,}\n",
       "          \\PY{n}{nn}\\PY{o}{.}\\PY{n}{ReLU}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "          \\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{n}{num\\PYZus{}ftrs}\\PY{p}{,} \\PY{l+m+mi}{3}\\PY{p}{)}\n",
       "        \\PY{p}{)}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} replace final fully connected layer}\n",
       "    \\PY{n}{resnet}\\PY{o}{.}\\PY{n}{fc} \\PY{o}{=} \\PY{n}{fc}\n",
       "    \\PY{k}{return} \\PY{n}{resnet}\n",
       "\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Model evaluation.}\n",
       "\\PY{c+c1}{\\PYZsh{} This is implemented in the same way as part 2 of this tutorial.}\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{eval\\PYZus{}model}\\PY{p}{(}\\PY{n}{data\\PYZus{}loader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{DEVICE}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{model}\\PY{o}{.}\\PY{n}{train}\\PY{p}{(}\\PY{k+kc}{False}\\PY{p}{)}\n",
       "    \\PY{n}{model}\\PY{o}{.}\\PY{n}{eval}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{n}{loss}\\PY{p}{,} \\PY{n}{accuracy} \\PY{o}{=} \\PY{l+m+mf}{0.0}\\PY{p}{,} \\PY{l+m+mf}{0.0}\n",
       "    \\PY{n}{n} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{data\\PYZus{}loader}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{for} \\PY{n}{i}\\PY{p}{,} \\PY{n}{data} \\PY{o+ow}{in} \\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{data\\PYZus{}loader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{x}\\PY{p}{,}\\PY{n}{y} \\PY{o}{=} \\PY{n}{data}\n",
       "        \\PY{n}{x}\\PY{p}{,}\\PY{n}{y} \\PY{o}{=} \\PY{n}{x}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\\PY{p}{,} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "        \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{+}\\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\\PY{o}{/}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{n}{pred\\PYZus{}label} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{argmax}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{axis} \\PY{o}{=} \\PY{l+m+mi}{1}\\PY{p}{)}\n",
       "        \\PY{n}{accuracy} \\PY{o}{+}\\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{sum}\\PY{p}{(}\\PY{n}{pred\\PYZus{}label} \\PY{o}{==} \\PY{n}{y}\\PY{p}{)}\\PY{o}{/}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{k}{return} \\PY{n}{loss}\\PY{o}{/}\\PY{n}{n}\\PY{p}{,} \\PY{n}{accuracy}\\PY{o}{/}\\PY{n}{n}\n",
       "\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Model training.}\n",
       "\\PY{c+c1}{\\PYZsh{} This is implemented in the same way as part 2 of this tutorial.}\n",
       "\\PY{k}{def} \\PY{n+nf}{train}\\PY{p}{(}\\PY{n}{train\\PYZus{}loader}\\PY{p}{,} \\PY{n}{val\\PYZus{}loader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{opt}\\PY{p}{,} \\PY{n}{scheduler}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{epoch\\PYZus{}start}\\PY{p}{,} \\PY{n}{epochs}\\PY{p}{,} \\PY{n}{DEVICE}\\PY{p}{,} \\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{,} \\PY{n}{prev\\PYZus{}best\\PYZus{}val\\PYZus{}acc}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{n} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{train\\PYZus{}loader}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "  \n",
       "    \\PY{n}{best\\PYZus{}val\\PYZus{}acc} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{tensor}\\PY{p}{(}\\PY{l+m+mf}{0.0}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)} \\PY{k}{if} \\PY{n}{prev\\PYZus{}best\\PYZus{}val\\PYZus{}acc} \\PY{o+ow}{is} \\PY{k+kc}{None} \\PY{k}{else} \\PY{n}{prev\\PYZus{}best\\PYZus{}val\\PYZus{}acc}\n",
       "    \n",
       "    \\PY{k}{for} \\PY{n}{epoch} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epoch\\PYZus{}start}\\PY{p}{,} \\PY{n}{epochs}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{model}\\PY{o}{.}\\PY{n}{train}\\PY{p}{(}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "    \n",
       "        \\PY{n}{train\\PYZus{}loader}\\PY{o}{.}\\PY{n}{sampler}\\PY{o}{.}\\PY{n}{set\\PYZus{}epoch}\\PY{p}{(}\\PY{n}{epoch}\\PY{p}{)}\n",
       "    \n",
       "        \\PY{n}{avg\\PYZus{}loss}\\PY{p}{,} \\PY{n}{val\\PYZus{}loss}\\PY{p}{,} \\PY{n}{val\\PYZus{}acc}\\PY{p}{,} \\PY{n}{avg\\PYZus{}acc}  \\PY{o}{=} \\PY{l+m+mf}{0.0}\\PY{p}{,} \\PY{l+m+mf}{0.0}\\PY{p}{,} \\PY{l+m+mf}{0.0}\\PY{p}{,} \\PY{l+m+mf}{0.0}\n",
       "    \n",
       "        \\PY{n}{start\\PYZus{}time} \\PY{o}{=} \\PY{n}{datetime}\\PY{o}{.}\\PY{n}{now}\\PY{p}{(}\\PY{p}{)}\n",
       "    \n",
       "        \\PY{k}{for} \\PY{n}{x}\\PY{p}{,} \\PY{n}{y} \\PY{o+ow}{in} \\PY{n}{train\\PYZus{}loader}\\PY{p}{:}\n",
       "            \\PY{n}{x}\\PY{p}{,} \\PY{n}{y} \\PY{o}{=} \\PY{n}{x}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\\PY{p}{,} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "            \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "            \\PY{n}{loss} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\n",
       "            \n",
       "            \\PY{n}{opt}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "            \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "            \\PY{n}{opt}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "            \\PY{n}{avg\\PYZus{}loss} \\PY{o}{+}\\PY{o}{=} \\PY{n}{loss}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{o}{/}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "            \\PY{n}{pred\\PYZus{}label} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{argmax}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{axis}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)}\n",
       "            \\PY{n}{avg\\PYZus{}acc} \\PY{o}{+}\\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{sum}\\PY{p}{(}\\PY{n}{pred\\PYZus{}label} \\PY{o}{==} \\PY{n}{y}\\PY{p}{)}\\PY{o}{/}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "\n",
       "\n",
       "        \\PY{n}{val\\PYZus{}loss}\\PY{p}{,} \\PY{n}{val\\PYZus{}acc} \\PY{o}{=} \\PY{n}{eval\\PYZus{}model}\\PY{p}{(}\\PY{n}{val\\PYZus{}loader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{DEVICE}\\PY{p}{)}\n",
       "    \n",
       "        \\PY{n}{end\\PYZus{}time} \\PY{o}{=} \\PY{n}{datetime}\\PY{o}{.}\\PY{n}{now}\\PY{p}{(}\\PY{p}{)}\n",
       "    \n",
       "        \\PY{n}{total\\PYZus{}time} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{tensor}\\PY{p}{(}\\PY{p}{(}\\PY{n}{end\\PYZus{}time}\\PY{o}{\\PYZhy{}}\\PY{n}{start\\PYZus{}time}\\PY{p}{)}\\PY{o}{.}\\PY{n}{seconds}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "    \n",
       "        \\PY{c+c1}{\\PYZsh{} Learning rate reducer takes action}\n",
       "        \\PY{n}{scheduler}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{n}{val\\PYZus{}loss}\\PY{p}{)}\n",
       "    \n",
       "        \\PY{n}{avg\\PYZus{}loss}\\PY{p}{,} \\PY{n}{avg\\PYZus{}acc} \\PY{o}{=} \\PY{n}{avg\\PYZus{}loss}\\PY{o}{/}\\PY{n}{n}\\PY{p}{,} \\PY{n}{avg\\PYZus{}acc}\\PY{o}{/}\\PY{n}{n}\n",
       "    \n",
       "        \\PY{c+c1}{\\PYZsh{} Only machine rank==0 (master machine) saves the model and prints the metrics    }\n",
       "        \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "        \n",
       "            \\PY{c+c1}{\\PYZsh{} Save the best model that has the highest val accuracy}\n",
       "            \\PY{k}{if} \\PY{n}{val\\PYZus{}acc}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)} \\PY{o}{\\PYZgt{}} \\PY{n}{best\\PYZus{}val\\PYZus{}acc}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "                \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s1}{lr for this epoch is }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{scheduler}\\PY{o}{.}\\PY{n}{get\\PYZus{}last\\PYZus{}lr}\\PY{p}{(}\\PY{p}{)}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "                \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Prev Best Val Acc: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{best\\PYZus{}val\\PYZus{}acc}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{ \\PYZlt{} Cur Val Acc: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{val\\PYZus{}acc}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "                \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Saving the new best model...}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "                \\PY{n}{torch}\\PY{o}{.}\\PY{n}{save}\\PY{p}{(}\\PY{p}{\\PYZob{}}\n",
       "                        \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{epoch}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\\PY{n}{epoch}\\PY{p}{,}\n",
       "                        \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{machine}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}\n",
       "                        \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{model\\PYZus{}state\\PYZus{}dict}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\\PY{n}{model}\\PY{o}{.}\\PY{n}{module}\\PY{o}{.}\\PY{n}{state\\PYZus{}dict}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "                        \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{accuracy}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\\PY{n}{val\\PYZus{}acc}\\PY{p}{,}\n",
       "                        \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{loss}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\\PY{n}{val\\PYZus{}loss}\n",
       "                \\PY{p}{\\PYZcb{}}\\PY{p}{,} \\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{)}\n",
       "                \\PY{n}{best\\PYZus{}val\\PYZus{}acc} \\PY{o}{=} \\PY{n}{val\\PYZus{}acc}\n",
       "                \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Finished saving model}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \n",
       "            \\PY{c+c1}{\\PYZsh{} Print the metrics (should be same on all machines)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{(Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epoch}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epochs}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{) Time: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{total\\PYZus{}time}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{s}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{(Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epoch}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epochs}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{) Average train loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{avg\\PYZus{}loss}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{, Average train accuracy: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{avg\\PYZus{}acc}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{(Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epoch}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epochs}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{) Val loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{val\\PYZus{}loss}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{, Val accuracy: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{val\\PYZus{}acc}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}  \n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{(Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epoch}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epochs}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{) Current best val acc: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{best\\PYZus{}val\\PYZus{}acc}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}  \n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{load\\PYZus{}checkpoint}\\PY{p}{(}\\PY{n}{checkpoint\\PYZus{}path}\\PY{p}{,} \\PY{n}{DEVICE}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{checkpoint} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{load}\\PY{p}{(}\\PY{n}{checkpoint\\PYZus{}path}\\PY{p}{,} \\PY{n}{map\\PYZus{}location}\\PY{o}{=}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{checkpoint}\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{load\\PYZus{}model\\PYZus{}fm\\PYZus{}checkpoint}\\PY{p}{(}\\PY{n}{checkpoint}\\PY{p}{,} \\PY{n}{primitive\\PYZus{}model}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{primitive\\PYZus{}model}\\PY{o}{.}\\PY{n}{load\\PYZus{}state\\PYZus{}dict}\\PY{p}{(}\\PY{n}{checkpoint}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{model\\PYZus{}state\\PYZus{}dict}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{primitive\\PYZus{}model} \n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \n",
       "    \\PY{n}{dist\\PYZus{}url} \\PY{o}{=} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{env://}\\PY{l+s+s2}{\\PYZdq{}}\n",
       "  \n",
       "    \\PY{n}{world\\PYZus{}size} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{WORLD\\PYZus{}SIZE}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)} \n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)} \n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{init\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{n}{backend}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{nccl}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{c+c1}{\\PYZsh{}\\PYZdq{}nccl\\PYZdq{} for using GPUs, \\PYZdq{}gloo\\PYZdq{} for using CPUs}\n",
       "                            \\PY{n}{init\\PYZus{}method}\\PY{o}{=}\\PY{n}{dist\\PYZus{}url}\\PY{p}{,}\n",
       "                            \\PY{n}{world\\PYZus{}size}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{,}\n",
       "                            \\PY{n}{rank}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\n",
       "    \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{set\\PYZus{}device}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\n",
       "\n",
       "\n",
       "\\PY{k}{def} \\PY{n+nf}{cleanup}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Cleaning up the distributed environment...}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{destroy\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Distributed environment has been properly closed}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \n",
       "    \n",
       "\\PY{k}{def} \\PY{n+nf}{main}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{hp} \\PY{o}{=} \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{lr}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\\PY{l+m+mf}{1e\\PYZhy{}4}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{batch\\PYZus{}size}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\\PY{l+m+mi}{16}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{epochs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\\PY{l+m+mi}{5}\\PY{p}{\\PYZcb{}}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} Please specify the path to train, cross\\PYZus{}validation, and test images below:}\n",
       "    \\PY{n}{train\\PYZus{}path} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SCRATCH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Dataset\\PYZus{}2/Train/}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{n}{val\\PYZus{}path}   \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SCRATCH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Dataset\\PYZus{}2/Validation/}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{n}{test\\PYZus{}path}  \\PY{o}{=} \\PY{k+kc}{None}\n",
       "    \n",
       "    \\PY{n}{local\\PYZus{}rank} \\PY{o}{=} \\PY{n+nb}{int}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{LOCAL\\PYZus{}RANK}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{DEVICE} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{n}{model\\PYZus{}folder\\PYZus{}path} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SCRATCH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cnn4\\PYZus{}damagelevel\\PYZus{}output\\PYZus{}model}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)} \n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{makedirs}\\PY{p}{(}\\PY{n}{model\\PYZus{}folder\\PYZus{}path}\\PY{p}{,}\\PY{n}{exist\\PYZus{}ok}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} same loss function as part 2 }\n",
       "    \\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{nn}\\PY{o}{.}\\PY{n}{CrossEntropyLoss}\\PY{p}{(}\\PY{n}{label\\PYZus{}smoothing}\\PY{o}{=}\\PY{l+m+mf}{0.1}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "    \\PY{n}{train\\PYZus{}set}\\PY{p}{,} \\PY{n}{val\\PYZus{}set}\\PY{p}{,} \\PY{n}{test\\PYZus{}set} \\PY{o}{=} \\PY{n}{load\\PYZus{}datasets}\\PY{p}{(}\\PY{n}{train\\PYZus{}path}\\PY{p}{,} \\PY{n}{val\\PYZus{}path}\\PY{p}{,} \\PY{n}{test\\PYZus{}path}\\PY{p}{)}\n",
       "    \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{val\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{test\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{construct\\PYZus{}dataloaders}\\PY{p}{(}\\PY{n}{train\\PYZus{}set}\\PY{p}{,} \\PY{n}{val\\PYZus{}set}\\PY{p}{,} \\PY{n}{test\\PYZus{}set}\\PY{p}{,} \\PY{n}{hp}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{batch\\PYZus{}size}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}\n",
       "                          \n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{getResNet}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "  \n",
       "    \\PY{c+c1}{\\PYZsh{} load the checkpoint that has the best performance in previous experiments}\n",
       "    \\PY{n}{prev\\PYZus{}best\\PYZus{}val\\PYZus{}acc} \\PY{o}{=} \\PY{k+kc}{None}\n",
       "    \\PY{n}{checkpoint\\PYZus{}file} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{model\\PYZus{}folder\\PYZus{}path}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{best\\PYZus{}model.pt}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{if} \\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{exists}\\PY{p}{(}\\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{checkpoint} \\PY{o}{=} \\PY{n}{load\\PYZus{}checkpoint}\\PY{p}{(}\\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{,} \\PY{n}{DEVICE}\\PY{p}{)}\n",
       "        \\PY{n}{prev\\PYZus{}best\\PYZus{}val\\PYZus{}acc} \\PY{o}{=} \\PY{n}{checkpoint}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{accuracy}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "        \\PY{n}{model} \\PY{o}{=} \\PY{n}{load\\PYZus{}model\\PYZus{}fm\\PYZus{}checkpoint}\\PY{p}{(}\\PY{n}{checkpoint}\\PY{p}{,}\\PY{n}{model}\\PY{p}{)}\n",
       "        \\PY{n}{epoch\\PYZus{}start} \\PY{o}{=} \\PY{n}{checkpoint}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{epoch}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "        \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{resuming training from epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{epoch\\PYZus{}start}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{else}\\PY{p}{:}\n",
       "        \\PY{n}{epoch\\PYZus{}start} \\PY{o}{=} \\PY{l+m+mi}{0}\n",
       "  \n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{nn}\\PY{o}{.}\\PY{n}{SyncBatchNorm}\\PY{o}{.}\\PY{n}{convert\\PYZus{}sync\\PYZus{}batchnorm}\\PY{p}{(}\\PY{n}{model}\\PY{p}{)}\n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{nn}\\PY{o}{.}\\PY{n}{parallel}\\PY{o}{.}\\PY{n}{DistributedDataParallel}\\PY{p}{(}\\PY{n}{model}\\PY{p}{,} \\PY{n}{device\\PYZus{}ids}\\PY{o}{=}\\PY{p}{[}\\PY{n}{local\\PYZus{}rank}\\PY{p}{]}\\PY{p}{)}\n",
       "    \\PY{n}{opt} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{Adam}\\PY{p}{(}\\PY{n}{model}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\\PY{n}{lr}\\PY{o}{=}\\PY{n}{hp}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{lr}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} same learning rate scheduler as part 2}\n",
       "    \\PY{n}{scheduler} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{lr\\PYZus{}scheduler}\\PY{o}{.}\\PY{n}{ReduceLROnPlateau}\\PY{p}{(}\\PY{n}{opt}\\PY{p}{,} \\PY{n}{mode}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{min}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,}\\PY{n}{factor}\\PY{o}{=}\\PY{l+m+mf}{0.1}\\PY{p}{,} \\PY{n}{patience}\\PY{o}{=}\\PY{l+m+mi}{5}\\PY{p}{,} \\PY{n}{min\\PYZus{}lr}\\PY{o}{=}\\PY{l+m+mf}{1e\\PYZhy{}8}\\PY{p}{)}\n",
       "  \n",
       "    \\PY{n}{train}\\PY{p}{(}\\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{val\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{opt}\\PY{p}{,} \\PY{n}{scheduler}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{epoch\\PYZus{}start}\\PY{p}{,} \\PY{n}{hp}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{epochs}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{,} \\PY{n}{DEVICE}\\PY{p}{,} \\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{,} \\PY{n}{prev\\PYZus{}best\\PYZus{}val\\PYZus{}acc}\\PY{p}{)}\n",
       "   \n",
       "\n",
       "  \\PY{c+c1}{\\PYZsh{} only the node with rank 0 does the loading, evaluation and printing to avoild duplicate }\n",
       "    \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "        \\PY{n}{primitive\\PYZus{}model} \\PY{o}{=} \\PY{n}{getResNet}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "        \\PY{n}{checkpoint} \\PY{o}{=} \\PY{n}{load\\PYZus{}checkpoint}\\PY{p}{(}\\PY{n}{checkpoint\\PYZus{}file}\\PY{p}{,} \\PY{n}{DEVICE}\\PY{p}{)}\n",
       "        \\PY{n}{best\\PYZus{}model} \\PY{o}{=} \\PY{n}{load\\PYZus{}model\\PYZus{}fm\\PYZus{}checkpoint}\\PY{p}{(}\\PY{n}{checkpoint}\\PY{p}{,}\\PY{n}{primitive\\PYZus{}model}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{p}{,} \\PY{n}{acc} \\PY{o}{=} \\PY{n}{eval\\PYZus{}model}\\PY{p}{(}\\PY{n}{val\\PYZus{}dataloader}\\PY{p}{,}\\PY{n}{best\\PYZus{}model}\\PY{p}{,}\\PY{n}{loss\\PYZus{}fn}\\PY{p}{,}\\PY{n}{DEVICE}\\PY{p}{)}\n",
       "        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{Best model (val loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{loss}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{, val accuracy: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{acc}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{) has been saved to }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{checkpoint\\PYZus{}file}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{n}{cleanup}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:}\n",
       "    \\PY{n}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{n}{gc}\\PY{o}{.}\\PY{n}{collect}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{k}{for} \\PY{n}{i} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{device\\PYZus{}count}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{k}{with} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{i}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{empty\\PYZus{}cache}\\PY{p}{(}\\PY{p}{)}\n",
       "  \n",
       "    \\PY{n}{main}\\PY{p}{(}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "# NOTE: This is the main script of using Distributed Data Parallel to train ResNet\n",
       "import sys\n",
       "import os\n",
       "import numpy as np\n",
       "import gc\n",
       "\n",
       "import torch\n",
       "import torchvision\n",
       "from torchvision import datasets, models, transforms\n",
       "import torch.nn as nn\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "import torch.distributed as dist\n",
       "from datetime import datetime\n",
       "import warnings\n",
       "import shutil\n",
       "\n",
       "warnings.filterwarnings(\"ignore\", message=\"torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\")\n",
       "\n",
       "# Define the GPUs that will be used in this script\n",
       "os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join(str(x) for x in list(range(torch.cuda.device_count())))\n",
       "\n",
       "# Apply transformations to our data.\n",
       "# The datasets transformations are the same as the ones from part 2 of this tutorial.\n",
       "def load_datasets(train_path, val_path, test_path):\n",
       "    val_img_transform = transforms.Compose([transforms.Resize((244,244)),\n",
       "                                         transforms.ToTensor()])\n",
       "    train_img_transform = transforms.Compose([transforms.AutoAugment(),\n",
       "                                           transforms.Resize((244,244)),\n",
       "                                           transforms.ToTensor()])\n",
       "    train_dataset = datasets.ImageFolder(train_path, transform=train_img_transform)\n",
       "    val_dataset = datasets.ImageFolder(val_path, transform=val_img_transform) \n",
       "    test_dataset = datasets.ImageFolder(test_path, transform=val_img_transform) if test_path is not None else None\n",
       "  \n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "    if local_rank == 0:\n",
       "        print(f\"Train set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")\n",
       "    return train_dataset, val_dataset, test_dataset\n",
       "\n",
       "# Construct Dataloaders\n",
       "# The DistributedSampler we use here restricts data loading to a subset of the dataset.\n",
       "# In conjunction with DistributedDataParallel (shows up later in this tutorial), each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.\n",
       "def construct_dataloaders(train_set, val_set, test_set, batch_size, shuffle=True):  \n",
       "    train_sampler = DistributedSampler(dataset=train_set,shuffle=shuffle)\n",
       "    val_sampler = DistributedSampler(dataset=val_set, shuffle=False)\n",
       "    test_sampler = DistributedSampler(dataset=test_set, shuffle=False) if test_set is not None else None\n",
       "  \n",
       "    train_dataloader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,sampler=train_sampler,num_workers=4,pin_memory=True)\n",
       "    val_dataloader = torch.utils.data.DataLoader(val_set,batch_size=batch_size,sampler=val_sampler,num_workers=4)\n",
       "    test_dataloader = torch.utils.data.DataLoader(test_aset, batch_size, sampler=test_sampler,num_workers=4) if test_set is not None else None\n",
       "    \n",
       "    return train_dataloader, val_dataloader, test_dataloader\n",
       "\n",
       "\n",
       "# Building the Neural Network\n",
       "# This is the same from part 2 of this tutorial\n",
       "def getResNet():\n",
       "    resnet = models.resnet34(weights='IMAGENET1K_V1')\n",
       "\n",
       "    # Fix the conv layers parameters\n",
       "    for conv_param in resnet.parameters():\n",
       "        conv_param.require_grad = False\n",
       "\n",
       "    # get the input dimension for this layer\n",
       "    num_ftrs = resnet.fc.in_features\n",
       "    \n",
       "    # build the new final mlp layers of network\n",
       "    fc = nn.Sequential(\n",
       "          nn.Linear(num_ftrs, num_ftrs),\n",
       "          nn.ReLU(),\n",
       "          nn.Linear(num_ftrs, 3)\n",
       "        )\n",
       "    \n",
       "    # replace final fully connected layer\n",
       "    resnet.fc = fc\n",
       "    return resnet\n",
       "\n",
       "\n",
       "# Model evaluation.\n",
       "# This is implemented in the same way as part 2 of this tutorial.\n",
       "@torch.no_grad()\n",
       "def eval_model(data_loader, model, loss_fn, DEVICE):\n",
       "    model.train(False)\n",
       "    model.eval()\n",
       "    loss, accuracy = 0.0, 0.0\n",
       "    n = len(data_loader)\n",
       "    \n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "\n",
       "    for i, data in enumerate(data_loader):\n",
       "        x,y = data\n",
       "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
       "        pred = model(x)\n",
       "        loss += loss_fn(pred, y)/len(x)\n",
       "        pred_label = torch.argmax(pred, axis = 1)\n",
       "        accuracy += torch.sum(pred_label == y)/len(x)\n",
       "    \n",
       "    return loss/n, accuracy/n\n",
       "\n",
       "\n",
       "# Model training.\n",
       "# This is implemented in the same way as part 2 of this tutorial.\n",
       "def train(train_loader, val_loader, model, opt, scheduler, loss_fn, epoch_start, epochs, DEVICE, checkpoint_file, prev_best_val_acc):\n",
       "    n = len(train_loader)\n",
       "\n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "  \n",
       "    best_val_acc = torch.tensor(0.0).to(DEVICE) if prev_best_val_acc is None else prev_best_val_acc\n",
       "    \n",
       "    for epoch in range(epoch_start, epochs):\n",
       "        model.train(True)\n",
       "    \n",
       "        train_loader.sampler.set_epoch(epoch)\n",
       "    \n",
       "        avg_loss, val_loss, val_acc, avg_acc  = 0.0, 0.0, 0.0, 0.0\n",
       "    \n",
       "        start_time = datetime.now()\n",
       "    \n",
       "        for x, y in train_loader:\n",
       "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
       "            pred = model(x)\n",
       "            loss = loss_fn(pred,y)\n",
       "            \n",
       "            opt.zero_grad()\n",
       "            loss.backward()\n",
       "            opt.step()\n",
       "\n",
       "            avg_loss += loss.item()/len(x)\n",
       "            pred_label = torch.argmax(pred, axis=1)\n",
       "            avg_acc += torch.sum(pred_label == y)/len(x)\n",
       "\n",
       "\n",
       "        val_loss, val_acc = eval_model(val_loader, model, loss_fn, DEVICE)\n",
       "    \n",
       "        end_time = datetime.now()\n",
       "    \n",
       "        total_time = torch.tensor((end_time-start_time).seconds).to(DEVICE)\n",
       "    \n",
       "        # Learning rate reducer takes action\n",
       "        scheduler.step(val_loss)\n",
       "    \n",
       "        avg_loss, avg_acc = avg_loss/n, avg_acc/n\n",
       "    \n",
       "        # Only machine rank==0 (master machine) saves the model and prints the metrics    \n",
       "        if local_rank == 0:\n",
       "        \n",
       "            # Save the best model that has the highest val accuracy\n",
       "            if val_acc.item() > best_val_acc.item():\n",
       "                print(f'\\nlr for this epoch is {scheduler.get_last_lr()}')\n",
       "                print(f\"Prev Best Val Acc: {best_val_acc} < Cur Val Acc: {val_acc}\")\n",
       "                print(\"Saving the new best model...\")\n",
       "                torch.save({\n",
       "                        'epoch':epoch,\n",
       "                        'machine':local_rank,\n",
       "                        'model_state_dict':model.module.state_dict(),\n",
       "                        'accuracy':val_acc,\n",
       "                        'loss':val_loss\n",
       "                }, checkpoint_file)\n",
       "                best_val_acc = val_acc\n",
       "                print(\"Finished saving model\\n\")\n",
       "        \n",
       "            # Print the metrics (should be same on all machines)\n",
       "            print(f\"\\n(Epoch {epoch+1}/{epochs}) Time: {total_time}s\")\n",
       "            print(f\"(Epoch {epoch+1}/{epochs}) Average train loss: {avg_loss}, Average train accuracy: {avg_acc}\")\n",
       "            print(f\"(Epoch {epoch+1}/{epochs}) Val loss: {val_loss}, Val accuracy: {val_acc}\")  \n",
       "            print(f\"(Epoch {epoch+1}/{epochs}) Current best val acc: {best_val_acc}\\n\")  \n",
       "\n",
       "def load_checkpoint(checkpoint_path, DEVICE):\n",
       "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
       "    return checkpoint\n",
       "\n",
       "def load_model_fm_checkpoint(checkpoint, primitive_model):\n",
       "    primitive_model.load_state_dict(checkpoint['model_state_dict'])\n",
       "    return primitive_model \n",
       "\n",
       "def init_distributed():\n",
       "    \n",
       "    dist_url = \"env://\"\n",
       "  \n",
       "    world_size = int(os.environ['WORLD_SIZE']) \n",
       "    local_rank = int(os.environ['LOCAL_RANK']) \n",
       "    dist.init_process_group(backend=\"nccl\", #\"nccl\" for using GPUs, \"gloo\" for using CPUs\n",
       "                            init_method=dist_url,\n",
       "                            world_size=world_size,\n",
       "                            rank=local_rank)\n",
       "    torch.cuda.set_device(local_rank)\n",
       "\n",
       "\n",
       "def cleanup():\n",
       "    print(\"Cleaning up the distributed environment...\")\n",
       "    dist.destroy_process_group()\n",
       "    print(\"Distributed environment has been properly closed\")\n",
       "    \n",
       "    \n",
       "def main():\n",
       "    hp = {\"lr\":1e-4, \"batch_size\":16, \"epochs\":5}\n",
       "    \n",
       "    # Please specify the path to train, cross_validation, and test images below:\n",
       "    train_path = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Train/\")\n",
       "    val_path   = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Validation/\")\n",
       "    test_path  = None\n",
       "    \n",
       "    local_rank = int(os.environ['LOCAL_RANK'])\n",
       "    DEVICE = torch.device(\"cuda\", local_rank)\n",
       "    \n",
       "    model_folder_path = os.path.join(os.environ['SCRATCH'], \"cnn4_damagelevel_output_model\") \n",
       "    os.makedirs(model_folder_path,exist_ok=True)\n",
       "    \n",
       "    # same loss function as part 2 \n",
       "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(DEVICE)\n",
       "    train_set, val_set, test_set = load_datasets(train_path, val_path, test_path)\n",
       "    train_dataloader, val_dataloader, test_dataloader = construct_dataloaders(train_set, val_set, test_set, hp[\"batch_size\"], True)\n",
       "                          \n",
       "    model = getResNet().to(DEVICE)\n",
       "  \n",
       "    # load the checkpoint that has the best performance in previous experiments\n",
       "    prev_best_val_acc = None\n",
       "    checkpoint_file = os.path.join(model_folder_path, \"best_model.pt\")\n",
       "    if os.path.exists(checkpoint_file):\n",
       "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
       "        prev_best_val_acc = checkpoint['accuracy']\n",
       "        model = load_model_fm_checkpoint(checkpoint,model)\n",
       "        epoch_start = checkpoint['epoch']\n",
       "        if local_rank == 0:\n",
       "            print(f\"resuming training from epoch {epoch_start}\")\n",
       "    else:\n",
       "        epoch_start = 0\n",
       "  \n",
       "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
       "    model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
       "    opt = torch.optim.Adam(model.parameters(),lr=hp[\"lr\"])\n",
       "    \n",
       "    # same learning rate scheduler as part 2\n",
       "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',factor=0.1, patience=5, min_lr=1e-8)\n",
       "  \n",
       "    train(train_dataloader, val_dataloader, model, opt, scheduler, loss_fn, epoch_start, hp[\"epochs\"], DEVICE, checkpoint_file, prev_best_val_acc)\n",
       "   \n",
       "\n",
       "  # only the node with rank 0 does the loading, evaluation and printing to avoild duplicate \n",
       "    if local_rank == 0:\n",
       "        primitive_model = getResNet().to(DEVICE)\n",
       "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
       "        best_model = load_model_fm_checkpoint(checkpoint,primitive_model)\n",
       "        loss, acc = eval_model(val_dataloader,best_model,loss_fn,DEVICE)\n",
       "        print(f\"\\nBest model (val loss: {loss}, val accuracy: {acc}) has been saved to {checkpoint_file}\\n\")\n",
       "        cleanup()\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    init_distributed()\n",
       "    \n",
       "    gc.collect()\n",
       "    for i in range(torch.cuda.device_count()):\n",
       "        with torch.cuda.device(f\"cuda:{i}\"):\n",
       "            torch.cuda.empty_cache()\n",
       "  \n",
       "    main()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Code(\"cnn_part4/torch_train_distributed.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc-per-node=4 cnn_part4/torch_train_distributed.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_course_container",
   "language": "python",
   "name": "cnn_course_container"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
