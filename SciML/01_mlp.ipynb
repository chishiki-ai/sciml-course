{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiNuUe-V1Qu_"
   },
   "source": [
    "# Neural Networks and Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE_vzDLg1QvB"
   },
   "source": [
    "**Slides:** [![View PDF](https://img.shields.io/badge/View-PDF-red?style=flat-square&logo=googledocs&logoColor=white)](https://github.com/chishiki-ai/sciml/raw/main/docs/00-mlp/mlp-slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzY2mxM21QvC"
   },
   "source": [
    "This notebook introduces the fundamental concepts of using neural networks for function approximation, a core task in scientific machine learning (SciML). We will build up from the basic building block, the perceptron, to a single-layer network and demonstrate its capacity to approximate a simple function, connecting theory to practice using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOsWFFoP1QvD"
   },
   "source": [
    "## The 1D Poisson Equation: Our Benchmark Problem\n",
    "\n",
    "We begin with a familiar problem: the one-dimensional Poisson equation on the unit interval $[0, 1]$ with homogeneous Dirichlet boundary conditions:\n",
    "\n",
    "$$-\\frac{d^2u}{dx^2} = f(x), \\quad x \\in [0, 1]$$\n",
    "\n",
    "subject to boundary conditions:\n",
    "$$u(0) = 0, \\quad u(1) = 0$$\n",
    "\n",
    "This equation models diverse physical phenomena: heat conduction in a rod, deflection of a loaded beam, or electrostatic potential in one dimension. The function $u(x)$ represents the unknown solution we seek, while $f(x)$ is the prescribed source term.\n",
    "\n",
    "For our initial exploration, we choose a source term that gives a simple, known solution:\n",
    "$$f(x) = \\pi^2 \\sin(\\pi x)$$\n",
    "\n",
    "This choice yields the analytical solution:\n",
    "$$u(x) = \\sin(\\pi x)$$\n",
    "\n",
    "We can verify this solution by direct substitution. The second derivative of $u(x) = \\sin(\\pi x)$ is $u''(x) = -\\pi^2 \\sin(\\pi x)$, so:\n",
    "$$-\\frac{d^2u}{dx^2} = -(-\\pi^2 \\sin(\\pi x)) = \\pi^2 \\sin(\\pi x) = f(x) \\quad \\checkmark$$\n",
    "\n",
    "The boundary conditions are satisfied: $u(0) = \\sin(0) = 0$ and $u(1) = \\sin(\\pi) = 0$ ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T02:20:43.977671Z",
     "start_time": "2025-07-09T02:20:42.207822Z"
    },
    "id": "SGWjf0OM1QvE",
    "outputId": "0dc5a637-a206-41dd-99aa-0402442e25f4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Set up plotting style for clarity\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'figure.figsize': (12, 8),\n",
    "    'lines.linewidth': 2.5,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "# Define the domain\n",
    "x_plot = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Define the analytical solution and source function\n",
    "def analytical_solution(x):\n",
    "    \"\"\"Analytical solution: u(x) = sin(π*x)\"\"\"\n",
    "    return np.sin(np.pi * x)\n",
    "\n",
    "def source_function(x):\n",
    "    \"\"\"Source function: f(x) = π²*sin(π*x)\"\"\"\n",
    "    return np.pi**2 * np.sin(np.pi * x)\n",
    "\n",
    "# Compute solutions for plotting\n",
    "u_analytical_plot = analytical_solution(x_plot)\n",
    "f_source_plot = source_function(x_plot)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot the analytical solution\n",
    "ax1.plot(x_plot, u_analytical_plot, 'b-', linewidth=3, label=r'$u(x) = \\sin(\\pi x)$')\n",
    "ax1.fill_between(x_plot, 0, u_analytical_plot, alpha=0.2, color='blue')\n",
    "ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax1.plot([0, 0], [0, 0], 'ro', markersize=8, label='Boundary: u(0)=0')\n",
    "ax1.plot([1, 1], [0, 0], 'ro', markersize=8, label='Boundary: u(1)=0')\n",
    "ax1.set_xlabel('Position x')\n",
    "ax1.set_ylabel('Solution u(x)')\n",
    "ax1.set_title('Analytical Solution of 1D Poisson Equation', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot the source function\n",
    "ax2.plot(x_plot, f_source_plot, 'r-', linewidth=3, label=r'$f(x) = \\pi^2 \\sin(\\pi x)$')\n",
    "ax2.fill_between(x_plot, 0, f_source_plot, alpha=0.2, color='red')\n",
    "ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax2.set_xlabel('Position x')\n",
    "ax2.set_ylabel('Source f(x)')\n",
    "ax2.set_title('Source Function', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Solution satisfies boundary conditions: u(0) = {analytical_solution(0):.6f}, u(1) = {analytical_solution(1):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIYz2LTc1QvG"
   },
   "source": [
    "## The Function Approximation Challenge\n",
    "\n",
    "Consider this question: *Can we learn to approximate $u(x) = \\sin(\\pi x)$ by observing only sparse data points?*\n",
    "\n",
    "Let's explore this with a concrete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LIzBVwz1QvG",
    "outputId": "bdf9f7aa-1ee5-4d9b-ed8f-d80d637cdbdd"
   },
   "outputs": [],
   "source": [
    "# Generate sparse training data\n",
    "n_training_points = 15\n",
    "x_train_np = np.linspace(0, 1, n_training_points)\n",
    "u_train_np = analytical_solution(x_train_np)\n",
    "\n",
    "# Add small amount of noise to make it more realistic\n",
    "noise_level = 0.02\n",
    "u_train_noisy_np = u_train_np + noise_level * np.random.randn(n_training_points)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train_np.reshape(-1, 1), dtype=torch.float32)\n",
    "u_train_tensor = torch.tensor(u_train_noisy_np.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot the true function\n",
    "x_fine = np.linspace(0, 1, 1000)\n",
    "u_true_fine = analytical_solution(x_fine)\n",
    "ax.plot(x_fine, u_true_fine, 'b-', linewidth=3, label='True Function: $u(x) = \\sin(\\pi x)$', alpha=0.7)\n",
    "\n",
    "# Plot training data\n",
    "ax.plot(x_train_np, u_train_noisy_np, 'ro', markersize=8, label=f'Noisy Training Data ({n_training_points} points, σ={noise_level})', markeredgecolor='darkred')\n",
    "\n",
    "# Add some visual elements to emphasize the challenge\n",
    "for i, xi in enumerate(x_train_np):\n",
    "    ax.axvline(x=xi, color='gray', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Position x', fontsize=14)\n",
    "ax.set_ylabel('Function Value u(x)', fontsize=14)\n",
    "ax.set_title('Function Approximation Challenge: Learning from Sparse Data', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('How can we reconstruct\\nthe smooth function\\nfrom these few points?',\n",
    "            xy=(0.7, 0.8), xytext=(0.75, 0.4),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "            fontsize=12, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Challenge: Approximate a continuous function using only {n_training_points} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VrrjUlD1QvH"
   },
   "source": [
    "## Traditional Methods: Finite Difference\n",
    "\n",
    "Traditional numerical methods like the Finite Difference Method or Finite Element Method solve PDEs by **discretizing the domain** into a grid or mesh. They approximate the solution $u(x)$ by finding its values at these specific, discrete points.\n",
    "\n",
    "For example, the Finite Difference method approximates the second derivative:\n",
    "$$\\frac{d^2u}{dx^2} \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}$$\n",
    "This transforms the differential equation into a system of algebraic equations for the values $u_i$ at grid points $x_i$. The result is a discrete representation of the solution.\n",
    "\n",
    "![FDM](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/finite-difference-methods.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-BPj0SJ1QvI",
    "outputId": "6a3dee6f-db72-493c-efc1-f5a02b22268c"
   },
   "outputs": [],
   "source": [
    "# Simple finite difference solution for comparison\n",
    "def solve_poisson_fd(n_points=51):\n",
    "    \"\"\"Solve 1D Poisson equation using finite differences\"\"\"\n",
    "    # Create grid\n",
    "    x_fd = np.linspace(0, 1, n_points)\n",
    "    h = x_fd[1] - x_fd[0]\n",
    "\n",
    "    # Create coefficient matrix A for -u'' = f\n",
    "    # Central difference: u''_i ≈ (u_{i+1} - 2u_i + u_{i-1})/h²\n",
    "    # Boundary conditions u_0 = u_{N-1} = 0 are handled by reducing the system size\n",
    "    A = np.zeros((n_points-2, n_points-2))\n",
    "    np.fill_diagonal(A, -2.0 / h**2)\n",
    "    np.fill_diagonal(A[1:], 1.0 / h**2)\n",
    "    np.fill_diagonal(A[:, 1:], 1.0 / h**2)\n",
    "\n",
    "    # Right-hand side (source function at interior points)\n",
    "    f_rhs = source_function(x_fd[1:-1])\n",
    "\n",
    "    # Solve linear system A u_interior = -f_rhs\n",
    "    u_interior = np.linalg.solve(A, -f_rhs)  # Note: A is for -u''\n",
    "\n",
    "    # Assemble full solution (including boundary conditions)\n",
    "    u_fd = np.zeros(n_points)\n",
    "    u_fd[1:-1] = u_interior\n",
    "    u_fd[0] = 0  # u(0) = 0\n",
    "    u_fd[-1] = 0  # u(1) = 0\n",
    "\n",
    "    return x_fd, u_fd\n",
    "\n",
    "\n",
    "# Solve using finite differences with different resolutions\n",
    "x_fd_coarse, u_fd_coarse = solve_poisson_fd(11)  # Coarse grid\n",
    "x_fd_fine, u_fd_fine = solve_poisson_fd(51)      # Fine grid\n",
    "\n",
    "# Create comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Method comparison\n",
    "x_exact = np.linspace(0, 1, 1000)\n",
    "u_exact = analytical_solution(x_exact)\n",
    "\n",
    "ax1.plot(x_exact, u_exact, 'k-', linewidth=3, label='Analytical Solution', alpha=0.8)\n",
    "ax1.plot(x_fd_coarse, u_fd_coarse, 'ro-', linewidth=2, markersize=6, label='Finite Difference (Coarse)', alpha=0.8)\n",
    "ax1.plot(x_fd_fine, u_fd_fine, 'b.-', linewidth=1, markersize=4, label='Finite Difference (Fine)', alpha=0.8)\n",
    "ax1.scatter(x_train_np, u_train_noisy_np, color='green', s=80, label='Neural Network Training Data',\n",
    "           edgecolors='darkgreen', linewidth=2, zorder=5)\n",
    "\n",
    "ax1.set_xlabel('Position x')\n",
    "ax1.set_ylabel('Solution u(x)')\n",
    "ax1.set_title('Comparison of Solution Methods', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error analysis\n",
    "error_coarse = np.abs(u_fd_coarse - analytical_solution(x_fd_coarse))\n",
    "error_fine = np.abs(u_fd_fine - analytical_solution(x_fd_fine))\n",
    "\n",
    "ax2.semilogy(x_fd_coarse, error_coarse, 'ro-', linewidth=2, markersize=6, label='Coarse Grid Error')\n",
    "ax2.semilogy(x_fd_fine, error_fine, 'b.-', linewidth=1, markersize=4, label='Fine Grid Error')\n",
    "ax2.set_xlabel('Position x')\n",
    "ax2.set_ylabel('Absolute Error (log scale)')\n",
    "ax2.set_title('Discretization Error Analysis', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print error statistics\n",
    "print(f\"Finite Difference Error Statistics:\")\n",
    "print(f\"Coarse grid (11 points): Max error = {np.max(error_coarse):.6f}\")\n",
    "print(f\"Fine grid (51 points):   Max error = {np.max(error_fine):.6f}\")\n",
    "print(f\"Error reduction factor: {np.max(error_coarse)/np.max(error_fine):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVuaOR1F1QvJ"
   },
   "source": [
    "## The Neural Network Approach: Function Approximation and Universal Approximation\n",
    "\n",
    "Our goal is to train a neural network $u_{NN}(x; \\theta)$ to approximate the continuous solution $u^*(x) = \\sin(\\pi x)$ over the interval $[0, 1]$. This is a **function approximation** problem.\n",
    "\n",
    "A key theoretical result in neural networks is the **Universal Approximation Theorem**. In essence, it states:\n",
    "\n",
    "**Theorem (Cybenko, 1989; Hornik, 1991)**: A feedforward network with a single hidden layer, containing a finite number of neurons and using a non-constant, bounded, and monotonically increasing activation function (like Sigmoid or Tanh), can approximate any continuous function on a compact domain to arbitrary accuracy.\n",
    "\n",
    "$$F(x) = \\sum_{i=1}^{N} w_i \\sigma(v_i x + b_i) + w_0$$\n",
    "\n",
    "**Mathematical statement**: For any continuous $f: [0,1] \\to \\mathbb{R}$ and $\\epsilon > 0$, there exists $N$ and parameters such that $|F(x) - f(x)| < \\epsilon$ for all $x \\in [0,1]$.\n",
    "\n",
    "*(Note: While the original theorem had specific activation requirements, it has been extended to other common activations like ReLU in practice).*\n",
    "\n",
    "\n",
    "The significance of this theorem is profound: it tells us that even a relatively simple network architecture (a single hidden layer) has the **theoretical capacity** to learn complex, non-linear functions like $\\sin(\\pi x)$, provided it has enough neurons and uses the right kind of non-linearity. We will experimentally demonstrate this capacity.\n",
    "\n",
    "### Traditional Numerical Method vs Neural Network: Discrete vs Continuous\n",
    "\n",
    "In contrast, the Neural Network approach aims to learn a **continuous function** $u_{NN}(x; \\theta)$ that approximates the true solution $u^*(x)$ over the entire domain $[0, 1]$.\n",
    "\n",
    "- This function is parameterized by the network's weights and biases $\\theta$.\n",
    "\n",
    "- We train the network by showing it examples of the solution at sparse points $(x_i, u_i)$ and adjusting $\\theta$ so the network's output $u_{NN}(x_i; \\theta)$ matches $u_i$ as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "383JJRrq1QvJ"
   },
   "source": [
    "## The Basic Building Block: The Perceptron\n",
    "\n",
    "The fundamental unit of a neural network is the **perceptron**, or artificial neuron. It takes an input vector $\\boldsymbol{x}$, computes a weighted sum of its elements, adds a bias, and passes the result through an activation function $g$.\n",
    "\n",
    "Mathematically, for an input vector $\\boldsymbol{x} = [x_1, x_2, ..., x_n]$ and corresponding weights $\\boldsymbol{w} = [w_1, w_2, ..., w_n]$, with a bias $b$, the operation is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= \\mathrm{bias} + \\mathrm{linear\\_ combination\\_ of \\_ inputs}\n",
    "z &= \\boldsymbol{w}^T\\boldsymbol{x} + b \\\\\n",
    " &= \\sum_{i=1}^n w_i x_i + b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The output $\\hat{y}$ is then computed by passing it through an activation function:\n",
    "\n",
    "$$\\hat{y} = g(z)$$\n",
    "\n",
    "Here, $z$ is the pre-activation or 'logit' value, and $g$ is the activation function. The activation function $g$ introduces nonlinearity to allow the perceptron to learn complex mappings from inputs to outputs. Typical choices for $g$ are `sigmoid`, `tanh`, or `ReLU` functions, though the original perceptron used a step function.\n",
    "\n",
    "\n",
    "The perceptron can be trained via supervised learning, adjusting the weights and biases to minimize the loss between the predicted $\\hat{y}$ and the true label $y^{\\text{true}}$. Backpropagation combined with gradient descent can be used to iteratively update the weights to reduce the loss.\n",
    "\n",
    "The key components of a perceptron are:\n",
    "* Input vector $\\boldsymbol{x}$\n",
    "* Weight vector $\\boldsymbol{w}$\n",
    "* Weighted sum $z = \\boldsymbol{w}^T\\boldsymbol{x}$\n",
    "* Nonlinear activation $g$\n",
    "* Output prediction $\\hat{y}$\n",
    "\n",
    "The perceptron provides a basic model of a neuron, and multilayer perceptrons composed of many interconnected perceptrons can be used to build neural networks with substantial representational power. A perceptron takes a set of inputs, scales them by corresponding weights, sums them together with a bias, applies a non-linear step function, and produces an output. This simple model can represent linear decision boundaries and serves as a building block for more complex neural networks. In training, weights are updated based on the difference between the predicted output and the actual label, often using the Perceptron learning algorithm.\n",
    "\n",
    "\n",
    "![Perceptron](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/perceptron.png?raw=1)\n",
    "\n",
    "> Credits: Alexander Amini, MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xvIXAP41QvK"
   },
   "source": [
    "## The Critical Role of Nonlinearity\n",
    "\n",
    "Why do we need the activation function $g$? What happens if we just use a linear function, like $g(z) = z$?\n",
    "\n",
    "Consider a network with multiple layers, but *no* non-linear activation functions between them. The output of one layer is just a linear transformation of its input. If we stack these linear layers:\n",
    "\n",
    "Let the first layer be $h_1 = W_1 x + b_1$.\n",
    "Let the second layer be $h_2 = W_2 h_1 + b_2$.\n",
    "\n",
    "Substituting the first into the second:\n",
    "$$h_2 = W_2 (W_1 x + b_1) + b_2$$\n",
    "$$h_2 = W_2 W_1 x + W_2 b_1 + b_2$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$h_2 = (W_2 W_1) x + (W_2 b_1 + b_2)$$\n",
    "\n",
    "Let $W_{eq} = W_2 W_1$ and $b_{eq} = W_2 b_1 + b_2$. Then:\n",
    "$$h_2 = W_{eq} x + b_{eq}$$\n",
    "\n",
    "This is just another linear transformation! No matter how many linear layers we stack, the entire network will only be able to compute a single linear function of the input. A linear network can only learn:\n",
    "\n",
    "> Linear network can only learn: y = mx + b (a straight line in 1D)\n",
    "> But sin(πx) is curved - impossible with just linear transformations!\n",
    "\n",
    "To approximate complex, non-linear functions like $\\sin(\\pi x)$, we **must** introduce non-linearity using activation functions between the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziP4A9yH1QvK",
    "outputId": "b20f1a05-cf68-41bc-f776-719428f4b7f8"
   },
   "outputs": [],
   "source": [
    "# Demonstrate Linear Network Failure\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"A simple network with only linear layers (no activation)\"\"\"\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            nn.Linear(width, width), # Another linear layer\n",
    "            nn.Linear(width, 1)      # Final linear layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Train the linear network\n",
    "linear_model = LinearNetwork(width=10)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(linear_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 3000\n",
    "for epoch in range(epochs):\n",
    "    predictions = linear_model(x_train_tensor)\n",
    "    loss = criterion(predictions, u_train_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Visualize the linear network's prediction\n",
    "x_test_tensor = torch.tensor(x_plot.reshape(-1, 1), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    u_pred_linear = linear_model(x_test_tensor).numpy().flatten()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(x_plot, u_analytical_plot, 'b-', linewidth=3, label='True Function: $\\sin(\\pi x)$', alpha=0.7)\n",
    "ax.plot(x_plot, u_pred_linear, 'r--', linewidth=2, label='Linear Network Prediction')\n",
    "ax.scatter(x_train_np, u_train_noisy_np, color='k', s=40, alpha=0.7, label='Training Data', zorder=5)\n",
    "\n",
    "ax.set_xlabel('Position x')\n",
    "ax.set_ylabel('Function Value u(x)')\n",
    "ax.set_title('Linear Network Failure to Approximate $\\sin(\\pi x)$', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Loss (Linear Network): {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F0bxazs1QvK"
   },
   "source": [
    "## Introducing Nonlinearity: Activation Functions\n",
    "\n",
    "Activation functions are applied element-wise to the output of a linear transformation within a neuron or layer. They introduce the non-linearity required for neural networks to learn complex mappings.\n",
    "\n",
    "Some common activation functions include:\n",
    "\n",
    "**Sigmoid**\n",
    "Squashes input to (0, 1). Useful for binary classification output. Can suffer from vanishing gradients.\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "**Tanh**\n",
    "Squashes input to (-1, 1). Zero-centered, often preferred over Sigmoid for hidden layers. Can also suffer from vanishing gradients.\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**\n",
    "Outputs input directly if positive, zero otherwise. Computationally efficient, helps mitigate vanishing gradients for positive inputs. Can suffer from \"dead neurons\" if inputs are always negative.\n",
    "$$ f(x) = \\max(0, x)$$\n",
    "\n",
    "**LeakyReLU**\n",
    "Similar to ReLU but allows a small gradient for negative inputs, preventing dead neurons.\n",
    "$$f(x) = \\max(\\alpha x, x) \\quad (\\alpha \\text{ is a small positive constant, e.g., 0.01})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y20GWaE61QvK",
    "outputId": "811d8173-84de-4a61-da0e-3e5efa87c81a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Parameterized Activation Functions ---\n",
    "\n",
    "def sigmoid(x, a=1.0):\n",
    "    \"\"\"\n",
    "    Parameterized Sigmoid activation function.\n",
    "    'a' controls the steepness.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-a * x))\n",
    "\n",
    "def tanh(x, a=1.0):\n",
    "    \"\"\"\n",
    "    Parameterized Hyperbolic Tangent activation function.\n",
    "    'a' controls the steepness.\n",
    "    \"\"\"\n",
    "    return np.tanh(a * x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) activation function.\n",
    "    It has no parameters.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Parameterized Leaky ReLU activation function.\n",
    "    'alpha' is the slope for negative inputs.\n",
    "    \"\"\"\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "# --- 2. Setup for Plotting ---\n",
    "\n",
    "# Input data range\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Create a 2x2 subplot grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Common Activation Functions (Parameterized)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- 3. Plotting each function on its subplot ---\n",
    "\n",
    "# Sigmoid Plot (Top-Left)\n",
    "axs[0, 0].plot(x, sigmoid(x, a=1), label='a=1 (Standard)')\n",
    "axs[0, 0].plot(x, sigmoid(x, a=2), label='a=2 (Steeper)', linestyle='--')\n",
    "axs[0, 0].plot(x, sigmoid(x, a=0.5), label='a=0.5 (Less Steep)', linestyle=':')\n",
    "axs[0, 0].set_title('Sigmoid Function')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Tanh Plot (Top-Right)\n",
    "axs[0, 1].plot(x, tanh(x, a=1), label='a=1 (Standard)')\n",
    "axs[0, 1].plot(x, tanh(x, a=2), label='a=2 (Steeper)', linestyle='--')\n",
    "axs[0, 1].plot(x, tanh(x, a=0.5), label='a=0.5 (Less Steep)', linestyle=':')\n",
    "axs[0, 1].set_title('Tanh Function')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# ReLU Plot (Bottom-Left)\n",
    "axs[1, 0].plot(x, relu(x), label='ReLU')\n",
    "axs[1, 0].set_title('ReLU Function')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Leaky ReLU Plot (Bottom-Right)\n",
    "axs[1, 1].plot(x, leaky_relu(x, alpha=0.1), label='α=0.1 (Standard)')\n",
    "axs[1, 1].plot(x, leaky_relu(x, alpha=0.3), label='α=0.3', linestyle='--')\n",
    "axs[1, 1].plot(x, leaky_relu(x, alpha=0.01), label='α=0.01', linestyle=':')\n",
    "axs[1, 1].set_title('Leaky ReLU Function')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# --- 4. Final Touches for all subplots ---\n",
    "\n",
    "# Apply common labels, grids, and axis lines to all subplots\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlabel('Input z')\n",
    "    ax.set_ylabel('Output g(z)')\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.8)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.8)\n",
    "\n",
    "# Adjust layout to prevent titles and labels from overlapping\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ6BEnN51QvL"
   },
   "source": [
    "## Interactive Demo: Visualizing Nonlinear Transformation\n",
    "\n",
    "This interactive demo illustrates how a combination of linear transformation and non-linearity can transform data in a way that linear transformations alone cannot. Observe how the data, initially not linearly separable in the input space (X), becomes separable after passing through a linear layer (Y) and then a non-linear activation (Z).\n",
    "\n",
    "This provides intuition for why layers with non-linear activations are powerful: they can map data into a new space where complex patterns become simpler (potentially linearly separable), making them learnable by subsequent layers.\n",
    "\n",
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo:ReLU-blue?style=for-the-badge)](https://cvw.cac.cornell.edu/SciML/mlp/demo-nonlinear-transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz5_UKX-1QvL"
   },
   "source": [
    "## Building Capacity: The Single Hidden Layer Neural Network\n",
    "\n",
    "A single perceptron is limited in the complexity of functions it can represent. To increase capacity, we combine multiple perceptrons into a **layer**. A single-layer feedforward neural network (also known as a shallow Multi-Layer Perceptron or MLP) consists of an input layer, one hidden layer of neurons, and an output layer.\n",
    "\n",
    "For our 1D input $x$, a single-layer network with $N_h$ hidden neurons works as follows:\n",
    "\n",
    "1.  **Input Layer**: Receives the input $x$.\n",
    "2.  **Hidden Layer**: Each of the $N_h$ neurons in this layer performs a linear transformation on the input $x$ and applies a non-linear activation function $g$. The output of this layer is a vector $\\boldsymbol{h}$ of size $N_h$.\n",
    "    *   Pre-activation vector $\\boldsymbol{z}^{(1)}$ (size $N_h$): $\\boldsymbol{z}^{(1)} = W^{(1)}\\boldsymbol{x} + \\boldsymbol{b}^{(1)}$\n",
    "        (Here, $W^{(1)}$ is a $N_h \\times 1$ weight matrix, $\\boldsymbol{x}$ is treated as a $1 \\times 1$ vector, and $\\boldsymbol{b}^{(1)}$ is a $N_h \\times 1$ bias vector).\n",
    "    *   Activation vector $\\boldsymbol{h}$ (size $N_h$): $\\boldsymbol{h} = g(\\boldsymbol{z}^{(1)})$ (where $g$ is applied element-wise).\n",
    "3.  **Output Layer**: This layer takes the vector $\\boldsymbol{h}$ from the hidden layer and performs another linear transformation to produce the final scalar output $\\hat{y}$. For regression, the output layer typically has a linear activation (or no activation function explicitly applied after the linear transformation).\n",
    "    *   Pre-activation scalar $z^{(2)}$: $z^{(2)} = W^{(2)}\\boldsymbol{h} + b^{(2)}$\n",
    "        (Here, $W^{(2)}$ is a $1 \\times N_h$ weight matrix, and $b^{(2)}$ is a scalar bias).\n",
    "    *   Final output $\\hat{y}$: $\\hat{y} = z^{(2)}$\n",
    "\n",
    "![Single layer NN](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/single-layer-nn2.png?raw=1)\n",
    "\n",
    "> Credits: Alexander Amini, MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k30K-nER1QvM"
   },
   "outputs": [],
   "source": [
    "# Implement Single-Layer NN in PyTorch\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    \"\"\"Single hidden layer neural network for 1D input/output\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=10):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "\n",
    "        # Input layer (1D) to Hidden layer (hidden_size)\n",
    "        self.hidden = nn.Linear(1, hidden_size)\n",
    "\n",
    "        # Hidden layer (hidden_size) to Output layer (1D)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Choose activation function for the hidden layer\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through hidden layer and apply activation\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # Pass through output layer (linear output)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jO3oGER1QvM"
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Neural networks are trained using an optimization algorithm that iteratively updates the network's weights and biases to minimize a loss function. The loss function measures how far the network's predictions are from the true target outputs in the training data. It is a measure of the model's error.\n",
    "\n",
    "We quantify this difference using a **Loss Function**, Some common loss functions include:\n",
    "\n",
    "* Mean squared error (MSE) - The average of the squared differences between the predicted and actual values. Measures the square of the error. Used for regression problems.\n",
    "\n",
    "* Cross-entropy loss - Measures the divergence between the predicted class probabilities and the true distribution. Used for classification problems. Penalizes confident incorrect predictions.\n",
    "\n",
    "* Hinge loss - Used for Support Vector Machines classifiers. Penalizes predictions that are on the wrong side of the decision boundary.\n",
    "\n",
    "For our function approximation (regression) task, the Mean Squared Error (MSE) is a common choice:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left(u_{NN}(x_i; \\theta) - u_i\\right)^2$$\n",
    "\n",
    "Minimizing this loss function with respect to the parameters $\\theta$ is an optimization problem.\n",
    "\n",
    "\n",
    "Loss optimization is the process of finding the network weights that acheives the lowest loss.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{w^*} &= \\arg\\min_{\\boldsymbol{w}}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(f(x^{(i)};\\boldsymbol{w}),y^{(i)})\\\\\n",
    "\\boldsymbol{w^*} &= \\arg\\min_{\\boldsymbol{w}} J(\\boldsymbol{w})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The training process works like this:\n",
    "\n",
    "1. **Initialization**: The weights and biases of the network are initialized, often with small random numbers.\n",
    "\n",
    "2. **Forward Pass**: The input is passed through the network, layer by layer, applying the necessary transformations (e.g., linear combinations of weights and inputs followed by activation functions) until an output is obtained.\n",
    "\n",
    "3. **Calculate Loss**: A loss function is used to quantify the difference between the predicted output and the actual target values.\n",
    "\n",
    "4. **Backward Pass (Backpropagation)**: The gradients of the loss with respect to the parameters (weights and biases) are computed using the chain rule for derivatives. This process is known as backpropagation.\n",
    "\n",
    "5. **Update Parameters**: The gradients computed in the backward pass are used to update the parameters of the network, typically using optimization algorithms like stochastic gradient descent (SGD) or more sophisticated ones like Adam. The update is done in the direction that minimizes the loss.\n",
    "\n",
    "6. **Repeat**: Steps 2-5 are repeated using the next batch of data until a stopping criterion is met, such as a set number of epochs (full passes through the training dataset) or convergence to a minimum loss value.\n",
    "\n",
    "7. **Validation**: The model is evaluated on a separate validation set to assess its generalization to unseen data.\n",
    "\n",
    "The goal of training is to find the optimal set of weights and biases $\\theta^*$ for the network that minimize the difference between the network's output $u_{NN}(x; \\theta)$ and the true training data $u_{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBao9cnB1QvN"
   },
   "source": [
    "## Computing gradients with Automatic Differentiation\n",
    "\n",
    "> The Core Insight: Functions Are Computational Graphs\n",
    "\n",
    "Every computer program that evaluates a mathematical function can be viewed as a **computational graph**. Consider this simple function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbV4NXR81QvN"
   },
   "source": [
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](https://cvw.cac.cornell.edu/SciML/mlp/demo-automatic-differentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHpySqgo1QvN"
   },
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    y = x1**2 + x2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkSYZEaJ1QvN"
   },
   "source": [
    "This creates a computational graph where each operation is a node. This decomposition is the key insight that makes automatic differentiation possible.\n",
    "\n",
    "![AD forward pass](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/ad3.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFfa0IBZ1QvN"
   },
   "source": [
    "## Forward Mode Automatic Differentiation\n",
    "\n",
    "Forward mode AD computes derivatives by propagating derivative information **forward** through the computational graph, following the same path as the function evaluation.\n",
    "\n",
    "![AD forward evaluation](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/forward-mode-ad.png?raw=1)\n",
    "\n",
    "### Forward Mode: Computing $\\frac{\\partial y}{\\partial x_1}$\n",
    "\n",
    "Starting with our function $y = x_1^2 + x_2$, let's trace through the computation:\n",
    "\n",
    "1. **Seed the input**: Set $\\dot{x}_1 = 1$ and $\\dot{x}_2 = 0$ (we're differentiating w.r.t. $x_1$)\n",
    "\n",
    "2. **Forward propagation**:\n",
    "   - $v_1 = x_1^2$, so $\\dot{v}_1 = 2x_1 \\cdot \\dot{x}_1 = 2x_1 \\cdot 1 = 2x_1$\n",
    "   - $y = v_1 + x_2$, so $\\dot{y} = \\dot{v}_1 + \\dot{x}_2 = 2x_1 + 0 = 2x_1$\n",
    "\n",
    "3. **Result**: $\\frac{\\partial y}{\\partial x_1} = 2x_1$\n",
    "\n",
    "### Forward Mode: Computing $\\frac{\\partial y}{\\partial x_2}$\n",
    "\n",
    "To get the derivative w.r.t. $x_2$, we seed differently:\n",
    "\n",
    "1. **Seed the input**: Set $\\dot{x}_1 = 0$ and $\\dot{x}_2 = 1$\n",
    "\n",
    "2. **Forward propagation**:\n",
    "   - $v_1 = x_1^2$, so $\\dot{v}_1 = 2x_1 \\cdot \\dot{x}_1 = 2x_1 \\cdot 0 = 0$\n",
    "   - $y = v_1 + x_2$, so $\\dot{y} = \\dot{v}_1 + \\dot{x}_2 = 0 + 1 = 1$\n",
    "\n",
    "3. **Result**: $\\frac{\\partial y}{\\partial x_2} = 1$\n",
    "\n",
    "**Key insight**: Forward mode requires one pass per input variable to compute all partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U35SYvrM1QvN"
   },
   "source": [
    "## Reverse Mode Automatic Differentiation\n",
    "\n",
    "Reverse mode AD (also called **backpropagation**) computes derivatives by propagating derivative information **backward** through the computational graph.\n",
    "\n",
    "### The Backward Pass Algorithm\n",
    "\n",
    "1. **Forward pass**: Compute function values and store intermediate results\n",
    "2. **Seed the output**: Set $\\bar{y} = 1$ (derivative of output w.r.t. itself)\n",
    "3. **Backward pass**: Use the chain rule to propagate derivatives backward\n",
    "\n",
    "![Final chain rule AD](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/ad7.png?raw=1)\n",
    "\n",
    "### Computing All Partial Derivatives in One Pass\n",
    "\n",
    "The beauty of reverse mode is that it computes **all** partial derivatives in a single backward pass:\n",
    "\n",
    "1. **Forward pass**: $y = x_1^2 + x_2$ (store intermediate values)\n",
    "\n",
    "2. **Backward pass with $\\bar{y} = 1$**:\n",
    "   - $\\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial x_1} = 1 \\cdot 2x_1 = 2x_1$\n",
    "   - $\\frac{\\partial y}{\\partial x_2} = \\frac{\\partial y}{\\partial x_2} = 1$\n",
    "\n",
    "**Key insight**: Reverse mode computes gradients w.r.t. all inputs in a single backward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PR5uJV8M1QvN"
   },
   "source": [
    "### AD: The Mathematical Foundation\n",
    "\n",
    "Automatic differentiation works because of a fundamental theorem:\n",
    "\n",
    "**Chain Rule**: For composite functions $f(g(x))$:\n",
    "$$\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "By systematically applying the chain rule to each operation in a computational graph, AD can compute exact derivatives for arbitrarily complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai7eWGAA1QvO"
   },
   "source": [
    "### Automatic Differentiation in Practice: PyTorch\n",
    "\n",
    "Let's see how automatic differentiation works in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vPVXeWH1QvO",
    "outputId": "7f2f2146-8348-458c-a5e2-b4a693b5c0b1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define variables that require gradients\n",
    "x1 = torch.tensor(2.0, requires_grad=True)\n",
    "x2 = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define the function\n",
    "y = x1**2 + x2\n",
    "\n",
    "# Compute gradients using reverse mode AD\n",
    "y.backward()\n",
    "\n",
    "# Access the computed gradients\n",
    "print(f\"dy/dx1: {x1.grad.item()}\")  # Should be 2*x1 = 4.0\n",
    "print(f\"dy/dx2: {x2.grad.item()}\")  # Should be 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDlFuSKa1QvO"
   },
   "source": [
    "### A More Complex Example: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KDfu6Rf1QvO",
    "outputId": "3d362706-7476-4a0b-c00b-1c9486c87623"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Implement Single-Layer NN in PyTorch\n",
    "\n",
    "class SingleLayerNN(nn.Module):\n",
    "    \"\"\"Single hidden layer neural network for 1D input/output\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=10):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.hidden = nn.Linear(1, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Create network and data\n",
    "model = SingleLayerNN(hidden_size=10)\n",
    "\n",
    "# Define MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass: compute predictions\n",
    "predictions = model(x_train_tensor)\n",
    "\n",
    "# Calculate loss (((output - target)**2).mean())\n",
    "loss = criterion(predictions, u_train_tensor)\n",
    "\n",
    "# Backward pass: compute gradients\n",
    "loss.backward()       # Compute gradients of the loss w.r.t. parameters\n",
    "\n",
    "# Access gradients\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: gradient shape {param.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y6mRAnr1QvP"
   },
   "source": [
    "### When to Use Forward vs Reverse Mode\n",
    "\n",
    "The choice depends on the structure of your problem:\n",
    "\n",
    "- **Forward Mode**: Efficient when **few inputs, many outputs** (e.g., $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ with $n \\ll m$)\n",
    "- **Reverse Mode**: Efficient when **many inputs, few outputs** (e.g., $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ with $n \\gg m$)\n",
    "\n",
    "In machine learning, we typically have millions of parameters (inputs) and a single loss function (output), making reverse mode the natural choice.\n",
    "\n",
    "### Computational Considerations\n",
    "\n",
    "#### Memory vs Computation Trade-offs\n",
    "\n",
    "**Forward Mode**:\n",
    "- Memory: O(1) additional storage\n",
    "- Computation: O(n) for n input variables\n",
    "\n",
    "**Reverse Mode**:\n",
    "- Memory: O(computation graph size)\n",
    "- Computation: O(1) for any number of input variables\n",
    "\n",
    "#### Modern Optimizations\n",
    "\n",
    "1. **Checkpointing**: Trade computation for memory by recomputing intermediate values\n",
    "2. **JIT compilation**: Compile computational graphs for faster execution\n",
    "3. **Parallelization**: Distribute gradient computation across multiple devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj8txQRa1QvP"
   },
   "source": [
    "### Gradient Descent\n",
    "Gradient Descent is a first-order iterative optimization algorithm used to find the minimum of a differentiable function. In the context of training a neural network, we are trying to minimize the loss function.\n",
    "\n",
    "1. **Initialize Parameters**:\n",
    "\n",
    "Choose an initial point (i.e., initial values for the weights and biases) in the parameter space, and set a learning rate that determines the step size in each iteration.\n",
    "\n",
    "2. **Compute the Gradient**:\n",
    "\n",
    "Calculate the gradient of the loss function with respect to the parameters at the current point. The gradient is a vector that points in the direction of the steepest increase of the function. It is obtained by taking the partial derivatives of the loss function with respect to each parameter.\n",
    "\n",
    "3. **Update Parameters**:\n",
    "\n",
    "Move in the opposite direction of the gradient by a distance proportional to the learning rate. This is done by subtracting the gradient times the learning rate from the current parameters:\n",
    "\n",
    "$$\\boldsymbol{w} = \\boldsymbol{w} - \\eta \\nabla J(\\boldsymbol{w})$$\n",
    "\n",
    "Here, $\\boldsymbol{w}$ represents the parameters, $\\eta$ is the learning rate, and $\\nabla J (\\boldsymbol{w})$ is the gradient of the loss function $J$ with respect to $\\boldsymbol{w}$.\n",
    "\n",
    "4. **Repeat**:\n",
    "\n",
    "Repeat steps 2 and 3 until the change in the loss function falls below a predefined threshold, or a maximum number of iterations is reached.\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "1. Initialize weights randomly $\\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "2. Loop until convergence\n",
    "3.   Compute gradient, $\\frac{\\partial J(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}$\n",
    "4.   Update weights, $\\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\eta \\frac{\\partial J(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}$\n",
    "5. Return weights\n",
    "\n",
    "![SGD](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/sgd.gif?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19U1F3rJ1QvP"
   },
   "source": [
    "Assuming a loss function is mean squared error (MSE). Let's compute the gradient of the loss with respect to the input weights.\n",
    "\n",
    "The loss function is mean squared error:\n",
    "\n",
    "$$\\text{loss} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where $y_i$ are the true target and $\\hat{y}_i$ are the predicted values.\n",
    "\n",
    "To minimize this loss, we need to compute the gradients with respect to the weights $\\mathbf{w}$ and bias $b$:\n",
    "\n",
    "Using the chain rule, the gradient of the loss with respect to the weights is:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial \\mathbf{w}} = \\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i) \\frac{\\partial y_i}{\\partial \\mathbf{w}}$$\n",
    "\n",
    "The term inside the sum is the gradient of the loss with respect to the output $y_i$, which we called $\\text{grad_output}$:\n",
    "$$\\text{grad_output} = \\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$\n",
    "\n",
    "The derivative $\\frac{\\partial y_i}{\\partial \\mathbf{w}}$ is just the input $\\mathbf{x}_i$ multiplied by the derivative of the activation. For simplicity, let's assume linear activation, so this is just $\\mathbf{x}_i$:\n",
    "\n",
    "$$\\therefore \\frac{\\partial \\text{loss}}{\\partial \\mathbf{w}} = \\mathbf{X}^T\\text{grad_output}$$\n",
    "\n",
    "The gradient for the bias is simpler:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial b} = \\sum_{i=1}^{n}\\text{grad_output}_i$$\n",
    "\n",
    "Finally, we update the weights and bias by gradient descent:\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{w} - \\eta \\frac{\\partial \\text{loss}}{\\partial \\mathbf{w}}$$\n",
    "\n",
    "$$b = b - \\eta \\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "#### Variants:\n",
    "\n",
    "There are several variants of Gradient Descent that modify or enhance these basic steps, including:\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**: Instead of using the entire dataset to compute the gradient, SGD uses a single random data point (or small batch) at each iteration. This adds noise to the gradient but often speeds up convergence and can escape local minima.\n",
    "\n",
    "- **Momentum**: Momentum methods use a moving average of past gradients to dampen oscillations and accelerate convergence, especially in cases where the loss surface has steep valleys.\n",
    "\n",
    "- **Adaptive Learning Rate Methods**: Techniques like Adagrad, RMSprop, and Adam adjust the learning rate individually for each parameter, often leading to faster convergence.\n",
    "\n",
    "#### Limitations:\n",
    "\n",
    "* It may converge to a local minimum instead of a global minimum if the loss surface is not convex.\n",
    "* Convergence can be slow if the learning rate is not properly tuned.\n",
    "* Sensitive to the scaling of features; poorly scaled data can cause the gradient descent to take a long time to converge or even diverge.\n",
    "\n",
    "#### Effect of learning rate\n",
    "\n",
    "The learning rate in gradient descent is a critical hyperparameter that can significantly influence the model's training dynamics. Let us now look at how the learning rate affects local minima, overshooting, and convergence:\n",
    "\n",
    "1. Effect on Local Minima:\n",
    "\n",
    "- High Learning Rate: A large learning rate can help the model escape shallow local minima, leading to the discovery of deeper (potentially global) minima. However, it can also cause instability, making it hard to settle in a good solution.\n",
    "\n",
    "- Low Learning Rate: A small learning rate may cause the model to get stuck in local minima, especially in complex loss landscapes with many shallow valleys. The model can lack the \"energy\" to escape these regions.\n",
    "\n",
    "2. Effect on Overshooting:\n",
    "\n",
    "- High Learning Rate: If the learning rate is set too high, the updates may be so large that they overshoot the minimum and cause the algorithm to diverge, or oscillate back and forth across the valley without ever reaching the bottom. This oscillation can be detrimental to convergence.\n",
    "   \n",
    "- Low Learning Rate: A very low learning rate will likely avoid overshooting but may lead to extremely slow convergence, as the updates to the parameters will be minimal. It might result in getting stuck in plateau regions where the gradient is small.\n",
    "\n",
    "3. Effect on Convergence:\n",
    "\n",
    "- High Learning Rate: While it can speed up convergence initially, a too-large learning rate risks instability and divergence, as mentioned above. The model may never converge to a satisfactory solution.\n",
    "   \n",
    "- Low Learning Rate: A small learning rate ensures more stable and reliable convergence but can significantly slow down the process. If set too low, it may also lead to premature convergence to a suboptimal solution.\n",
    "\n",
    "##### Finding the Right Balance:\n",
    "\n",
    "Choosing the right learning rate is often a trial-and-error process, sometimes guided by techniques like learning rate schedules or adaptive learning rate algorithms like Adam. These approaches attempt to balance the trade-offs by adjusting the learning rate throughout training, often starting with larger values to escape local minima and avoid plateaus, then reducing it to stabilize convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cA0nCh31QvP"
   },
   "source": [
    "[![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](https://cvw.cac.cornell.edu/SciML/mlp/demo-gradient-descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n76lBBQK1QvP"
   },
   "outputs": [],
   "source": [
    "# Implement PyTorch Training Loop Function\n",
    "def train_network(model, x_train, u_train, epochs=5000, lr=0.01):\n",
    "    \"\"\"Train a neural network model using MSE loss and Adam optimizer\"\"\"\n",
    "    criterion = nn.MSELoss() # Mean Squared Error Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # Adam optimizer\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: compute predictions\n",
    "        predictions = model(x_train)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, u_train)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "        loss.backward()       # Compute gradients of the loss w.r.t. parameters\n",
    "\n",
    "        # Optimizer step: update parameters\n",
    "        optimizer.step()      # Perform a single optimization step\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Optional: Print loss periodically\n",
    "        # if (epoch + 1) % 1000 == 0:\n",
    "        #     print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7RYHgcq1QvQ",
    "outputId": "11a5faac-73d2-4918-ff3d-1dd22837c14f"
   },
   "outputs": [],
   "source": [
    "# Initialize the single layer NN\n",
    "hidden_size = 10\n",
    "\n",
    "model = SingleLayerNN(hidden_size=hidden_size)\n",
    "\n",
    "# Train the model\n",
    "losses = train_network(model, x_train_tensor, u_train_tensor, epochs=8000, lr=0.01)\n",
    "\n",
    "print(f\"Final loss for {hidden_size} neurons: {losses[-1]:.6f}\")\n",
    "\n",
    "# Get predictions from the trained model\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    u_pred = model(x_test_tensor).numpy().flatten()\n",
    "\n",
    "# Plotting\n",
    "# Plot true function\n",
    "plt.plot(x_plot, u_analytical_plot, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "\n",
    "# Plot NN prediction\n",
    "plt.plot(x_plot, u_pred, 'b-', linewidth=2.5,\n",
    "        label=f'NN ({hidden_size} neurons)')\n",
    "\n",
    "# Plot training data\n",
    "plt.scatter(x_train_np, u_train_noisy_np, color='red', s=40, alpha=0.7,\n",
    "            label='Training Data', zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.title(f'Neural Network Prediction', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.2, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbjUbCfo1QvQ"
   },
   "source": [
    "## Universal Approximation Theorem: The Theoretical Foundation\n",
    "\n",
    "**Universal Approximation Theorem** (Cybenko, 1989):\n",
    "\n",
    "*A single hidden layer network with sufficient neurons can approximate any continuous function to arbitrary accuracy.*\n",
    "\n",
    "$F(x) = \\sum_{i=1}^{N} w_i \\sigma(v_i x + b_i) + w_0$\n",
    "\n",
    "**Mathematical statement**: For any continuous $f: [0,1] \\to \\mathbb{R}$ and $\\epsilon > 0$, there exists $N$ and parameters such that $|F(x) - f(x)| < \\epsilon$ for all $x \\in [0,1]$.\n",
    "\n",
    "**Key questions**:\n",
    "1. How many neurons $N$ do we need?\n",
    "2. Is this practical?\n",
    "3. Can we verify this experimentally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPwhEZCY1QvQ"
   },
   "source": [
    "## The Key Experiment: Width vs Approximation Quality\n",
    "\n",
    "**Hypothesis**: More neurons → better approximation (Universal Approximation Theorem)\n",
    "\n",
    "**Test**: Train networks with 5, 10, 20, 50 neurons and measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU1eXVjW1QvQ",
    "outputId": "74d34bdf-46cd-4f23-9dda-2cf7221cf019"
   },
   "outputs": [],
   "source": [
    "# Train Single-Layer NNs with Varying Width (The Experiment)\n",
    "\n",
    "# Define hidden layer sizes to experiment with\n",
    "hidden_sizes = [5, 10, 20, 50]\n",
    "\n",
    "# Choose an activation function for the hidden layer\n",
    "activation_fn = 'tanh' # Tanh often works well for smooth functions\n",
    "\n",
    "# Dictionaries to store trained models and their loss histories\n",
    "single_layer_models = {}\n",
    "single_layer_losses = {}\n",
    "\n",
    "epochs = 8000 # Number of training epochs\n",
    "lr = 0.01     # Learning rate\n",
    "\n",
    "print(f\"Training single-layer networks with {activation_fn} activation...\")\n",
    "\n",
    "# Loop through different hidden sizes and train a model for each\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\nTraining network with {hidden_size} neurons...\")\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = SingleLayerNN(hidden_size=hidden_size)\n",
    "\n",
    "    # Train the model\n",
    "    losses = train_network(model, x_train_tensor, u_train_tensor, epochs=epochs, lr=lr)\n",
    "\n",
    "    # Store the trained model and loss history\n",
    "    single_layer_models[hidden_size] = model\n",
    "    single_layer_losses[hidden_size] = losses\n",
    "\n",
    "    print(f\"Final loss for {hidden_size} neurons: {losses[-1]:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1NGNANE1QvX",
    "outputId": "6d4d8639-bb5f-4053-f5b3-efd6abe4bfe9"
   },
   "outputs": [],
   "source": [
    "# Visualize Results: Approximation Quality vs. Width\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x_test_tensor = torch.tensor(x_plot.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "for i, hidden_size in enumerate(hidden_sizes):\n",
    "    ax = axes[i]\n",
    "    model = single_layer_models[hidden_size]\n",
    "\n",
    "    # Get predictions from the trained model\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        u_pred = model(x_test_tensor).numpy().flatten()\n",
    "\n",
    "    # Plot true function\n",
    "    ax.plot(x_plot, u_analytical_plot, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "\n",
    "    # Plot NN prediction\n",
    "    ax.plot(x_plot, u_pred, 'b-', linewidth=2.5,\n",
    "            label=f'NN ({hidden_size} neurons)')\n",
    "\n",
    "    # Plot training data\n",
    "    ax.scatter(x_train_np, u_train_noisy_np, color='red', s=40, alpha=0.7,\n",
    "              label='Training Data', zorder=5)\n",
    "\n",
    "    # Calculate and display error metrics\n",
    "    mse = np.mean((u_pred - u_analytical_plot)**2)\n",
    "    max_error = np.max(np.abs(u_pred - u_analytical_plot))\n",
    "\n",
    "    ax.text(0.05, 0.95, f'MSE: {mse:.6f}\\nMax Error: {max_error:.4f}',\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "            verticalalignment='top', fontsize=10)\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('u(x)')\n",
    "    ax.set_title(f'Single Layer: {hidden_size} Neurons', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CWOZwaw1QvY",
    "outputId": "0992b48c-3880-4384-b829-35e845793306"
   },
   "outputs": [],
   "source": [
    "# Visualize Results: Training Convergence vs. Width\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for hidden_size in hidden_sizes:\n",
    "    losses = single_layer_losses[hidden_size]\n",
    "    plt.semilogy(losses, label=f'{hidden_size} neurons', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss (log scale)')\n",
    "plt.title('Training Convergence', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "final_losses = [single_layer_losses[h][-1] for h in hidden_sizes]\n",
    "plt.loglog(hidden_sizes, final_losses, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Final Loss (log scale)')\n",
    "plt.title('Final Loss vs Network Width', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Loss Summary:\")\n",
    "for hidden_size in hidden_sizes:\n",
    "    loss = single_layer_losses[hidden_size][-1]\n",
    "    print(f\"{hidden_size:2d} neurons: {loss:.8f}\")\n",
    "\n",
    "print(f\"\\nImprovement from 5 to 50 neurons: {final_losses[0]/final_losses[-1]:.1f}x better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XQqA9381QvY"
   },
   "source": [
    "## Analysis: Universal Approximation, Width, and Practicalities\n",
    "\n",
    "The visualizations from our experiment (Cells 18 & 19) show a clear trend: as we increased the number of neurons in the hidden layer (the network's **width**), the network's ability to approximate the $\\sin(\\pi x)$ function significantly improved, and the final training loss decreased.\n",
    "\n",
    "This experimental result **experimentally validates** the statement of the **Universal Approximation Theorem** (from Cell 5) – a single hidden layer with non-linearity *does* have the capacity to approximate continuous functions, and increasing the number of neurons provides more of this capacity, allowing it to better fit the target function.\n",
    "\n",
    "However, the theorem guarantees existence, not practicality. Our experiment also hints at practical considerations:\n",
    "\n",
    "*   **Number of Neurons Needed**: While 50 neurons did a good job for $\\sin(\\pi x)$, approximating more complex functions might require a very large number of neurons in a single layer. This can be computationally expensive and require a lot of data.\n",
    "*   **Training Difficulty**: The theorem doesn't guarantee that gradient descent will successfully *find* the optimal parameters. Training can be challenging, especially for very wide networks or complex functions.\n",
    "*   **Remaining Errors**: Even with 50 neurons, there's still some error. For more complex functions or higher accuracy requirements, a single layer might struggle or need excessive width.\n",
    "\n",
    "### Practical Considerations: Overfitting and Hyperparameters\n",
    "\n",
    "As network capacity increases (e.g., by adding more neurons), there's a risk of **overfitting**. This occurs when the model learns the training data (including noise) too well, capturing spurious patterns that don't generalize to unseen data, leading to poor performance outside the training set.\n",
    "\n",
    "![overfitting](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/overfitting.png?raw=1)\n",
    "\n",
    "> Example of under and overfitting the data\n",
    "\n",
    "Overfitting can be detected by monitoring performance on a separate **validation set** during training. If the validation loss starts increasing while the training loss continues to decrease, it's a sign of overfitting.\n",
    "\n",
    "Strategies to mitigate overfitting include using more training data, regularization techniques, early stopping (stopping training when validation performance degrades), or reducing model complexity.\n",
    "\n",
    "**Problem**: High-capacity networks can memorize training data instead of learning the true function\n",
    "\n",
    "**Detection**: Monitor validation loss - if it increases while training loss decreases, you're overfitting\n",
    "\n",
    "**Solutions**:\n",
    "- More training data\n",
    "- Regularization (L1/L2, dropout)\n",
    "- Early stopping\n",
    "- Simpler architectures\n",
    "\n",
    "### Hyperparameter Choices\n",
    "Hyperparameters are settings chosen *before* training that significantly influence the learning process and the final model. Key hyperparameters we've encountered include:\n",
    "\n",
    "*   **Learning Rate** ($\\eta$): Controls the step size in gradient descent. Too high can cause divergence; too low can lead to slow convergence or getting stuck in local minima.\n",
    "*   **Number of Epochs**: How many times the training data is passed through the network. Too few may result in underfitting; too many can cause overfitting.\n",
    "*   **Hidden Layer Size** ($N_h$): The number of neurons in the hidden layer. Impacts model capacity. Too small can underfit; too large can overfit.\n",
    "*   **Choice of Activation Function**: Impacts the network's ability to model specific shapes and the training dynamics (e.g., Tanh/Sigmoid for smooth functions but potential vanishing gradients, ReLU for efficiency but \"dead neuron\" issue). The best choice can be problem-dependent.\n",
    "\n",
    "Finding the right balance of hyperparameters is crucial for successful training and generalization.\n",
    "\n",
    "#### How to detect overfitting with validation dataset\n",
    "\n",
    "In practice, the learning algorithm does not actually ﬁnd the best function, but merely one thatsigniﬁcantly reduces the training error. These additional limitations, such as theimperfection of the optimization algorithm, mean that the learning algorithm’seﬀective capacitymay be less than the representational capacity of the modelfamily.\n",
    "\n",
    "Our modern ideas about improving the generalization of machine learningmodels are reﬁnements of thought dating back to philosophers at least as early as Ptolemy. Many early scholars invoke a principle of parsimony that is now mostwidely known as `Occam’s razor` (c. 1287–1347). This principle states that amongcompeting hypotheses that explain known observations equally well, we shouldchoose the “simplest” one. This idea was formalized and made more precise in the twentieth century by the founders of statistical learning theory.\n",
    "\n",
    "We must remember that while simpler functions are more likely to generalize(to have a small gap between training and test error), we must still choose asuﬃciently complex hypothesis to achieve low training error. Typically, trainingerror decreases until it asymptotes to the minimum possible error value as modelcapacity increases (assuming the error measure has a minimum value). Typically generalization error has a U-shaped curve as a function of model capacity.\n",
    "\n",
    "At the left end of the graph, training error and generalization errorare both high. This is the **underfitting region**. As we increase capacity, training error decreases, but the gap between training and generalization error increases. Eventually,the size of this gap outweighs the decrease in training error, and we enter the **overfitting region**, where capacity is too large, above the **optimal capacity**.\n",
    "\n",
    "![Training validation fit](https://github.com/chishiki-ai/sciml/blob/main/docs/00-mlp/figs/training-validation-fit.png?raw=1)\n",
    "\n",
    "> Image credits: Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaCNrobR1QvY",
    "outputId": "f03ceaa0-5f55-4936-bf52-d6bb71be850a"
   },
   "outputs": [],
   "source": [
    "# Demonstrate overfitting with very wide network\n",
    "print(\"Demonstrating potential overfitting with a very wide network...\")\n",
    "\n",
    "# Train a very wide network\n",
    "width = 500\n",
    "wide_model = SingleLayerNN(width)\n",
    "wide_losses = train_network(wide_model, x_train_tensor, u_train_tensor, epochs=5000, lr=0.01)\n",
    "\n",
    "normal_model = SingleLayerNN(50)\n",
    "normal_losses = train_network(normal_model, x_train_tensor, u_train_tensor, epochs=5000, lr=0.01)\n",
    "\n",
    "# Compare to our best previous model\n",
    "x_test = np.linspace(0, 1, 200)\n",
    "x_test_tensor = torch.tensor(x_test.reshape(-1, 1), dtype=torch.float32)\n",
    "u_true_test = analytical_solution(x_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    wide_pred = wide_model(x_test_tensor).numpy().flatten()\n",
    "    normal_pred = normal_model(x_test_tensor).numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_test, u_true_test, 'k-', linewidth=3, label='True Function')\n",
    "plt.plot(x_test, normal_pred, 'b-', linewidth=3, label='50 Neurons')\n",
    "plt.scatter(x_train_tensor, u_train_tensor, color='red', s=40, alpha=0.7, zorder=5)\n",
    "plt.title('Normal Network (50 neurons)', fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_test, u_true_test, 'k-', linewidth=3, label='True Function')\n",
    "plt.plot(x_test, wide_pred, 'r-', linewidth=3, label=f'{width:d} Neurons')\n",
    "plt.scatter(x_train_tensor, u_train_tensor, color='red', s=40, alpha=0.7, zorder=5)\n",
    "plt.title(f'Very Wide Network ({width:d} neurons)', fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.semilogy(normal_losses, 'b-', label='50 Neurons', linewidth=2)\n",
    "plt.semilogy(wide_losses, 'r-', label=f'{width:d} Neurons', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss (log scale)')\n",
    "plt.title('Training Loss Comparison', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate generalization error (on test points not in training)\n",
    "x_test_fine = np.linspace(0.05, 0.95, 100)  # Points between training points\n",
    "x_test_fine_tensor = torch.tensor(x_test_fine.reshape(-1, 1), dtype=torch.float32)\n",
    "u_true_fine = analytical_solution(x_test_fine)\n",
    "\n",
    "with torch.no_grad():\n",
    "    wide_pred_fine = wide_model(x_test_fine_tensor).numpy().flatten()\n",
    "    normal_pred_fine = normal_model(x_test_fine_tensor).numpy().flatten()\n",
    "\n",
    "normal_mse = np.mean((normal_pred_fine - u_true_fine)**2)\n",
    "wide_mse = np.mean((wide_pred_fine - u_true_fine)**2)\n",
    "\n",
    "print(f\"\\nGeneralization Error (on unseen test points):\")\n",
    "print(f\"50 neurons:  {normal_mse:.6f}\")\n",
    "print(f\"{width:d} neurons: {wide_mse:.6f}\")\n",
    "print(f\"\\nNote: Very wide networks can sometimes generalize worse despite lower training loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X06lDWwi1QvZ"
   },
   "source": [
    "**The story so far**: Single-layer networks can approximate any function (Universal Approximation) but may need impractically many neurons. **The question**: Can depth be more efficient than width?\n",
    "\n",
    "## The Need for Depth - The XOR Problem: A Historical Turning Point\n",
    "\n",
    "The XOR problem exposed fundamental limitations of **true** single-layer perceptrons, causing the \"AI winter\" of the 1970s. This simple problem reveals why depth is essential.\n",
    "\n",
    "**XOR Truth Table**:\n",
    "```\n",
    "x₁  x₂  │  y\n",
    "────────┼────\n",
    " 0   0  │  0\n",
    " 0   1  │  1  \n",
    " 1   0  │  1\n",
    " 1   1  │  0\n",
    "```\n",
    "\n",
    "**The crisis**: No single line can separate these classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW6nX5Es1QvZ",
    "outputId": "d6269704-ae84-42a1-a7e8-c411cf516e4f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, 'figure.figsize': (14, 8)})\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_xor = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X_xor_tensor = torch.tensor(X_xor)\n",
    "y_xor_tensor = torch.tensor(y_xor)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "for i in range(4):\n",
    "    print(f\"({X_xor[i,0]}, {X_xor[i,1]}) → {y_xor[i,0]}\")\n",
    "\n",
    "# Visualize the impossibility\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: XOR problem with failed linear attempts\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y_xor.flatten() == i\n",
    "    ax1.scatter(X_xor[mask, 0], X_xor[mask, 1],\n",
    "               c=colors[i], s=200, alpha=0.8,\n",
    "               label=f'Class {i}', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Failed linear separation attempts\n",
    "x_line = np.linspace(-0.5, 1.5, 100)\n",
    "ax1.plot(x_line, 0.5 * np.ones_like(x_line), 'g--', linewidth=2, alpha=0.7, label='Failed Line 1')\n",
    "ax1.plot(0.5 * np.ones_like(x_line), x_line, 'm--', linewidth=2, alpha=0.7, label='Failed Line 2')\n",
    "ax1.plot(x_line, x_line, 'orange', linestyle='--', linewidth=2, alpha=0.7, label='Failed Line 3')\n",
    "\n",
    "ax1.set_xlim(-0.3, 1.3)\n",
    "ax1.set_ylim(-0.3, 1.3)\n",
    "ax1.set_xlabel('x₁')\n",
    "ax1.set_ylabel('x₂')\n",
    "ax1.set_title('XOR: No Linear Separation Possible', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Required non-linear boundary\n",
    "for i in range(2):\n",
    "    mask = y_xor.flatten() == i\n",
    "    ax2.scatter(X_xor[mask, 0], X_xor[mask, 1],\n",
    "               c=colors[i], s=200, alpha=0.8,\n",
    "               label=f'Class {i}', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Conceptual non-linear boundary\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "x_circle1 = 0.25 + 0.15*np.cos(theta)\n",
    "y_circle1 = 0.25 + 0.15*np.sin(theta)\n",
    "x_circle2 = 0.75 + 0.15*np.cos(theta)\n",
    "y_circle2 = 0.75 + 0.15*np.sin(theta)\n",
    "\n",
    "ax2.plot(x_circle1, y_circle1, 'g-', linewidth=3, alpha=0.8, label='Required Boundary')\n",
    "ax2.plot(x_circle2, y_circle2, 'g-', linewidth=3, alpha=0.8)\n",
    "\n",
    "ax2.set_xlim(-0.3, 1.3)\n",
    "ax2.set_ylim(-0.3, 1.3)\n",
    "ax2.set_xlabel('x₁')\n",
    "ax2.set_ylabel('x₂')\n",
    "ax2.set_title('Required Non-Linear Boundary', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe fundamental problem: XOR is NOT linearly separable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Cp7b9081QvZ"
   },
   "source": [
    "### The Critical Distinction: True Single-Layer vs Multi-Layer\n",
    "\n",
    "**Historical confusion**: What Minsky & Papert analyzed was a **TRUE** single-layer perceptron (Input → Output directly). This is different from our \"single-layer\" networks that have hidden layers!\n",
    "\n",
    "**Architecture comparison**:\n",
    "- **True Single-Layer**: Input → Output (NO hidden layers)\n",
    "- **Multi-Layer**: Input → Hidden → Output (1+ hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qK-4-4vC1Qva",
    "outputId": "5f5c1169-a860-41a4-cc19-26b7aafe8230"
   },
   "outputs": [],
   "source": [
    "# Define the architectures correctly\n",
    "class TrueSingleLayerPerceptron(nn.Module):\n",
    "    \"\"\"TRUE single-layer perceptron: Input → Output (NO hidden layers)\n",
    "    This is what Minsky & Papert showed cannot solve XOR!\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 1)  # Direct: 2 inputs → 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.layer(x))\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron: Input → Hidden → Output\n",
    "    This CAN solve XOR!\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=4):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.sigmoid(self.hidden(x))\n",
    "        return torch.sigmoid(self.output(h))\n",
    "\n",
    "def train_xor_model(model, X, y, epochs=3000, lr=10.0):\n",
    "    \"\"\"Train model on XOR problem\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            accuracy = ((pred > 0.5).float() == y).float().mean()\n",
    "            print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}')\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "print(\"Network architectures defined:\")\n",
    "print(\"1. TrueSingleLayerPerceptron: Input → Output (what fails)\")\n",
    "print(\"2. MultiLayerPerceptron: Input → Hidden → Output (what works)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jygJlyKX1Qva"
   },
   "source": [
    "### The Historical Failure: True Single-Layer on XOR\n",
    "\n",
    "**Prediction**: The true single-layer perceptron will fail spectacularly at XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj5dzQgY1Qva",
    "outputId": "08b984a8-aba4-47be-9056-ff6e12383aed"
   },
   "outputs": [],
   "source": [
    "# Demonstrate the historical failure\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING TRUE SINGLE-LAYER PERCEPTRON\")\n",
    "print(\"(This is what Minsky & Papert showed fails!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "true_single = TrueSingleLayerPerceptron()\n",
    "true_single_loss = train_xor_model(true_single, X_xor_tensor, y_xor_tensor)\n",
    "\n",
    "# Analyze the failure\n",
    "with torch.no_grad():\n",
    "    pred = true_single(X_xor_tensor)\n",
    "    accuracy = ((pred > 0.5).float() == y_xor_tensor).float().mean()\n",
    "\n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} (should be ~0.5 = random guessing)\")\n",
    "    print(f\"Final loss: {true_single_loss:.4f}\")\n",
    "    print(\"\\nPredictions vs Targets:\")\n",
    "    for i in range(4):\n",
    "        print(f\"  ({X_xor[i,0]}, {X_xor[i,1]}) → {pred[i,0]:.4f} (target: {y_xor[i,0]})\")\n",
    "\n",
    "    print(\"\\n❌ FAILURE CONFIRMED: True single-layer cannot solve XOR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9-heJGw1Qva"
   },
   "source": [
    "### The Solution: Adding Hidden Layers\n",
    "\n",
    "**Hypothesis**: Adding just ONE hidden layer should solve XOR completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuiinoLL1Qva",
    "outputId": "db1588b9-32e3-4222-a968-d8334ab4a719"
   },
   "outputs": [],
   "source": [
    "# Demonstrate the solution\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MULTI-LAYER PERCEPTRON\")\n",
    "print(\"(Adding ONE hidden layer should solve XOR!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "multi_layer = MultiLayerPerceptron(4)\n",
    "multi_layer_loss = train_xor_model(multi_layer, X_xor_tensor, y_xor_tensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = multi_layer(X_xor_tensor)\n",
    "    accuracy = ((pred > 0.5).float() == y_xor_tensor).float().mean()\n",
    "\n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} (should be 1.0000!)\")\n",
    "    print(f\"Final loss: {multi_layer_loss:.4f}\")\n",
    "    print(\"\\nPredictions vs Targets:\")\n",
    "    for i in range(4):\n",
    "        print(f\"  ({X_xor[i,0]}, {X_xor[i,1]}) → {pred[i,0]:.4f} (target: {y_xor[i,0]})\")\n",
    "\n",
    "    print(\"\\n✅ SUCCESS: Multi-layer network solves XOR perfectly!\")\n",
    "    print(\"\\nImprovement factor: Infinite (from failure to perfect solution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlwmX5z21Qva"
   },
   "source": [
    "#### Visualizing the Decision Boundaries\n",
    "\n",
    "**The geometric insight**: Linear vs non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMjcAxOn1Qva",
    "outputId": "785f89d0-b7e6-4c45-a47c-0e814497ab2d"
   },
   "outputs": [],
   "source": [
    "# Create decision boundary visualization\n",
    "def plot_decision_boundary(model, title, ax):\n",
    "    h = 0.01\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "\n",
    "    grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_points).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Create contour plot\n",
    "    contour = ax.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=3)\n",
    "\n",
    "    # Plot XOR points\n",
    "    colors = ['red', 'blue']\n",
    "    markers = ['o', 's']\n",
    "    for i in range(2):\n",
    "        mask = y_xor.flatten() == i\n",
    "        ax.scatter(X_xor[mask, 0], X_xor[mask, 1],\n",
    "                  c=colors[i], s=300, alpha=1.0,\n",
    "                  edgecolors='black', linewidth=3,\n",
    "                  marker=markers[i], label=f'Class {i}')\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('x₁')\n",
    "    ax.set_ylabel('x₂')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "plot_decision_boundary(true_single, 'True Single-Layer\\n(Linear - FAILS)', ax1)\n",
    "plot_decision_boundary(multi_layer, 'Multi-Layer\\n(Non-Linear - SUCCEEDS)', ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"• True single-layer: Can only create straight lines → FAILS\")\n",
    "print(\"• Multi-layer: Creates curved boundaries → SUCCEEDS\")\n",
    "print(\"• Hidden layers enable non-linear transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNWbLhhD1Qva"
   },
   "source": [
    "### Mathematical Explanation: Why Depth Solves XOR\n",
    "\n",
    "**True single-layer limitation**:\n",
    "$$y = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "Decision boundary: $w_1 x_1 + w_2 x_2 + b = 0$ (always a straight line)\n",
    "\n",
    "**Multi-layer solution**: Decompose XOR into simpler operations\n",
    "$$h_1 = \\sigma(w_{11} x_1 + w_{12} x_2 + b_1) \\quad \\text{(≈ OR gate)}$$\n",
    "$$h_2 = \\sigma(w_{21} x_1 + w_{22} x_2 + b_2) \\quad \\text{(≈ AND gate)}$$\n",
    "$$y = \\sigma(v_1 h_1 + v_2 h_2 + b_3) \\quad \\text{(≈ OR AND NOT)}$$\n",
    "\n",
    "**Result**: XOR = (OR) AND (NOT AND) = compositional solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIYMAa2f1Qva"
   },
   "source": [
    "### Beyond XOR: High-Frequency Functions\n",
    "\n",
    "**The deeper question**: Does the depth advantage extend beyond simple classification?\n",
    "\n",
    "**Test case**: High-frequency function $f(x) = \\sin(\\pi x) + 0.3\\sin(10\\pi x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEHr8Q4k1Qva",
    "outputId": "95004899-015d-4eb6-ccb3-6db73ae616ba"
   },
   "outputs": [],
   "source": [
    "# High-frequency function challenge\n",
    "def high_freq_function(x):\n",
    "    return np.sin(np.pi * x) + 0.3 * np.sin(10 * np.pi * x)\n",
    "\n",
    "# Generate data\n",
    "x_hf = np.linspace(0, 1, 200)\n",
    "y_hf_true = high_freq_function(x_hf)\n",
    "\n",
    "# Sparse training data\n",
    "x_hf_train = np.linspace(0, 1, 25)\n",
    "y_hf_train = high_freq_function(x_hf_train) + 0.01 * np.random.randn(25)\n",
    "\n",
    "# Convert to tensors\n",
    "x_hf_train_t = torch.tensor(x_hf_train.reshape(-1, 1), dtype=torch.float32)\n",
    "y_hf_train_t = torch.tensor(y_hf_train.reshape(-1, 1), dtype=torch.float32)\n",
    "x_hf_test_t = torch.tensor(x_hf.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Define architectures\n",
    "class ShallowNetwork(nn.Module):\n",
    "    \"\"\"Single hidden layer with many neurons\"\"\"\n",
    "    def __init__(self, width=100):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, width),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(width, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"Multiple hidden layers with fewer neurons each\"\"\"\n",
    "    def __init__(self, width=25, depth=4):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(1, width), nn.Tanh()]\n",
    "        for _ in range(depth-1):\n",
    "            layers.extend([nn.Linear(width, width), nn.Tanh()])\n",
    "        layers.append(nn.Linear(width, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_regressor(model, x_train, y_train, epochs=5000, lr=0.01):\n",
    "    \"\"\"Train regression model\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pred = model(x_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f'Epoch {epoch+1}: Loss = {loss.item():.6f}')\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Train models\n",
    "print(\"Training shallow network (1 layer, 100 neurons)...\")\n",
    "shallow_net = ShallowNetwork(100)\n",
    "shallow_loss = train_regressor(shallow_net, x_hf_train_t, y_hf_train_t)\n",
    "\n",
    "print(\"\\nTraining deep network (4 layers, 25 neurons each)...\")\n",
    "deep_net = DeepNetwork(25, 4)\n",
    "deep_loss = train_regressor(deep_net, x_hf_train_t, y_hf_train_t)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Shallow final loss: {shallow_loss:.6f}\")\n",
    "print(f\"Deep final loss: {deep_loss:.6f}\")\n",
    "print(f\"Improvement: {shallow_loss/deep_loss:.1f}x better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2yVkofL1Qva",
    "outputId": "7a398648-e7b5-4697-d5bb-754d21a46a75"
   },
   "outputs": [],
   "source": [
    "# Visualize high-frequency results\n",
    "with torch.no_grad():\n",
    "    shallow_pred = shallow_net(x_hf_test_t).numpy().flatten()\n",
    "    deep_pred = deep_net(x_hf_test_t).numpy().flatten()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Shallow network\n",
    "ax1.plot(x_hf, y_hf_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "ax1.plot(x_hf, shallow_pred, 'r-', linewidth=2, label='Shallow Network (100 neurons)')\n",
    "ax1.scatter(x_hf_train, y_hf_train, color='blue', s=30, alpha=0.7, zorder=5)\n",
    "\n",
    "shallow_mse = np.mean((shallow_pred - y_hf_true)**2)\n",
    "ax1.text(0.05, 0.95, f'MSE: {shallow_mse:.4f}', transform=ax1.transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "         fontsize=12, fontweight='bold')\n",
    "\n",
    "ax1.set_title('Shallow Network (1 Hidden Layer)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "# Deep network\n",
    "ax2.plot(x_hf, y_hf_true, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "ax2.plot(x_hf, deep_pred, 'g-', linewidth=2, label='Deep Network (4 layers)')\n",
    "ax2.scatter(x_hf_train, y_hf_train, color='blue', s=30, alpha=0.7, zorder=5)\n",
    "\n",
    "deep_mse = np.mean((deep_pred - y_hf_true)**2)\n",
    "ax2.text(0.05, 0.95, f'MSE: {deep_mse:.4f}', transform=ax2.transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "         fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2.set_title('Deep Network (4 Hidden Layers)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "plt.suptitle('High-Frequency Function: Shallow vs Deep', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Parameter comparison\n",
    "shallow_params = sum(p.numel() for p in shallow_net.parameters())\n",
    "deep_params = sum(p.numel() for p in deep_net.parameters())\n",
    "\n",
    "print(f\"\\nParameter Efficiency:\")\n",
    "print(f\"Shallow network: {shallow_params} parameters, MSE: {shallow_mse:.6f}\")\n",
    "print(f\"Deep network: {deep_params} parameters, MSE: {deep_mse:.6f}\")\n",
    "print(f\"\\nDeep network: {shallow_mse/deep_mse:.1f}x better performance\")\n",
    "print(f\"              {deep_params/shallow_params:.1f}x more parameters\")\n",
    "print(f\"\\nConclusion: Deep networks are more parameter-efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFNy_aUj1Qva"
   },
   "source": [
    "### Historical Timeline: From Crisis to Revolution\n",
    "\n",
    "**The XOR crisis and its resolution transformed AI:**\n",
    "\n",
    "| Year | Event | Impact |\n",
    "|------|-------|--------|\n",
    "| 1943 | McCulloch-Pitts neuron | Foundation laid |\n",
    "| 1957 | Rosenblatt's Perceptron | First learning success |\n",
    "| **1969** | **Minsky & Papert: XOR problem** | **Showed true single-layer limits** |\n",
    "| 1970s-80s | \"AI Winter\" | Funding dried up |\n",
    "| 1986 | Backpropagation algorithm | Enabled multi-layer training |\n",
    "| 1989 | Universal Approximation Theorem | Theoretical foundation |\n",
    "| 2006+ | Deep Learning Revolution | Depth proves essential |\n",
    "\n",
    "**The lesson**: XOR taught us that **depth is not luxury—it's necessity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqjMyc8n1Qvb"
   },
   "source": [
    "### Why Depth Matters: The Four Key Insights\n",
    "\n",
    "1. **Representation Efficiency**\n",
    "- **Shallow networks**: May need exponentially many neurons\n",
    "- **Deep networks**: Hierarchical composition is exponentially more efficient\n",
    "- **Example**: XOR impossible with 1 layer, trivial with 2 layers\n",
    "\n",
    "2. **Feature Hierarchy**\n",
    "- **Layer 1**: Simple features (edges, basic patterns)\n",
    "- **Layer 2**: Feature combinations (corners, textures)\n",
    "- **Layer 3+**: Complex abstractions (objects, concepts)\n",
    "- **Key insight**: Real-world problems have hierarchical structure\n",
    "\n",
    "3. **Geometric Transformation**\n",
    "- Each layer performs **coordinate transformation**\n",
    "- Deep networks \"unfold\" complex data manifolds\n",
    "- **XOR example**: Transform non-separable → separable\n",
    "- **General principle**: Depth enables progressive simplification\n",
    "\n",
    "4. **Compositional Learning**\n",
    "- Complex functions = composition of simple functions\n",
    "- **Mathematical**: $f(x) = f_L(f_{L-1}(...f_1(x)))$\n",
    "- **Practical**: Build complexity incrementally\n",
    "- **Universal**: Applies across domains (vision, language, science)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
