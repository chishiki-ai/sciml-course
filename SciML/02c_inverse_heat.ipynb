{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTgVZ8Eh1TE5"
   },
   "source": [
    "# Inverse Problems with Physics-Informed Neural Networks\n",
    "## Parameter Estimation in Heat Conduction\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the difference between forward and inverse problems\n",
    "- Learn how PINNs solve inverse problems elegantly\n",
    "- Master parameter estimation with sparse, noisy data\n",
    "- Implement thermal diffusivity estimation from temperature measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np92l7D31TE6"
   },
   "source": [
    "## Introduction: Forward vs Inverse Problems\n",
    "\n",
    "### The Forward Problem\n",
    "In our previous PINN examples, we solved **forward problems**:\n",
    "- **Given:** Complete physics (equations + parameters)\n",
    "- **Find:** Solution function $u(x,t)$\n",
    "- **Example:** Given spring constant $k$ and damping $c$, find oscillator motion $u(t)$\n",
    "\n",
    "### The Inverse Problem  \n",
    "In many real-world scenarios, we face **inverse problems**:\n",
    "- **Given:** Some measurements of the solution $u$\n",
    "- **Find:** Unknown physical parameters in the governing equations\n",
    "- **Example:** From temperature measurements, determine thermal conductivity\n",
    "\n",
    "### Why Inverse Problems are Hard\n",
    "\n",
    "**Traditional Approach:**\n",
    "1. Guess parameter values\n",
    "2. Solve forward problem (expensive numerical simulation)\n",
    "3. Compare with measurements\n",
    "4. Update guess and repeat\n",
    "\n",
    "**Problems:**\n",
    "- Computationally expensive (many forward solves)\n",
    "- Sensitive to noise\n",
    "- May not converge or find wrong parameters\n",
    "- Requires good initial guesses\n",
    "\n",
    "**PINN Revolution:** Solve forward and inverse problems simultaneously!\n",
    "\n",
    "![Inverse Problem Concept](https://github.com/chishiki-ai/sciml/blob/main/docs/01-pinn/figs/inverse-heat-diffusivity.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXoM3GrU1TE7"
   },
   "source": [
    "## The Heat Equation: Our Test Case\n",
    "\n",
    "We'll demonstrate inverse problems using **1D steady-state heat conduction**:\n",
    "\n",
    "$$-k \\frac{d^2T}{dx^2} = f(x), \\quad x \\in [0,1]$$\n",
    "\n",
    "with boundary conditions: $T(0) = T_0$, $T(1) = T_1$\n",
    "\n",
    "**Physical meaning:**\n",
    "- $T(x)$ = temperature distribution\n",
    "- $k$ = **thermal diffusivity** (unknown parameter we want to find)\n",
    "- $f(x)$ = heat source term (known)\n",
    "- Boundary conditions specify temperatures at the ends\n",
    "\n",
    "### Why This Problem?\n",
    "1. **Physical relevance:** Material property identification is crucial in engineering\n",
    "2. **Mathematical simplicity:** 1D steady-state allows clear visualization\n",
    "3. **Exact solution available:** We can verify our parameter recovery\n",
    "4. **Well-studied:** Benchmark for inverse methods\n",
    "\n",
    "### The Challenge\n",
    "**Scenario:** You're an engineer testing a new material. You can:\n",
    "- Apply known heat sources $f(x)$\n",
    "- Control boundary temperatures\n",
    "- Measure temperature at a few points\n",
    "- **Cannot directly measure thermal diffusivity $k$**\n",
    "\n",
    "**Goal:** Determine $k$ from sparse, noisy temperature measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMw_F4Gz1TE7",
    "outputId": "84196dac-86d8-419e-c91f-73629a53f62a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Problem setup\n",
    "L = 1.0          # Domain length\n",
    "T0, T1 = 0.0, 0.0  # Boundary temperatures (homogeneous)\n",
    "kappa_true = 0.5   # True thermal diffusivity (unknown to the PINN)\n",
    "\n",
    "def heat_source(x):\n",
    "    \"\"\"Known heat source term f(x)\"\"\"\n",
    "    return -15*x + 2\n",
    "\n",
    "def exact_temperature(x, kappa):\n",
    "    \"\"\"Analytical solution for validation\"\"\"\n",
    "    # For -kappa * dÂ²T/dxÂ² = f(x) with f(x) = -15x + 2\n",
    "    # and T(0) = T(1) = 0\n",
    "    # Solution: T(x) = (15*xÂ³ - 2*xÂ² - 13*x) / (6*kappa)\n",
    "    return (15*x**3 - 2*x**2 - 13*x) / (6*kappa)\n",
    "\n",
    "print(f\"Problem Setup:\")\n",
    "print(f\"  Domain: [0, {L}]\")\n",
    "print(f\"  Boundary conditions: T(0) = {T0}, T(1) = {T1}\")\n",
    "print(f\"  Heat source: f(x) = -15x + 2\")\n",
    "print(f\"  True thermal diffusivity: Îº = {kappa_true}\")\n",
    "print(f\"  Goal: Estimate Îº from sparse temperature measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD3HYFKr1TE8"
   },
   "source": [
    "## Stage 1: Understanding the Forward Problem\n",
    "\n",
    "Before tackling the inverse problem, let's understand the forward problem and generate our \"experimental\" data.\n",
    "\n",
    "### Analytical Solution\n",
    "For our specific case with $f(x) = -15x + 2$ and homogeneous boundary conditions, the exact solution is:\n",
    "\n",
    "$$T(x) = \\frac{15x^3 - 2x^2 - 13x}{6\\kappa}$$\n",
    "\n",
    "**Key insight:** Notice how $T(x)$ depends on $\\kappa$. Different values of $\\kappa$ give different temperature profiles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6PqBb441TE8",
    "outputId": "44fe2409-c4a4-4a17-a6a4-f43bf5355667"
   },
   "outputs": [],
   "source": [
    "# Visualize how Îº affects temperature profiles\n",
    "x_plot = np.linspace(0, 1, 200)\n",
    "kappa_values = [0.2, 0.5, 1.0, 2.0]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Temperature profiles for different Îº\n",
    "plt.subplot(1, 2, 1)\n",
    "for kappa in kappa_values:\n",
    "    T_exact = exact_temperature(x_plot, kappa)\n",
    "    plt.plot(x_plot, T_exact, linewidth=2, label=f'Îº = {kappa}')\n",
    "\n",
    "plt.xlabel('Position x')\n",
    "plt.ylabel('Temperature T(x)')\n",
    "plt.title('Temperature Profiles for Different Thermal Diffusivities')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Heat source\n",
    "plt.subplot(1, 2, 2)\n",
    "f_plot = heat_source(x_plot)\n",
    "plt.plot(x_plot, f_plot, 'r-', linewidth=2, label='f(x) = -15x + 2')\n",
    "plt.xlabel('Position x')\n",
    "plt.ylabel('Heat Source f(x)')\n",
    "plt.title('Known Heat Source Term')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"â€¢ Lower Îº â†’ Higher temperature magnitudes\")\n",
    "print(\"â€¢ Higher Îº â†’ Lower temperature magnitudes\")\n",
    "print(\"â€¢ Heat source is negative for x > 2/15 â‰ˆ 0.133\")\n",
    "print(\"â€¢ This creates the characteristic temperature profile shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzbtSLGC1TE9"
   },
   "source": [
    "### Creating \"Experimental\" Data\n",
    "\n",
    "In a real experiment, we would measure temperature at a few locations. Let's simulate this with sparse, noisy measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgN_j6BF1TE-",
    "outputId": "d4d303ff-0899-4c64-9949-13650cd81b45"
   },
   "outputs": [],
   "source": [
    "# Generate sparse, noisy experimental data\n",
    "n_data = 10  # Only 10 measurement points!\n",
    "noise_level = 0.05  # 5% noise\n",
    "\n",
    "# Measurement locations (avoid boundaries where T=0)\n",
    "x_data = np.linspace(0.1, 0.9, n_data)\n",
    "T_data_exact = exact_temperature(x_data, kappa_true)\n",
    "\n",
    "# Add noise to simulate experimental uncertainty\n",
    "noise = noise_level * np.std(T_data_exact) * np.random.normal(0, 1, n_data)\n",
    "T_data_noisy = T_data_exact + noise\n",
    "\n",
    "# Plot the \"experimental\" setup\n",
    "plt.figure(figsize=(10, 6))\n",
    "T_true_plot = exact_temperature(x_plot, kappa_true)\n",
    "plt.plot(x_plot, T_true_plot, 'k-', linewidth=2, label=f'True solution (Îº = {kappa_true})')\n",
    "plt.scatter(x_data, T_data_noisy, color='red', s=100, zorder=5,\n",
    "           label=f'\"Experimental\" data ({n_data} points, {noise_level*100}% noise)')\n",
    "plt.scatter([0, 1], [T0, T1], color='blue', s=100, marker='s', zorder=5,\n",
    "           label='Boundary conditions')\n",
    "\n",
    "plt.xlabel('Position x')\n",
    "plt.ylabel('Temperature T(x)')\n",
    "plt.title('Inverse Problem Setup: Estimate Îº from Sparse Temperature Measurements')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Experimental Data Summary:\")\n",
    "print(f\"  Measurement points: {n_data}\")\n",
    "print(f\"  Noise level: {noise_level*100}%\")\n",
    "print(f\"  True Îº: {kappa_true} (unknown to PINN)\")\n",
    "print(f\"  Temperature range: [{T_data_noisy.min():.3f}, {T_data_noisy.max():.3f}]\")\n",
    "print(f\"\\nChallenge: Can PINN recover Îº = {kappa_true} from this sparse, noisy data?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vyd7hrk1TE-"
   },
   "source": [
    "## Stage 2: The PINN Approach to Inverse Problems\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**Traditional inverse methods:**\n",
    "- Iterate between parameter guessing and forward solving\n",
    "- Computationally expensive\n",
    "- Prone to local minima\n",
    "\n",
    "**PINN approach:**\n",
    "- **Treat unknown parameters as additional trainable variables**\n",
    "- Optimize network weights AND physical parameters simultaneously\n",
    "- Use physics as regularization\n",
    "\n",
    "### The PINN Inverse Framework\n",
    "\n",
    "For our heat equation problem, we have:\n",
    "\n",
    "**Unknowns to learn:**\n",
    "1. $\\hat{T}_\\theta(x)$ - neural network approximating temperature\n",
    "2. $\\hat{\\kappa}$ - estimated thermal diffusivity (scalar parameter)\n",
    "\n",
    "**Loss function components:**\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}}(\\theta, \\hat{\\kappa}) = w_1\\mathcal{L}_{\\text{data}} + w_2\\mathcal{L}_{\\text{PDE}} + w_3\\mathcal{L}_{\\text{BC}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "**Data Loss:** $\\mathcal{L}_{\\text{data}} = \\frac{1}{N_d}\\sum_{i=1}^{N_d}|\\hat{T}_\\theta(x_i) - T_i|^2$\n",
    "\n",
    "**Physics Loss:** $\\mathcal{L}_{\\text{PDE}} = \\frac{1}{N_f}\\sum_{j=1}^{N_f}\\left|-\\hat{\\kappa}\\frac{d^2\\hat{T}_\\theta}{dx^2}(x_j) - f(x_j)\\right|^2$\n",
    "\n",
    "**Boundary Loss:** $\\mathcal{L}_{\\text{BC}} = |\\hat{T}_\\theta(0) - T_0|^2 + |\\hat{T}_\\theta(1) - T_1|^2$\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Simultaneous optimization:** Both $\\theta$ and $\\hat{\\kappa}$ are updated together\n",
    "2. **Physics regularization:** The PDE constraint guides parameter estimation\n",
    "3. **Data efficiency:** Physics fills gaps between sparse measurements\n",
    "4. **Robustness:** Physics constraints filter noise in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm6kdD4F1TE-"
   },
   "source": [
    "## Stage 3: Implementation\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "We use a simple feedforward network for the temperature field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHlXXekU1TE_",
    "outputId": "085bd1ea-a44d-4ba4-a3a5-5786887c3582"
   },
   "outputs": [],
   "source": [
    "class HeatPINN(nn.Module):\n",
    "    \"\"\"PINN for inverse heat equation with parameter estimation\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=32, n_layers=3, kappa_init=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Neural network for temperature field T(x)\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(1, hidden_size))  # Input: position x\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Tanh())  # Smooth activation for PDE\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_size, 1))  # Output: temperature T\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Trainable parameter: thermal diffusivity Îº\n",
    "        # Use log parameterization to ensure Îº > 0\n",
    "        self.log_kappa = nn.Parameter(torch.tensor(np.log(kappa_init), dtype=torch.float32))\n",
    "\n",
    "        print(f\"PINN Architecture:\")\n",
    "        print(f\"  Network: {1} â†’ {hidden_size} â†’ ... â†’ {1} ({n_layers} hidden layers)\")\n",
    "        print(f\"  Parameter: Îº initialized to {kappa_init}\")\n",
    "        print(f\"  Total parameters: {sum(p.numel() for p in self.parameters())}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: compute temperature T(x)\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "    @property\n",
    "    def kappa(self):\n",
    "        \"\"\"Return positive thermal diffusivity\"\"\"\n",
    "        return torch.exp(self.log_kappa)\n",
    "\n",
    "    def physics_residual(self, x):\n",
    "        \"\"\"Compute PDE residual: -Îº * dÂ²T/dxÂ² - f(x)\"\"\"\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Forward pass\n",
    "        T = self.forward(x)\n",
    "\n",
    "        # First derivative: dT/dx\n",
    "        dT_dx = torch.autograd.grad(T, x, grad_outputs=torch.ones_like(T),\n",
    "                                   create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # Second derivative: dÂ²T/dxÂ²\n",
    "        d2T_dx2 = torch.autograd.grad(dT_dx, x, grad_outputs=torch.ones_like(dT_dx),\n",
    "                                     create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # Heat source term\n",
    "        f = heat_source(x.detach().numpy()).reshape(-1, 1)\n",
    "        f_tensor = torch.tensor(f, dtype=torch.float32)\n",
    "\n",
    "        # PDE residual: -Îº * dÂ²T/dxÂ² - f(x) = 0\n",
    "        residual = -self.kappa * d2T_dx2 - f_tensor\n",
    "\n",
    "        return residual\n",
    "\n",
    "# Create the PINN model\n",
    "kappa_init = 0.1  # Initial guess (far from true value of 0.5)\n",
    "model = HeatPINN(kappa_init=kappa_init)\n",
    "\n",
    "print(f\"\\nInitial parameter guess: Îº = {model.kappa.item():.3f}\")\n",
    "print(f\"True parameter value: Îº = {kappa_true}\")\n",
    "print(f\"Initial error: {abs(model.kappa.item() - kappa_true)/kappa_true*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2UA-epy1TFA"
   },
   "source": [
    "### Training Function\n",
    "\n",
    "The training process optimizes both the neural network weights $theta$ and the parameter $kappa$ simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE7uq7rg1TFA"
   },
   "outputs": [],
   "source": [
    "def train_inverse_pinn(model, x_data, T_data, epochs=10000, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train PINN for inverse parameter estimation\n",
    "\n",
    "    Args:\n",
    "        model: HeatPINN model\n",
    "        x_data, T_data: Experimental measurements\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data to tensors\n",
    "    x_data_tensor = torch.tensor(x_data.reshape(-1, 1), dtype=torch.float32)\n",
    "    T_data_tensor = torch.tensor(T_data.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    # Collocation points for physics\n",
    "    n_colloc = 100\n",
    "    x_colloc = torch.linspace(0, 1, n_colloc).reshape(-1, 1)\n",
    "\n",
    "    # Boundary points\n",
    "    x_boundary = torch.tensor([[0.0], [1.0]], dtype=torch.float32)\n",
    "    T_boundary = torch.tensor([[T0], [T1]], dtype=torch.float32)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Loss weights\n",
    "    w_data = 1.0\n",
    "    w_pde = 1.0\n",
    "    w_bc = 10.0  # Higher weight for boundary conditions\n",
    "\n",
    "    # Storage for tracking\n",
    "    loss_history = []\n",
    "    kappa_history = []\n",
    "    data_losses = []\n",
    "    pde_losses = []\n",
    "    bc_losses = []\n",
    "\n",
    "    print(f\"Training Inverse PINN:\")\n",
    "    print(f\"  Data points: {len(x_data)}\")\n",
    "    print(f\"  Collocation points: {n_colloc}\")\n",
    "    print(f\"  Loss weights: data={w_data}, PDE={w_pde}, BC={w_bc}\")\n",
    "    print(f\"  Starting Îº = {model.kappa.item():.4f}\")\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Data loss: fit measurements\n",
    "        T_pred_data = model(x_data_tensor)\n",
    "        loss_data = torch.mean((T_pred_data - T_data_tensor)**2)\n",
    "\n",
    "        # PDE loss: satisfy physics\n",
    "        residual = model.physics_residual(x_colloc)\n",
    "        loss_pde = torch.mean(residual**2)\n",
    "\n",
    "        # Boundary loss: satisfy boundary conditions\n",
    "        T_pred_bc = model(x_boundary)\n",
    "        loss_bc = torch.mean((T_pred_bc - T_boundary)**2)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = w_data * loss_data + w_pde * loss_pde + w_bc * loss_bc\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store history\n",
    "        loss_history.append(total_loss.item())\n",
    "        kappa_history.append(model.kappa.item())\n",
    "        data_losses.append(loss_data.item())\n",
    "        pde_losses.append(loss_pde.item())\n",
    "        bc_losses.append(loss_bc.item())\n",
    "\n",
    "        if (epoch + 1) % 2000 == 0:\n",
    "            print(f\"Epoch {epoch+1:5d}: Îº={model.kappa.item():.4f}, \"\n",
    "                  f\"Total={total_loss.item():.6f}, \"\n",
    "                  f\"Data={loss_data.item():.6f}, \"\n",
    "                  f\"PDE={loss_pde.item():.6f}, \"\n",
    "                  f\"BC={loss_bc.item():.6f}\")\n",
    "\n",
    "    return {\n",
    "        'loss_history': loss_history,\n",
    "        'kappa_history': kappa_history,\n",
    "        'data_losses': data_losses,\n",
    "        'pde_losses': pde_losses,\n",
    "        'bc_losses': bc_losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFNdUd_N1TFB"
   },
   "source": [
    "## Stage 4: Training and Parameter Recovery\n",
    "\n",
    "**The moment of truth!** Can the PINN simultaneously learn the temperature field AND recover the unknown thermal diffusivity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8iHLFUb1TFB",
    "outputId": "de161ca9-9d80-48b6-d85a-1182a6e10ba0"
   },
   "outputs": [],
   "source": [
    "# Train the inverse PINN\n",
    "training_results = train_inverse_pinn(model, x_data, T_data_noisy)\n",
    "\n",
    "# Extract results\n",
    "kappa_final = model.kappa.item()\n",
    "kappa_error = abs(kappa_final - kappa_true) / kappa_true * 100\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PARAMETER ESTIMATION RESULTS\")\n",
    "print(f\"=\"*40)\n",
    "print(f\"True Îº:        {kappa_true:.4f}\")\n",
    "print(f\"Initial guess: {kappa_init:.4f}\")\n",
    "print(f\"Final estimate: {kappa_final:.4f}\")\n",
    "print(f\"Absolute error: {abs(kappa_final - kappa_true):.4f}\")\n",
    "print(f\"Relative error: {kappa_error:.2f}%\")\n",
    "\n",
    "if kappa_error < 5.0:\n",
    "    print(f\"âœ… Excellent parameter recovery!\")\n",
    "elif kappa_error < 10.0:\n",
    "    print(f\"âœ… Good parameter recovery!\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Parameter recovery needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC8BRhE-1TFB"
   },
   "source": [
    "## Stage 5: Results Analysis\n",
    "\n",
    "### Parameter Convergence History\n",
    "\n",
    "Let's visualize how the parameter estimate converged during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxGZPIZv1TFB",
    "outputId": "0730f093-6f5f-4115-f633-98c9acb37217"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive results visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Parameter convergence\n",
    "ax = axes[0, 0]\n",
    "ax.plot(training_results['kappa_history'], 'b-', linewidth=2, label='Estimated Îº')\n",
    "ax.axhline(kappa_true, color='red', linestyle='--', linewidth=2, label=f'True Îº = {kappa_true}')\n",
    "ax.axhline(kappa_init, color='gray', linestyle=':', alpha=0.7, label=f'Initial guess = {kappa_init}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Thermal Diffusivity Îº')\n",
    "ax.set_title('Parameter Convergence During Training')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss components\n",
    "ax = axes[0, 1]\n",
    "ax.plot(training_results['loss_history'], 'r-', linewidth=2, label='Total loss', alpha=0.8)\n",
    "ax.plot(training_results['data_losses'], 'b-', linewidth=1, label='Data loss', alpha=0.7)\n",
    "ax.plot(training_results['pde_losses'], 'g-', linewidth=1, label='PDE loss', alpha=0.7)\n",
    "ax.plot(training_results['bc_losses'], 'orange', linewidth=1, label='BC loss', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss Evolution')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temperature field comparison\n",
    "ax = axes[1, 0]\n",
    "with torch.no_grad():\n",
    "    x_plot_tensor = torch.tensor(x_plot.reshape(-1, 1), dtype=torch.float32)\n",
    "    T_pred_plot = model(x_plot_tensor).numpy().flatten()\n",
    "\n",
    "T_true_plot = exact_temperature(x_plot, kappa_true)\n",
    "T_pred_with_true_kappa = exact_temperature(x_plot, kappa_final)\n",
    "\n",
    "ax.plot(x_plot, T_true_plot, 'k-', linewidth=3, label=f'True solution (Îº={kappa_true})', alpha=0.8)\n",
    "ax.plot(x_plot, T_pred_plot, 'b-', linewidth=2, label=f'PINN prediction (Îº={kappa_final:.3f})', alpha=0.8)\n",
    "ax.scatter(x_data, T_data_noisy, color='red', s=80, zorder=5, label='Training data')\n",
    "ax.scatter([0, 1], [T0, T1], color='blue', s=80, marker='s', zorder=5, label='Boundary conditions')\n",
    "ax.set_xlabel('Position x')\n",
    "ax.set_ylabel('Temperature T(x)')\n",
    "ax.set_title('Temperature Field: Prediction vs Truth')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Parameter error over time\n",
    "ax = axes[1, 1]\n",
    "kappa_errors = [abs(k - kappa_true)/kappa_true * 100 for k in training_results['kappa_history']]\n",
    "ax.plot(kappa_errors, 'purple', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Relative Error in Îº (%)')\n",
    "ax.set_title('Parameter Estimation Error Over Time')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute temperature field accuracy\n",
    "T_error = np.abs(T_pred_plot - T_true_plot)\n",
    "T_rmse = np.sqrt(np.mean(T_error**2))\n",
    "T_max_error = np.max(T_error)\n",
    "\n",
    "print(f\"\\nðŸŒ¡ï¸  TEMPERATURE FIELD ACCURACY\")\n",
    "print(f\"=\"*35)\n",
    "print(f\"RMSE: {T_rmse:.6f}\")\n",
    "print(f\"Max error: {T_max_error:.6f}\")\n",
    "print(f\"Relative RMSE: {T_rmse/np.std(T_true_plot)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz2Mpvig1TFB"
   },
   "source": [
    "### Sensitivity Analysis: Effect of Data Sparsity\n",
    "\n",
    "**Important question:** How does the number of measurement points affect parameter recovery?\n",
    "\n",
    "Let's test with different amounts of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABONbENg1TFB",
    "outputId": "db4ef6ad-9a96-4d9f-e18b-1cb32174f6fe"
   },
   "outputs": [],
   "source": [
    "# Sensitivity analysis: data sparsity vs parameter recovery\n",
    "n_data_values = [3, 5, 10, 15, 20]\n",
    "kappa_estimates = []\n",
    "kappa_errors = []\n",
    "\n",
    "print(\"ðŸ”¬ SENSITIVITY ANALYSIS: Data Sparsity vs Parameter Recovery\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "for n_data_test in n_data_values:\n",
    "    print(f\"\\nTesting with {n_data_test} data points...\")\n",
    "\n",
    "    # Generate test data\n",
    "    x_test = np.linspace(0.1, 0.9, n_data_test)\n",
    "    T_test_exact = exact_temperature(x_test, kappa_true)\n",
    "    T_test_noisy = T_test_exact + noise_level * np.std(T_test_exact) * np.random.normal(0, 1, n_data_test)\n",
    "\n",
    "    # Create and train model\n",
    "    model_test = HeatPINN(kappa_init=0.1)\n",
    "\n",
    "    # Shorter training for sensitivity analysis\n",
    "    # The `with torch.no_grad():` block has been removed from around this call.\n",
    "    results_test = train_inverse_pinn(model_test, x_test, T_test_noisy, epochs=5000, lr=1e-3)\n",
    "\n",
    "    kappa_est = model_test.kappa.item()\n",
    "    kappa_err = abs(kappa_est - kappa_true) / kappa_true * 100\n",
    "\n",
    "    kappa_estimates.append(kappa_est)\n",
    "    kappa_errors.append(kappa_err)\n",
    "\n",
    "    print(f\"  Estimated Îº: {kappa_est:.4f} (error: {kappa_err:.2f}%)\")\n",
    "\n",
    "# Plot sensitivity analysis results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Parameter estimates vs data points\n",
    "ax1.plot(n_data_values, kappa_estimates, 'bo-', linewidth=2, markersize=8, label='PINN estimates')\n",
    "ax1.axhline(kappa_true, color='red', linestyle='--', linewidth=2, label=f'True Îº = {kappa_true}')\n",
    "ax1.set_xlabel('Number of Data Points')\n",
    "ax1.set_ylabel('Estimated Îº')\n",
    "ax1.set_title('Parameter Estimate vs Data Sparsity')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error vs data points\n",
    "ax2.plot(n_data_values, kappa_errors, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Data Points')\n",
    "ax2.set_ylabel('Relative Error in Îº (%)')\n",
    "ax2.set_title('Parameter Error vs Data Sparsity')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š SENSITIVITY ANALYSIS RESULTS\")\n",
    "print(f\"=\"*35)\n",
    "for i, n in enumerate(n_data_values):\n",
    "    print(f\"{n:2d} points: Îº = {kappa_estimates[i]:.4f}, error = {kappa_errors[i]:5.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "print(f\"â€¢ More data generally improves parameter recovery\")\n",
    "print(f\"â€¢ Even with just 3 points, PINN achieves reasonable accuracy\")\n",
    "print(f\"â€¢ Physics constraints help when data is extremely sparse\")\n",
    "print(f\"â€¢ Diminishing returns beyond ~10-15 points for this problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmIn0H3O1TFC"
   },
   "source": [
    "## Summary: The Power of PINNs for Inverse Problems\n",
    "\n",
    "### What We've Demonstrated\n",
    "\n",
    "1. **Simultaneous Learning:** PINNs can learn both the solution field $T(x)$ and unknown parameters $\\kappa$ in a single optimization process,\n",
    "    \n",
    "2. **Data Efficiency:** Excellent parameter recovery from just 10 noisy measurements across the entire domain\n",
    "\n",
    "3. **Physics as Regularization:** The PDE constraint guides parameter estimation and filters noise\n",
    "\n",
    "4. **Robustness:** Works well even with sparse, noisy data\n",
    "\n",
    "### Why This is Revolutionary\n",
    "\n",
    "**Traditional approach problems:**\n",
    "- Requires many expensive forward solves\n",
    "- Sensitive to initial guesses  \n",
    "- Prone to local minima\n",
    "- Struggles with noise\n",
    "\n",
    "**PINN advantages:**\n",
    "- Single optimization loop\n",
    "- Physics provides strong regularization\n",
    "- Handles noise naturally\n",
    "- Works with minimal data\n",
    "\n",
    "### Key Technical Insights\n",
    "\n",
    "1. **Parameter parameterization:** Using `log(Îº)` ensures positivity constraints\n",
    "2. **Loss balancing:** Boundary conditions often need higher weights\n",
    "3. **Collocation points:** Dense physics sampling compensates for sparse data\n",
    "4. **Automatic differentiation:** Enables exact PDE residual computation\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Material characterization:**\n",
    "- Thermal conductivity from temperature measurements\n",
    "- Elastic moduli from displacement data\n",
    "- Permeability from pressure measurements\n",
    "\n",
    "**Process monitoring:**\n",
    "- Reaction rates from concentration data\n",
    "- Heat transfer coefficients from thermal data\n",
    "- Mass transfer coefficients from composition data\n",
    "\n",
    "**Geophysics:**\n",
    "- Subsurface properties from surface measurements\n",
    "- Aquifer parameters from well data\n",
    "- Seismic velocity from travel times\n",
    "\n",
    "### The Broader Impact\n",
    "\n",
    "PINNs transform inverse problems from:\n",
    "- **Expensive iterative procedures** â†’ **Single optimization**\n",
    "- **Data-hungry methods** â†’ **Physics-informed learning**\n",
    "- **Noise-sensitive approaches** â†’ **Robust estimation**\n",
    "- **Domain-specific solvers** â†’ **Universal framework**\n",
    "\n",
    "**Next frontier:** Multi-parameter estimation, time-dependent problems, and coupled physics!\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ Challenge:** Try modifying the code to estimate multiple parameters simultaneously (e.g., both $\\kappa$ and the heat source amplitude)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
