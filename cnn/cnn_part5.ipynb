{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divided-endorsement",
   "metadata": {},
   "source": [
    "# Multi node Distributed training with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de185291",
   "metadata": {},
   "source": [
    "## Multi Node Application with PyTorch Distributed: Message Exchanging Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a86e55",
   "metadata": {},
   "source": [
    "Make sure you are <b>NOT</b> using the container <code>cnn_course_container</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bad5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p cnn_part5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cnn_part5/message_passing.py\n",
    "\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "import torch\n",
    "\n",
    "LOCAL_RANK = int(os.environ['LOCAL_RANK'])\n",
    "WORLD_RANK = int(os.environ['RANK'])\n",
    "WORLD_SIZE = int(os.environ['WORLD_SIZE'])\n",
    "\n",
    "def run():\n",
    "    tensor = torch.zeros(1)\n",
    "    \n",
    "    # Send tensor to GPU device\n",
    "    device = torch.device(\"cuda:{}\".format(LOCAL_RANK))\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    if WORLD_RANK == 0:\n",
    "        for rank_recv in range(1, WORLD_SIZE):\n",
    "            dist.send(tensor=tensor, dst=rank_recv)\n",
    "            print('worker_{} sent data to Rank {}\\n'.format(0, rank_recv))\n",
    "    else:\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "        print('worker_{} has received data from rank {}\\n'.format(WORLD_RANK, 0))\n",
    "\n",
    "def init_processes():\n",
    "     dist.init_process_group(backend=\"nccl\", #\"nccl\" for using GPUs, \"gloo\" for using CPUs\n",
    "                          world_size=WORLD_SIZE, \n",
    "                          rank=WORLD_RANK)\n",
    "     run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init_processes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cnn_part5/message_passing.sh\n",
    "\n",
    "#!/bin/bash\n",
    "LOCAL_RANK=$PMI_RANK\n",
    "\n",
    "NODEFILE=\"$SCRATCH/hostfile\"\n",
    "scontrol show hostnames > $NODEFILE\n",
    "if [[ -z \"$NODEFILE\" ]]; then\n",
    "    RANKS=$NODEFILE\n",
    "    NNODES=1\n",
    "else\n",
    "    MAIN_RANK=$(head -n 1 $NODEFILE)\n",
    "    RANKS=$(tr '\\n' ' ' < $NODEFILE)\n",
    "    NNODES=$(< $NODEFILE wc -l)\n",
    "fi\n",
    "\n",
    "\n",
    "PRELOAD=\"/opt/apps/tacc-apptainer/1.3.3/bin/apptainer exec --nv --bind /run/user:/run/user /scratch1/07980/sli4/containers/cnn_course_latest.sif \"\n",
    "CMD=\"python3 -m torch.distributed.run --nproc_per_node 4 --nnodes $NNODES --node_rank=$LOCAL_RANK --master_addr=$MAIN_RANK --master_port=1234 cnn_part5/message_passing.py\"\n",
    "\n",
    "FULL_CMD=\"$PRELOAD $CMD\"\n",
    "echo \"Training command: $FULL_CMD\"\n",
    "\n",
    "eval $FULL_CMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03695c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x cnn_part5/message_passing.sh\n",
    "! mpirun -np 2 -ppn 1 cnn_part5/message_passing.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead7808",
   "metadata": {},
   "source": [
    "## Multi Node Application with PyTorch Distributed: Damage Level Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab44d1c",
   "metadata": {},
   "source": [
    "Download the dataset to $SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c44a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cnn_part5/copy_data.sh \n",
    "\n",
    "SOURCE_TAR=$1\n",
    "DEST_DIR=$2\n",
    "\n",
    "mkdir -p $DEST_DIR\n",
    "\n",
    "FULL_CMD=\"cp -r $SOURCE_TAR $DEST_DIR; \"\n",
    "FULL_CMD+=\"tar zxf $DEST_DIR/data.tar.gz -C $DEST_DIR; \"\n",
    "FULL_CMD+=\"ls $DEST_DIR/Dataset_2; \"\n",
    "FULL_CMD+=\"rm $DEST_DIR/data.tar.gz; \"\n",
    "\n",
    "\n",
    "NODEFILE=\"$SCRATCH/hostfile\"\n",
    "scontrol show hostnames $SLURM_NODELIST > $NODEFILE\n",
    "\n",
    "if [[ -z \"${NODEFILE}\" ]]; then\n",
    "    RANKS=$HOSTNAME\n",
    "else\n",
    "    RANKS=$(tr '\\n' ' ' < $NODEFILE)\n",
    "fi\n",
    "\n",
    "echo \"Command: $FULL_CMD\"\n",
    "\n",
    "# Launch execute the command on each worker (use ssh for remote nodes)\n",
    "RANK=0\n",
    "for NODE in $RANKS; do\n",
    "    if [[ \"$NODE\" == \"$HOSTNAME\" ]]; then\n",
    "        echo \"Launching rank $RANK on local node $NODE\"\n",
    "        eval $FULL_CMD &\n",
    "    fi\n",
    "    RANK=$((RANK+1))\n",
    "done\n",
    "\n",
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x cnn_part5/copy_data.sh \n",
    "! cnn_part5/copy_data.sh /work2/10000/zw427/data.tar.gz $SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70201fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cnn_part5/torch_train_distributed.py\n",
    "\n",
    "# NOTE: This is the main script of using Distributed Data Parallel to train ResNet\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "    message=\"torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\",\n",
    ")\n",
    "\n",
    "# Define the GPUs that will be used in this script\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(\n",
    "    str(x) for x in list(range(torch.cuda.device_count()))\n",
    ")\n",
    "\n",
    "# Apply transformations to our data.\n",
    "# The datasets transformations are the same as the ones from part 2 of this tutorial.\n",
    "def load_datasets(train_path, val_path, test_path):\n",
    "    val_img_transform = transforms.Compose(\n",
    "        [transforms.Resize((244, 244)), transforms.ToTensor()]\n",
    "    )\n",
    "    train_img_transform = transforms.Compose(\n",
    "        [transforms.AutoAugment(), transforms.Resize((244, 244)), transforms.ToTensor()]\n",
    "    )\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform=train_img_transform)\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=val_img_transform)\n",
    "    test_dataset = (\n",
    "        datasets.ImageFolder(test_path, transform=val_img_transform)\n",
    "        if test_path is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    if rank == 0:\n",
    "        print(\n",
    "            f\"Train set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\"\n",
    "        )\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Construct Dataloaders\n",
    "# The DistributedSampler we use here restricts data loading to a subset of the dataset.\n",
    "# In conjunction with DistributedDataParallel (shows up later in this tutorial), each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.\n",
    "def construct_dataloaders(train_set, val_set, test_set, batch_size, shuffle=True):\n",
    "    train_sampler = DistributedSampler(dataset=train_set, shuffle=shuffle)\n",
    "    val_sampler = DistributedSampler(dataset=val_set, shuffle=False)\n",
    "    test_sampler = (\n",
    "        DistributedSampler(dataset=test_set, shuffle=False)\n",
    "        if test_set is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_set, batch_size=batch_size, sampler=val_sampler, num_workers=4\n",
    "    )\n",
    "    test_dataloader = (\n",
    "        torch.utils.data.DataLoader(\n",
    "            test_set, batch_size, sampler=test_sampler, num_workers=4\n",
    "        )\n",
    "        if test_set is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "# Building the Neural Network\n",
    "# This is the same from part 2 of this tutorial\n",
    "def getResNet():\n",
    "    resnet = models.resnet34(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "    # Fix the conv layers parameters\n",
    "    for conv_param in resnet.parameters():\n",
    "        conv_param.require_grad = False\n",
    "\n",
    "    # get the input dimension for this layer\n",
    "    num_ftrs = resnet.fc.in_features\n",
    "\n",
    "    # build the new final mlp layers of network\n",
    "    fc = nn.Sequential(nn.Linear(num_ftrs, num_ftrs), nn.ReLU(), nn.Linear(num_ftrs, 3))\n",
    "\n",
    "    # replace final fully connected layer\n",
    "    resnet.fc = fc\n",
    "    return resnet\n",
    "\n",
    "\n",
    "# Model evaluation.\n",
    "# This is implemented in the same way as part 2 of this tutorial.\n",
    "@torch.no_grad()\n",
    "def eval_model(data_loader, model, loss_fn, DEVICE):\n",
    "    model.train(False)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0.0, 0.0\n",
    "    n = len(data_loader)\n",
    "\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        pred = model(x)\n",
    "        loss += loss_fn(pred, y) / len(x)\n",
    "        pred_label = torch.argmax(pred, axis=1)\n",
    "        accuracy += torch.sum(pred_label == y) / len(x)\n",
    "\n",
    "    return loss / n, accuracy / n\n",
    "\n",
    "\n",
    "# Model training.\n",
    "# This is implemented in the same way as part 2 of this tutorial.\n",
    "def train(train_loader, val_loader, model, opt, scheduler, loss_fn, epoch_start, epochs, DEVICE, checkpoint_file, prev_best_val_acc):\n",
    "    n = len(train_loader)\n",
    "\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "\n",
    "    best_val_acc = (\n",
    "        torch.tensor(0.0).to(DEVICE) if prev_best_val_acc is None else prev_best_val_acc\n",
    "    )\n",
    "\n",
    "    for epoch in range(epoch_start, epochs):\n",
    "        model.train(True)\n",
    "\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        avg_loss, val_loss, val_acc, avg_acc = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(x)\n",
    "            pred_label = torch.argmax(pred, axis=1)\n",
    "            avg_acc += torch.sum(pred_label == y) / len(x)\n",
    "\n",
    "        val_loss, val_acc = eval_model(val_loader, model, loss_fn, DEVICE)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        total_time = torch.tensor((end_time - start_time).seconds).to(DEVICE)\n",
    "\n",
    "        # Learning rate reducer takes action\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        avg_loss, avg_acc = avg_loss / n, avg_acc / n\n",
    "\n",
    "        # Only machine rank==0 (master machine) saves the model and prints the metrics\n",
    "        if rank == 0:\n",
    "\n",
    "            # Save the best model that has the highest val accuracy\n",
    "            if val_acc.item() > best_val_acc.item():\n",
    "                print(f'\\nlr for this epoch is {scheduler.get_last_lr()}')\n",
    "                print(f\"Prev Best Val Acc: {best_val_acc} < Cur Val Acc: {val_acc}\")\n",
    "                print(\"Saving the new best model...\")\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"machine\": local_rank,\n",
    "                        \"model_state_dict\": model.module.state_dict(),\n",
    "                        \"accuracy\": val_acc,\n",
    "                        \"loss\": val_loss,\n",
    "                    },\n",
    "                    checkpoint_file,\n",
    "                )\n",
    "                best_val_acc = val_acc\n",
    "                print(\"Finished saving model\\n\")\n",
    "\n",
    "            # Print the metrics (should be same on all machines)\n",
    "            print(f\"\\n(Epoch {epoch+1}/{epochs}) Time: {total_time}s\")\n",
    "            print(\n",
    "                f\"(Epoch {epoch+1}/{epochs}) Average train loss: {avg_loss}, Average train accuracy: {avg_acc}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"(Epoch {epoch+1}/{epochs}) Val loss: {val_loss}, Val accuracy: {val_acc}\"\n",
    "            )\n",
    "            print(f\"(Epoch {epoch+1}/{epochs}) Current best val acc: {best_val_acc}\\n\")\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, DEVICE):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def load_model_fm_checkpoint(checkpoint, primitive_model):\n",
    "    primitive_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    return primitive_model\n",
    "\n",
    "\n",
    "def init_distributed():\n",
    "\n",
    "    dist_url = \"env://\"\n",
    "\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",  # \"nccl\" for using GPUs, \"gloo\" for using CPUs\n",
    "        init_method=dist_url,\n",
    "        world_size=world_size,\n",
    "        rank=rank,\n",
    "    )\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    print(\"Cleaning up the distributed environment...\")\n",
    "    dist.destroy_process_group()\n",
    "    print(\"Distributed environment has been properly closed\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    hp = {\"lr\": 1e-4, \"batch_size\": 16, \"epochs\": 5}\n",
    "\n",
    "    # Please specify the path to train, cross_validation, and test images below:\n",
    "\n",
    "    train_path = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Train/\")\n",
    "    val_path   = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Validation/\")\n",
    "    test_path  = None\n",
    "    \n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    DEVICE = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "    # For saving the trained model\n",
    "    model_folder_path = os.path.join(os.environ['SCRATCH'], \"cnn5_output_model\") \n",
    "    os.makedirs(model_folder_path,exist_ok=True)\n",
    "\n",
    "    # same loss function as part 2\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(DEVICE)\n",
    "    train_set, val_set, test_set = load_datasets(train_path, val_path, test_path)\n",
    "    train_dataloader, val_dataloader, test_dataloader = construct_dataloaders(\n",
    "        train_set, val_set, test_set, hp[\"batch_size\"], True\n",
    "    )\n",
    "\n",
    "    model = getResNet().to(DEVICE)\n",
    "\n",
    "    # load the checkpoint that has the best performance in previous experiments\n",
    "    prev_best_val_acc = None\n",
    "    checkpoint_file = os.path.join(model_folder_path, \"best_model.pt\")  \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        prev_best_val_acc = checkpoint[\"accuracy\"]\n",
    "        model = load_model_fm_checkpoint(checkpoint, model)\n",
    "        epoch_start = checkpoint[\"epoch\"]\n",
    "        if rank == 0:\n",
    "            print(f\"resuming training from epoch {epoch_start}\")\n",
    "    else:\n",
    "        epoch_start = 0\n",
    "\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=hp[\"lr\"])\n",
    "\n",
    "    # same learning rate scheduler as part 2\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"min\", factor=0.1, patience=5, min_lr=1e-8\n",
    "    )\n",
    "\n",
    "    train(train_dataloader, val_dataloader, model, opt, scheduler, loss_fn, epoch_start, hp[\"epochs\"], DEVICE, checkpoint_file, prev_best_val_acc)\n",
    "\n",
    "    # only the node with rank 0 does the loading, evaluation and printing to avoild duplicate\n",
    "    if rank == 0:\n",
    "        primitive_model = getResNet().to(DEVICE)\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        best_model = load_model_fm_checkpoint(checkpoint, primitive_model)\n",
    "        loss, acc = eval_model(val_dataloader, best_model, loss_fn, DEVICE)\n",
    "        print(f\"\\nBest model (val loss: {loss}, val accuracy: {acc}) has been saved to {checkpoint_file}\\n\")\n",
    "        cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init_distributed()\n",
    "\n",
    "    gc.collect()\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        with torch.cuda.device(f\"cuda:{i}\"):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39937dda",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile cnn_part5/run_distributed.sh\n",
    "\n",
    "#!/bin/bash\n",
    "LOCAL_RANK=$PMI_RANK\n",
    "\n",
    "NODEFILE=\"$SCRATCH/hostfile\"\n",
    "scontrol show hostnames > $NODEFILE\n",
    "if [[ -z \"$NODEFILE\" ]]; then\n",
    "    RANKS=$NODEFILE\n",
    "    NNODES=1\n",
    "else\n",
    "    MAIN_RANK=$(head -n 1 $NODEFILE)\n",
    "    RANKS=$(tr '\\n' ' ' < $NODEFILE)\n",
    "    NNODES=$(< $NODEFILE wc -l)\n",
    "fi\n",
    "\n",
    "\n",
    "PRELOAD=\"/opt/apps/tacc-apptainer/1.3.3/bin/apptainer exec --nv --bind /run/user:/run/user /scratch1/07980/sli4/containers/cnn_course_latest.sif \"\n",
    "CMD=\"python3 -m torch.distributed.run --nproc_per_node 4 --nnodes $NNODES --node_rank=$LOCAL_RANK --master_addr=$MAIN_RANK --master_port=1234 cnn_part5/torch_train_distributed.py $@\"\n",
    "\n",
    "FULL_CMD=\"$PRELOAD $CMD\"\n",
    "echo \"Training command: $FULL_CMD\"\n",
    "\n",
    "eval $FULL_CMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x cnn_part5/run_distributed.sh\n",
    "! mpirun -np 2 -ppn 1 cnn_part5/run_distributed.sh "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
