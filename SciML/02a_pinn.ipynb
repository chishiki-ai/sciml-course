{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4TDxi5D1SMi"
   },
   "source": [
    "# Physics-Informed Neural Networks (PINNs)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand why standard neural networks fail for physics problems\n",
    "- Learn how to incorporate physics into neural network training\n",
    "- Master automatic differentiation for computing derivatives\n",
    "- Compare data-driven vs physics-informed approaches\n",
    "\n",
    "\n",
    "**Slides:** [![View PDF](https://img.shields.io/badge/View-PDF-red?style=flat-square&logo=googledocs&logoColor=white)](https://github.com/chishiki-ai/sciml/raw/main/docs/01-pinn/pinn-slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7subCox1SMj"
   },
   "source": [
    "## The Problem: A Damped Harmonic Oscillator\n",
    "\n",
    "We begin with a concrete problem that everyone understands: a mass on a spring with damping. This is the perfect starting point because:\n",
    "\n",
    "1. **Physical intuition**: Everyone knows how springs work\n",
    "2. **Mathematical tractability**: We have an exact solution\n",
    "3. **Clear demonstration**: Shows why standard ML fails and PINNs succeed\n",
    "\n",
    "### The Physical System\n",
    "\n",
    "![Harmonic Oscillator](https://github.com/chishiki-ai/sciml/blob/main/docs/01-pinn/figs/harmonic-oscillator.png?raw=1)\n",
    "\n",
    "A mass $m$ attached to a spring (constant $k$) with damping (coefficient $c$). The displacement $u(t)$ from equilibrium satisfies:\n",
    "\n",
    "$$m \\frac{d^2 u}{dt^2} + c \\frac{du}{dt} + ku = 0$$\n",
    "\n",
    "**Initial conditions:** $u(0) = 1$, $\\frac{du}{dt}(0) = 0$ (starts at rest, displaced)\n",
    "\n",
    "**Parameters:** $m = 1$, $c = 4$, $k = 400$ (underdamped: $c^2 < 4mk$)\n",
    "\n",
    "### The Exact Solution\n",
    "\n",
    "For underdamped motion ($\\delta < \\omega_0$ where $\\delta = c/(2m)$ and $\\omega_0 = \\sqrt{k/m}$):\n",
    "\n",
    "$$u(t) = e^{-\\delta t}\\left(\\cos(\\omega t) + \\frac{\\delta}{\\omega}\\sin(\\omega t)\\right)$$\n",
    "\n",
    "where $\\omega = \\sqrt{\\omega_0^2 - \\delta^2}$ is the damped frequency.\n",
    "\n",
    "> This example is adapted from Ben Moseley's [blog post](https://web.archive.org/web/20240618153956/https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXczpI-i1SMj",
    "outputId": "67a2f701-e153-4bee-cb37-c710f945931e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Physical parameters\n",
    "m = 1.0      # mass\n",
    "c = 4.0      # damping coefficient\n",
    "k = 400.0    # spring constant\n",
    "\n",
    "# Derived parameters\n",
    "delta = c / (2 * m)                # damping ratio\n",
    "omega_0 = np.sqrt(k / m)           # natural frequency\n",
    "omega = np.sqrt(omega_0**2 - delta**2) # damped frequency\n",
    "\n",
    "print(f\"Physical parameters:\")\n",
    "print(f\"  δ = {delta:.2f} (damping ratio)\")\n",
    "print(f\"  ω₀ = {omega_0:.2f} (natural frequency)\")\n",
    "print(f\"  ω = {omega:.2f} (damped frequency)\")\n",
    "print(f\"  Underdamped: {delta < omega_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQwwx8Nm1SMk",
    "outputId": "089ec785-4d8a-411e-c913-856a8dd8ba0c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def oscillator(d, w0, x):\n",
    "    \"\"\"Analytical solution to the 1D underdamped harmonic oscillator problem.\"\"\"\n",
    "    assert d < w0\n",
    "    w = np.sqrt(w0**2 - d**2)\n",
    "    phi = np.arctan(-d / w)\n",
    "    A = 1 / (2 * np.cos(phi))\n",
    "    cos = np.cos(phi + w * x)\n",
    "    exp = np.exp(-d * x)\n",
    "    y = exp * 2 * A * cos\n",
    "    return y\n",
    "\n",
    "def create_spring_oscillator_animation_inline():\n",
    "    d = 2  # damping coefficient\n",
    "    w0 = 20  # natural frequency\n",
    "\n",
    "    # Animation variables\n",
    "    totalTime = 1.0  # Time domain [0, 1]\n",
    "    dt = 0.0075\n",
    "    t_array = np.arange(0, totalTime, dt)\n",
    "    y_array = oscillator(d, w0, t_array)\n",
    "\n",
    "    # Scaling factors\n",
    "    scale = 1.0\n",
    "    centerY = 0.0\n",
    "\n",
    "    # Compute y_min and y_max from the displacement data\n",
    "    y_min = np.min(y_array * scale + centerY)\n",
    "    y_max = np.max(y_array * scale + centerY)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, (ax_trace, ax_spring) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # Plot the displacement curve on ax_trace\n",
    "    ax_trace.plot(t_array, y_array * scale + centerY, color='gray')\n",
    "    trace_point, = ax_trace.plot([], [], 'bo', markersize=8)\n",
    "    ax_trace.set_xlim(0, totalTime)\n",
    "    ax_trace.set_ylim(-1.1, 1.1)\n",
    "    ax_trace.set_xlabel('Time (s)')\n",
    "    ax_trace.set_ylabel('Displacement')\n",
    "    ax_trace.set_title('Displacement vs. Time')\n",
    "\n",
    "    # Set up the mass-spring system on ax_spring\n",
    "    ax_spring.set_xlim(-1, 1)\n",
    "    ax_spring.set_ylim(-1.1, 1.1)\n",
    "    ax_spring.axis('off')\n",
    "    ax_spring.set_title('Mass-Spring System')\n",
    "\n",
    "    # Draw the fixed block at equilibrium position (y=0)\n",
    "    ax_spring.plot([-0.2, 0.2], [1.0, 1.0], 'k-', linewidth=4)\n",
    "\n",
    "    # Initialize the mass and spring\n",
    "    mass, = ax_spring.plot([], [], 'bo', markersize=20)\n",
    "    spring_line, = ax_spring.plot([], [], 'k-', linewidth=1.5)\n",
    "\n",
    "    def get_spring(y_start, y_end, coils=10, points_per_coil=15):\n",
    "        length = y_end - y_start\n",
    "        t = np.linspace(0, 1, coils * points_per_coil)\n",
    "        x = 0.06 * np.sin(2 * np.pi * coils * t)\n",
    "        y = y_start + length * t\n",
    "        return x, y\n",
    "\n",
    "    def update(frame):\n",
    "        t = t_array[frame % len(t_array)]\n",
    "        y = oscillator(d, w0, t) * scale + centerY\n",
    "\n",
    "        # Update trace point\n",
    "        trace_point.set_data([t], [y])\n",
    "\n",
    "        # Update mass position\n",
    "        mass.set_data([0], [y])\n",
    "\n",
    "        # Update spring\n",
    "        x_spring, y_spring = get_spring(1.0, y)  # Starting from y=1.0 (fixed point)\n",
    "        spring_line.set_data(x_spring, y_spring)\n",
    "\n",
    "        return trace_point, mass, spring_line\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(t_array), interval=20, blit=True)\n",
    "\n",
    "    plt.close(fig)\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "# Call the function to display the animation\n",
    "create_spring_oscillator_animation_inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HPbxhDT1SMl"
   },
   "source": [
    "### Creating Sparse Training Data\n",
    "\n",
    "In real applications, we don't have the complete solution. We only have **sparse, potentially noisy measurements**. Let's simulate this scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1i6s_BTB1SMl",
    "outputId": "0d5bb679-ba30-4889-8e21-2902dcb4d2e7"
   },
   "outputs": [],
   "source": [
    "# Generate sparse training data\n",
    "n_data = 10  # Only 10 data points!\n",
    "\n",
    "def exact_solution(t):\n",
    "    \"\"\"Analytical solution to the damped harmonic oscillator\"\"\"\n",
    "    return np.exp(-delta * t) * (np.cos(omega * t) + (delta/omega) * np.sin(omega * t))\n",
    "\n",
    "# Get solution\n",
    "t_data = np.linspace(0, 0.3607, n_data)\n",
    "u_data = exact_solution(t_data)\n",
    "\n",
    "# Exact solution\n",
    "t_exact = np.linspace(0, 1, 500)\n",
    "u_exact = exact_solution(t_exact)\n",
    "\n",
    "\n",
    "# Add some noise to make it realistic\n",
    "noise_level = 0.02\n",
    "u_data_noisy = u_data + noise_level * np.random.normal(0, 1, len(u_data))\n",
    "\n",
    "# Visualize the sparse data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t_exact, u_exact, 'k--', linewidth=2, alpha=0.7, label='True solution')\n",
    "plt.scatter(t_data, u_data_noisy, color='red', s=100, zorder=5, label=f'Training data ({n_data} points)')\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('Displacement u(t)')\n",
    "plt.title('The Challenge: Reconstruct the Full Solution from Sparse Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training data: {n_data} points with noise level {noise_level}\")\n",
    "print(\"Challenge: Can a neural network reconstruct the full solution?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Spn7kdSw1SMl"
   },
   "outputs": [],
   "source": [
    "def exact_velocity(t):\n",
    "    \"\"\"\n",
    "    First derivative of the exact solution: du/dt\n",
    "\n",
    "    Given: u(t) = e^(-δt) * [cos(ωt) + (δ/ω)sin(ωt)]\n",
    "\n",
    "    Using product rule: d/dt[f(t)g(t)] = f'(t)g(t) + f(t)g'(t)\n",
    "    where f(t) = e^(-δt) and g(t) = cos(ωt) + (δ/ω)sin(ωt)\n",
    "    \"\"\"\n",
    "    exp_term = np.exp(-delta * t)\n",
    "    cos_term = np.cos(omega * t)\n",
    "    sin_term = np.sin(omega * t)\n",
    "\n",
    "    # Derivative of exponential term: d/dt[e^(-δt)] = -δe^(-δt)\n",
    "    d_exp_term = -delta * exp_term\n",
    "\n",
    "    # Derivative of trigonometric term: d/dt[cos(ωt) + (δ/ω)sin(ωt)]\n",
    "    d_trig_term = -omega * sin_term + (delta/omega) * omega * cos_term\n",
    "    d_trig_term = -omega * sin_term + delta * cos_term\n",
    "\n",
    "    # Apply product rule\n",
    "    velocity = d_exp_term * (cos_term + (delta/omega) * sin_term) + exp_term * d_trig_term\n",
    "\n",
    "    return velocity\n",
    "\n",
    "def exact_acceleration(t):\n",
    "    \"\"\"\n",
    "    Second derivative of the exact solution: d²u/dt²\n",
    "\n",
    "    This can be computed by differentiating the velocity function,\n",
    "    or directly from the ODE: d²u/dt² = -(c/m)(du/dt) - (k/m)u\n",
    "\n",
    "    Using the ODE relation is more numerically stable:\n",
    "    m * d²u/dt² + c * du/dt + k * u = 0\n",
    "    Therefore: d²u/dt² = -(c/m) * du/dt - (k/m) * u\n",
    "    \"\"\"\n",
    "    u = exact_solution(t)\n",
    "    du_dt = exact_velocity(t)\n",
    "\n",
    "    # From the ODE: m * d²u/dt² = -c * du/dt - k * u\n",
    "    d2u_dt2 = -(c/m) * du_dt - (k/m) * u\n",
    "\n",
    "    return d2u_dt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHAN2KN81SMm"
   },
   "source": [
    "## Stage 1: The Data-Only Approach (Why It Fails)\n",
    "\n",
    "**The Natural First Attempt:** Train a neural network to fit the sparse data points.\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "![Standard Neural Network](https://github.com/chishiki-ai/sciml/blob/main/docs/01-pinn/figs/oscillator-nn.png?raw=1)\n",
    "\n",
    "A simple feedforward network:\n",
    "- **Input:** Time $t$\n",
    "- **Hidden layers:** Dense layers with activation functions\n",
    "- **Output:** Predicted displacement $\\hat{u}_\\theta(t)$\n",
    "\n",
    "**Loss function:** Mean squared error between predictions and data\n",
    "$$\\mathcal{L}_{\\text{data}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N |\\hat{u}_\\theta(t_i) - u_i|^2$$\n",
    "\n",
    "**Training:** Standard gradient descent to minimize $\\mathcal{L}_{\\text{data}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG1I_xz-1SMm"
   },
   "source": [
    "## Theoretical Foundation: Universal Approximation Theorem\n",
    "\n",
    "Before we dive into implementation, we need to understand **why** neural networks can solve differential equations. The answer lies in the Universal Approximation Theorem and its extension to **Sobolev spaces**.\n",
    "\n",
    "### Classical Universal Approximation Theorem\n",
    "\n",
    "As we learned in the MLP module, the classical UAT tells us that feedforward networks can approximate continuous functions:\n",
    "\n",
    "**Theorem (Cybenko, 1989):** Let $\\sigma$ be a continuous, non-constant, and bounded activation function. Then finite sums of the form:\n",
    "$$F(x) = \\sum_{j=1}^N c_j \\sigma(w_j \\cdot x + b_j)$$\n",
    "are dense in $C(K)$ for any compact set $K \\subset \\mathbb{R}^d$.\n",
    "\n",
    "**Translation:** Given enough neurons, neural networks can approximate any continuous function arbitrarily well.\n",
    "\n",
    "### Extension to Sobolev Spaces: The Key for PDEs\n",
    "\n",
    "**But here's the critical insight:** For differential equations, we don't just need to approximate functions—we need to approximate **functions and their derivatives simultaneously**.\n",
    "\n",
    "This is where **Sobolev spaces** become essential.\n",
    "\n",
    "**Definition (Sobolev Space $H^k(\\Omega)$):** The space of functions whose weak derivatives up to order $k$ are square-integrable:\n",
    "$$H^k(\\Omega) = \\left\\{ u : \\Omega \\to \\mathbb{R} \\,:\\, \\sum_{|\\alpha| \\leq k} \\|D^\\alpha u\\|_{L^2(\\Omega)}^2 < \\infty \\right\\}$$\n",
    "\n",
    "where $D^\\alpha u$ denotes the weak derivative of multi-index $\\alpha$ with $|\\alpha| = \\alpha_1 + \\alpha_2 + \\cdots + \\alpha_d$.\n",
    "\n",
    "**Extended Universal Approximation Theorem:** Neural networks with sufficiently smooth activation functions can approximate functions in Sobolev spaces $H^k(\\Omega)$.\n",
    "\n",
    "**Mathematical Statement:** Let $\\sigma \\in C^k(\\mathbb{R})$ (i.e., $\\sigma$ is $k$ times continuously differentiable). Then for any $u \\in H^k(\\Omega)$ and $\\epsilon > 0$, there exists a neural network $\\hat{u}_\\theta$ such that:\n",
    "$$\\|u - \\hat{u}_\\theta\\|_{H^k} < \\epsilon$$\n",
    "\n",
    "where the Sobolev norm is:\n",
    "$$\\|u\\|_{H^k}^2 = \\sum_{|\\alpha| \\leq k} \\|D^\\alpha u\\|_{L^2}^2$$\n",
    "\n",
    "### Why This Matters for PINNs\n",
    "\n",
    "**Critical Connection:** When we solve a differential equation of order $k$, we need:\n",
    "\n",
    "1. **Function approximation:** $\\hat{u}_\\theta(x) \\approx u(x)$\n",
    "2. **Derivative approximation:** $\\frac{\\partial^j \\hat{u}_\\theta}{\\partial x^j} \\approx \\frac{\\partial^j u}{\\partial x^j}$ for $j = 1, 2, \\ldots, k$\n",
    "\n",
    "The extended UAT guarantees this is possible provided:\n",
    "- **Activation function smoothness:** $\\sigma \\in C^k$ (at least $k$ times differentiable)\n",
    "- **Sufficient network capacity:** Enough neurons and layers\n",
    "\n",
    "**For our oscillator ODE:** $m\\frac{d^2u}{dt^2} + c\\frac{du}{dt} + ku = 0$\n",
    "- We need $k = 2$ (second-order equation)\n",
    "- Activation function must be $C^2$ (twice differentiable)\n",
    "- $\\tanh$, $\\sin$, Swish ✓ | ReLU ✗\n",
    "\n",
    "**The Magic:** Automatic differentiation + UAT in Sobolev spaces = neural networks that can learn solutions to differential equations!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKvmochP1SMm",
    "outputId": "d598f554-8950-4943-9e53-59aa4830ba45"
   },
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Standard feedforward neural network\"\"\"\n",
    "    def __init__(self, hidden_size=32, n_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(1, hidden_size))  # Input: time t\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Tanh())  # Smooth activation (important!)\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_size, 1))  # Output: displacement u\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.network(t)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "t_data_tensor = torch.tensor(t_data.reshape(-1, 1), dtype=torch.float32)\n",
    "u_data_tensor = torch.tensor(u_data_noisy.reshape(-1, 1), dtype=torch.float32)\n",
    "t_test_tensor = torch.tensor(t_exact.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"  Training: {t_data_tensor.shape} -> {u_data_tensor.shape}\")\n",
    "print(f\"  Testing: {t_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3knd9u6c1SMn"
   },
   "source": [
    "### Training the Standard Neural Network\n",
    "\n",
    "**What we expect:** The network should learn to pass through the data points.\n",
    "\n",
    "**What we hope:** It will interpolate smoothly between points.\n",
    "\n",
    "**What actually happens:** Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqJDx-Ph1SMn",
    "outputId": "a53103c2-84d1-49af-9e58-838925feab3f"
   },
   "outputs": [],
   "source": [
    "def train_standard_nn(model, t_data, u_data, epochs=5000, lr=1e-3):\n",
    "    \"\"\"Train a standard neural network on data only\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    pbar = tqdm(range(epochs), desc=\"Training Standard NN\")\n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        u_pred = model(t_data)\n",
    "        # Data loss only\n",
    "        loss = criterion(u_pred, u_data)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update progress bar with loss information every 100 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.6f}'})\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Create and train the standard neural network\n",
    "standard_nn = SimpleNN()\n",
    "losses_standard = train_standard_nn(standard_nn, t_data_tensor, u_data_tensor)\n",
    "print(f\"Final training loss: {losses_standard[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNi4MjzZ1SMn"
   },
   "source": [
    "### The Failure of the Data-Only Approach\n",
    "\n",
    "**Critical Question:** How well does it predict the full solution?\n",
    "\n",
    "![Standard NN Result](https://github.com/chishiki-ai/sciml/blob/main/docs/01-pinn/figs/oscillator-result-nn.png?raw=1)\n",
    "\n",
    "**What we observe:** The network fits the training points but fails catastrophically between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az8wUHrx1SMn",
    "outputId": "1bfa5327-5182-460a-886f-99bcff759846"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the full time domain\n",
    "with torch.no_grad():\n",
    "    u_pred_standard = standard_nn(t_test_tensor).numpy().flatten()\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Full comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t_exact, u_exact, 'k-', linewidth=3, label='True solution', alpha=0.8)\n",
    "plt.plot(t_exact, u_pred_standard, 'b--', linewidth=2, label='Standard NN prediction')\n",
    "plt.scatter(t_data, u_data_noisy, color='red', s=80, zorder=5, label='Training data')\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('Displacement u(t)')\n",
    "plt.title('Standard Neural Network: Data-Only Training')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error analysis\n",
    "plt.subplot(2, 1, 2)\n",
    "error = np.abs(u_pred_standard - u_exact)\n",
    "plt.plot(t_exact, error, 'r-', linewidth=2, label='Absolute error')\n",
    "plt.scatter(t_data, np.zeros_like(t_data), color='red', s=80, zorder=5,\n",
    "            label='Training data locations')\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('|Error|')\n",
    "plt.title('Prediction Error vs Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute metrics\n",
    "mse = np.mean((u_pred_standard - u_exact)**2)\n",
    "max_error = np.max(np.abs(u_pred_standard - u_exact))\n",
    "\n",
    "print(f\" Standard NN Performance:\")\n",
    "print(f\"  MSE: {mse:.6f}\")\n",
    "print(f\"  Max Error: {max_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw1w7-yK1SMn"
   },
   "source": [
    "## Stage 2: Enter Physics-Informed Neural Networks\n",
    "\n",
    "**The Key Insight:** Instead of just fitting data, enforce the differential equation!\n",
    "\n",
    "### The PINN Architecture\n",
    "\n",
    "![PINN Architecture](https://github.com/chishiki-ai/sciml/blob/main/docs/01-pinn/figs/oscillator-pinn-nn.png?raw=1)\n",
    "\n",
    "**Same network, different loss function:**\n",
    "- Network still predicts $\\hat{u}_\\theta(t)$\n",
    "- But now we compute derivatives via **automatic differentiation**\n",
    "- Physics residual: $\\mathcal{R}_\\theta(t) = m\\frac{d^2\\hat{u}_\\theta}{dt^2} + c\\frac{d\\hat{u}_\\theta}{dt} + k\\hat{u}_\\theta$\n",
    "\n",
    "### The Physics Residual\n",
    "\n",
    "**Mathematical Foundation:** If $\\hat{u}_\\theta(t)$ is the exact solution, then:\n",
    "$$m\\frac{d^2\\hat{u}_\\theta}{dt^2} + c\\frac{d\\hat{u}_\\theta}{dt} + k\\hat{u}_\\theta = 0$$\n",
    "\n",
    "**PINN Strategy:** Make this residual as small as possible everywhere in the domain.\n",
    "\n",
    "**Collocation Points:** We evaluate the residual at many points $\\{t_j\\}$ throughout $[0,1]$, not just at data points.\n",
    "\n",
    "### The Complete PINN Loss Function\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}}(\\theta) = \\mathcal{L}_{\\text{data}}(\\theta) + \\lambda \\mathcal{L}_{\\text{physics}}(\\theta)$$\n",
    "\n",
    "where:\n",
    "\n",
    "**Data Loss:** $\\mathcal{L}_{\\text{data}}(\\theta) = \\frac{1}{N_{\\text{data}}} \\sum_{i=1}^{N_{\\text{data}}} |\\hat{u}_\\theta(t_i) - u_i|^2$\n",
    "\n",
    "**Physics Loss:** $\\mathcal{L}_{\\text{physics}}(\\theta) = \\frac{1}{N_{\\text{colloc}}} \\sum_{j=1}^{N_{\\text{colloc}}} |\\mathcal{R}_\\theta(t_j)|^2$\n",
    "\n",
    "**Balance Parameter:** $\\lambda$ controls data vs physics trade-off\n",
    "\n",
    "### Automatic Differentiation: The Secret Weapon\n",
    "\n",
    "**Critical Question:** How do we compute $\\frac{d\\hat{u}_\\theta}{dt}$ and $\\frac{d^2\\hat{u}_\\theta}{dt^2}$?\n",
    "\n",
    "**Answer:** Automatic differentiation (AD) gives us **exact** derivatives!\n",
    "\n",
    "- No finite differences\n",
    "- No numerical errors\n",
    "- Computed via chain rule through the computational graph\n",
    "- Available in PyTorch, TensorFlow, JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF9Z98Cv1SMn"
   },
   "source": [
    "### Demonstration: Automatic Differentiation in Action\n",
    "\n",
    "Let's see how automatic differentiation works in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kynk84He1SMn",
    "outputId": "9aac8f40-ccde-4f19-dfdc-367b501cca64"
   },
   "outputs": [],
   "source": [
    "def compute_derivatives_demo():\n",
    "    \"\"\"Demonstrate automatic differentiation\"\"\"\n",
    "    # Create a simple test case: u(t) = sin(t)\n",
    "    t = torch.tensor([0.5], requires_grad=True)  # Enable gradient computation\n",
    "    u = torch.sin(t)  # u = sin(t)\n",
    "\n",
    "    print(\"Function: u(t) = sin(t)\")\n",
    "    print(f\"At t = {t.item():.2f}:\")\n",
    "    print(f\"  u = {u.item():.6f}\")\n",
    "\n",
    "    # First derivative: du/dt\n",
    "    du_dt = torch.autograd.grad(u, t, create_graph=True)[0]\n",
    "    print(f\"  du/dt = {du_dt.item():.6f} (exact: cos({t.item():.2f}) = {np.cos(t.item()):.6f})\")\n",
    "\n",
    "    # Second derivative: d²u/dt²\n",
    "    d2u_dt2 = torch.autograd.grad(du_dt, t, create_graph=True)[0]\n",
    "    print(f\"  d²u/dt² = {d2u_dt2.item():.6f} (exact: -sin({t.item():.2f}) = {-np.sin(t.item()):.6f})\")\n",
    "\n",
    "    print(\"✅ Automatic differentiation gives exact derivatives!\")\n",
    "\n",
    "compute_derivatives_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsEBQBfA1SMn"
   },
   "source": [
    "## Stage 3: PINN Implementation\n",
    "\n",
    "**Now the real work begins!** Let's implement a Physics-Informed Neural Network step by step.\n",
    "\n",
    "### Step 1: Physics Loss Function\n",
    "\n",
    "The heart of PINN is computing the physics residual using automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T8E2P9g1SMn",
    "outputId": "6b7a2991-930c-498c-e5e1-b18f4ce9be05"
   },
   "outputs": [],
   "source": [
    "def physics_loss(model, t_colloc, m, c, k):\n",
    "    \"\"\"\n",
    "    Compute the physics loss for the damped harmonic oscillator\n",
    "\n",
    "    ODE: m * d²u/dt² + c * du/dt + k * u = 0\n",
    "    \"\"\"\n",
    "    # Ensure gradients are enabled for input\n",
    "    t_colloc = t_colloc.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Forward pass: compute u(t)\n",
    "    u = model(t_colloc)\n",
    "\n",
    "    # First derivative: du/dt\n",
    "    du_dt = torch.autograd.grad(\n",
    "        outputs=u,\n",
    "        inputs=t_colloc,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True,  # Allow higher-order derivatives\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Second derivative: d²u/dt²\n",
    "    d2u_dt2 = torch.autograd.grad(\n",
    "        outputs=du_dt,\n",
    "        inputs=t_colloc,\n",
    "        grad_outputs=torch.ones_like(du_dt),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Physics residual: R = m * d²u/dt² + c * du/dt + k * u\n",
    "    residual = m * d2u_dt2 + c * du_dt + k * u\n",
    "\n",
    "    # Mean squared residual\n",
    "    physics_loss = torch.mean(residual**2)\n",
    "\n",
    "    return physics_loss\n",
    "\n",
    "# Test the physics loss function\n",
    "test_model = SimpleNN()\n",
    "t_test_colloc = torch.linspace(0, 1, 50).reshape(-1, 1)\n",
    "test_loss = physics_loss(test_model, t_test_colloc, m, c, k)\n",
    "\n",
    "print(f\"Physics loss (untrained model): {test_loss.item():.6f}\")\n",
    "print(\"This is large since the model hasn't learned the physics yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tisvXk81SMo"
   },
   "source": [
    "### Step 2: Complete PINN Training Loop\n",
    "\n",
    "**Key Components:**\n",
    "1. **Data loss:** Fit the sparse measurements\n",
    "2. **Physics loss:** Satisfy the differential equation\n",
    "3. **Collocation points:** Where we enforce physics (not necessarily data points)\n",
    "4. **Balance parameter $\\lambda$:** Controls the trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQCKd6Bu1SMo",
    "outputId": "400b8410-6401-4406-ceec-fc58bc2d9daf"
   },
   "outputs": [],
   "source": [
    "def train_pinn(model, t_data, u_data, t_colloc, m, c, k,\n",
    "               epochs=10000, lr=1e-3, lambda_physics=1e-4):\n",
    "    \"\"\"\n",
    "    Train a Physics-Informed Neural Network\n",
    "\n",
    "    Args:\n",
    "        model: Neural network\n",
    "        t_data, u_data: Training data points\n",
    "        t_colloc: Collocation points for physics\n",
    "        m, c, k: Physical parameters\n",
    "        lambda_physics: Balance parameter between data and physics\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Storage for loss history\n",
    "    data_losses = []\n",
    "    physics_losses = []\n",
    "    total_losses = []\n",
    "\n",
    "    print(f\"Training PINN with λ = {lambda_physics}\")\n",
    "    print(f\"Data points: {len(t_data)}, Collocation points: {len(t_colloc)}\")\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Training PINN\")\n",
    "    for epoch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Data loss: how well do we fit the measurements?\n",
    "        u_pred_data = model(t_data)\n",
    "        loss_data = criterion(u_pred_data, u_data)\n",
    "\n",
    "        # Physics loss: how well do we satisfy the ODE?\n",
    "        loss_physics = physics_loss(model, t_colloc, m, c, k)\n",
    "\n",
    "        # Total loss: balance data fitting and physics\n",
    "        total_loss = loss_data + lambda_physics * loss_physics\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store losses\n",
    "        data_losses.append(loss_data.item())\n",
    "        physics_losses.append(loss_physics.item())\n",
    "        total_losses.append(total_loss.item())\n",
    "\n",
    "        if (epoch + 1) % 2000 == 0:\n",
    "            pbar.set_postfix({'Loss': f'{loss_data.item():.6f}',\n",
    "                              'Physics': f'{loss_physics.item():.6f}',\n",
    "                              'Total': f'{total_loss.item():.6f}'})\n",
    "\n",
    "    return data_losses, physics_losses, total_losses\n",
    "\n",
    "# Setup for PINN training\n",
    "n_colloc = 200  # Number of collocation points\n",
    "t_colloc = torch.linspace(0, 1, n_colloc).reshape(-1, 1)\n",
    "lambda_physics = 1e-4  # Balance parameter\n",
    "\n",
    "print(f\"Collocation points: uniformly distributed in [0, 1]\")\n",
    "print(f\"Physics weight λ = {lambda_physics} (typically much smaller than 1)\")\n",
    "\n",
    "# Visualize training setup\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(t_data, u_data_noisy, color='red', s=100, zorder=5, label='Data points (fit these)')\n",
    "plt.scatter(t_colloc.numpy().flatten()[::10], np.zeros(len(t_colloc[::10])),\n",
    "            color='green', marker='^', s=60, alpha=0.7, label='Collocation points (enforce physics)')\n",
    "plt.plot(t_exact, u_exact, 'k--', alpha=0.7, label='True solution')\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('Displacement')\n",
    "plt.title('PINN Training Setup: Data Points vs Collocation Points')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyydqYuC1SMo"
   },
   "source": [
    "### Step 3: Train the PINN\n",
    "\n",
    "**The moment of truth!** Let's train the PINN and see if it can learn both the data and the physics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyt5j-p91SMo",
    "outputId": "616b07f9-116b-4de2-efd9-fdf911302c21"
   },
   "outputs": [],
   "source": [
    "# Create a fresh PINN model\n",
    "pinn_model = SimpleNN()\n",
    "\n",
    "# Train the PINN\n",
    "data_losses, physics_losses, total_losses = train_pinn(\n",
    "    pinn_model, t_data_tensor, u_data_tensor, t_colloc, m, c, k, epochs=20000\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    u_pred_pinn = pinn_model(t_test_tensor).numpy().flatten()\n",
    "\n",
    "print(f\"PINN Training Complete!\")\n",
    "print(f\"Final data loss: {data_losses[-1]:.6f}\")\n",
    "print(f\"Final physics loss: {physics_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnFulouu1SMo"
   },
   "source": [
    "## Stage 4: The Moment of Truth - Comparing Results\n",
    "\n",
    "### Direct Comparison: Standard NN vs PINN\n",
    "\n",
    "![PINN Result](https://github.com/chishiki-ai/sciml/blob/main/docs/01-pinn/figs/oscillator-result-pinn.png?raw=1)\n",
    "\n",
    "**What we expect to see:**\n",
    "- **Standard NN:** Fits data points but fails between them\n",
    "- **PINN:** Fits data points AND follows physics everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IDiO3x01SMo",
    "outputId": "03e836a6-26a6-43b9-c4b4-612576bedd6a"
   },
   "outputs": [],
   "source": [
    "# Compare Standard NN vs PINN\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Solutions comparison\n",
    "ax = axes[0, 0]\n",
    "ax.plot(t_exact, u_exact, 'k-', linewidth=3, label='True solution', alpha=0.8)\n",
    "ax.plot(t_exact, u_pred_standard, 'b--', linewidth=2, label='Standard NN', alpha=0.8)\n",
    "ax.plot(t_exact, u_pred_pinn, 'r-', linewidth=2, label='PINN', alpha=0.8)\n",
    "ax.scatter(t_data, u_data_noisy, color='red', s=80, zorder=5, label='Training data')\n",
    "ax.set_xlabel('Time t')\n",
    "ax.set_ylabel('Displacement u(t)')\n",
    "ax.set_title('Solution Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error comparison\n",
    "ax = axes[0, 1]\n",
    "error_standard = np.abs(u_pred_standard - u_exact)\n",
    "error_pinn = np.abs(u_pred_pinn - u_exact)\n",
    "ax.plot(t_exact, error_standard, 'b-', linewidth=2, label='Standard NN error')\n",
    "ax.plot(t_exact, error_pinn, 'r-', linewidth=2, label='PINN error')\n",
    "ax.set_xlabel('Time t')\n",
    "ax.set_ylabel('Absolute Error')\n",
    "ax.set_title('Error Comparison')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Loss evolution for Standard NN\n",
    "ax = axes[1, 0]\n",
    "ax.plot(losses_standard, 'b-', linewidth=2, label='Data loss only')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Standard NN Training')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Loss evolution for PINN\n",
    "ax = axes[1, 1]\n",
    "ax.plot(data_losses, 'b-', linewidth=2, label='Data loss', alpha=0.8)\n",
    "ax.plot(physics_losses, 'g-', linewidth=2, label='Physics loss', alpha=0.8)\n",
    "ax.plot(total_losses, 'r-', linewidth=2, label='Total loss', alpha=0.8)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('PINN Training (Multiple Loss Components)')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantitative comparison\n",
    "mse_standard = np.mean((u_pred_standard - u_exact)**2)\n",
    "mse_pinn = np.mean((u_pred_pinn - u_exact)**2)\n",
    "max_error_standard = np.max(np.abs(u_pred_standard - u_exact))\n",
    "max_error_pinn = np.max(np.abs(u_pred_pinn - u_exact))\n",
    "\n",
    "print(\"📊 QUANTITATIVE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Metric':<20} {'Standard NN':<15} {'PINN':<15} {'Improvement':<15}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'MSE':<20} {mse_standard:<15.6f} {mse_pinn:<15.6f} {mse_standard/mse_pinn:<15.1f}x\")\n",
    "print(f\"{'Max Error':<20} {max_error_standard:<15.6f} {max_error_pinn:<15.6f} {max_error_standard/max_error_pinn:<15.1f}x\")\n",
    "\n",
    "print(f\"✅ PINN is {mse_standard/mse_pinn:.0f}x more accurate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayPWJZnJ1SMo"
   },
   "source": [
    "### Phase Portrait Analysis: The Ultimate Physics Test\n",
    "\n",
    "**Physical Insight:** For a harmonic oscillator, the phase portrait (velocity vs displacement) reveals the underlying dynamics. Real oscillators trace smooth spirals in phase space as energy dissipates.\n",
    "\n",
    "**Critical Test:** Can our neural networks capture this fundamental physical behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLXn8WrJ1SMp",
    "outputId": "9d133314-8b47-4a6d-f96c-16e0ef3a94b7"
   },
   "outputs": [],
   "source": [
    "def compute_model_derivatives(model, t_tensor):\n",
    "    \"\"\"Compute derivatives of the model using automatic differentiation\"\"\"\n",
    "    t_tensor = t_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # Function value\n",
    "    u = model(t_tensor)\n",
    "\n",
    "    # First derivative\n",
    "    du_dt = torch.autograd.grad(u, t_tensor, grad_outputs=torch.ones_like(u),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # Second derivative\n",
    "    d2u_dt2 = torch.autograd.grad(du_dt, t_tensor, grad_outputs=torch.ones_like(du_dt),\n",
    "                                 create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    return u.detach().numpy(), du_dt.detach().numpy(), d2u_dt2.detach().numpy()\n",
    "\n",
    "# Compute derivatives for both models\n",
    "# REMOVED torch.no_grad() context - this was causing the error\n",
    "# Standard NN derivatives\n",
    "u_std, du_dt_std, d2u_dt2_std = compute_model_derivatives(standard_nn, t_test_tensor)\n",
    "\n",
    "# PINN derivatives\n",
    "u_pinn, du_dt_pinn, d2u_dt2_pinn = compute_model_derivatives(pinn_model, t_test_tensor)\n",
    "\n",
    "# Analytical derivatives for comparison\n",
    "v_exact = exact_velocity(t_exact)\n",
    "a_exact = exact_acceleration(t_exact)\n",
    "\n",
    "# Plot derivative comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "# Displacement\n",
    "ax = axes[0]\n",
    "ax.plot(t_exact, u_exact, 'k-', linewidth=3, label='Exact')\n",
    "ax.plot(t_exact, u_std.flatten(), 'b--', linewidth=2, label='Standard NN', alpha=0.8)\n",
    "ax.plot(t_exact, u_pinn.flatten(), 'r-', linewidth=2, label='PINN', alpha=0.8)\n",
    "ax.scatter(t_data, u_data_noisy, color='red', s=60, zorder=5, alpha=0.8)\n",
    "ax.set_ylabel('Displacement u(t)')\n",
    "ax.set_title('Function and Derivatives: Standard NN vs PINN')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# First derivative (velocity)\n",
    "ax = axes[1]\n",
    "ax.plot(t_exact, v_exact, 'k-', linewidth=3, label='Exact du/dt')\n",
    "ax.plot(t_exact, du_dt_std.flatten(), 'b--', linewidth=2, label='Standard NN du/dt', alpha=0.8)\n",
    "ax.plot(t_exact, du_dt_pinn.flatten(), 'r-', linewidth=2, label='PINN du/dt', alpha=0.8)\n",
    "ax.set_ylabel('Velocity du/dt')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Second derivative (acceleration)\n",
    "ax = axes[2]\n",
    "ax.plot(t_exact, a_exact, 'k-', linewidth=3, label='Exact d²u/dt²')\n",
    "ax.plot(t_exact, d2u_dt2_std.flatten(), 'b--', linewidth=2, label='Standard NN d²u/dt²', alpha=0.8)\n",
    "ax.plot(t_exact, d2u_dt2_pinn.flatten(), 'r-', linewidth=2, label='PINN d²u/dt²', alpha=0.8)\n",
    "ax.set_ylabel('Acceleration d²u/dt²')\n",
    "ax.set_xlabel('Time t')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Derivative errors\n",
    "print(\"📈 DERIVATIVE ACCURACY\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "velocity_error_std = np.mean((du_dt_std.flatten() - v_exact)**2)\n",
    "velocity_error_pinn = np.mean((du_dt_pinn.flatten() - v_exact)**2)\n",
    "accel_error_std = np.mean((d2u_dt2_std.flatten() - a_exact)**2)\n",
    "accel_error_pinn = np.mean((d2u_dt2_pinn.flatten() - a_exact)**2)\n",
    "\n",
    "print(f\"Velocity MSE (du/dt):\")\n",
    "print(f\"  Standard NN: {velocity_error_std:.6f}\")\n",
    "print(f\"  PINN: {velocity_error_pinn:.6f}\")\n",
    "print(f\"  Improvement: {velocity_error_std/velocity_error_pinn:.1f}x\")\n",
    "\n",
    "print(f\"Acceleration MSE (d²u/dt²):\")\n",
    "print(f\"  Standard NN: {accel_error_std:.6f}\")\n",
    "print(f\"  PINN: {accel_error_pinn:.6f}\")\n",
    "print(f\"  Improvement: {accel_error_std/accel_error_pinn:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbXUVAzN1SMp"
   },
   "source": [
    "### Deep Dive: Derivative Analysis\n",
    "\n",
    "**Critical Test:** Can the PINN learn physically consistent derivatives?\n",
    "\n",
    "Since we enforce the ODE through derivatives, the PINN should naturally learn correct $\\frac{du}{dt}$ and $\\frac{d^2u}{dt^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSWJ9vy01SMp",
    "outputId": "204963b1-6c4a-49f8-85fa-30ab79672874"
   },
   "outputs": [],
   "source": [
    "# Analytical derivatives for comparison\n",
    "t_exact = np.linspace(0, 1, 500)\n",
    "v_exact = exact_velocity(t_exact)\n",
    "a_exact = exact_acceleration(t_exact)\n",
    "\n",
    "# Phase Portrait Analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Phase Portrait\n",
    "ax1.plot(u_exact, v_exact, 'k-', linewidth=3, label='Exact phase portrait', alpha=0.8)\n",
    "ax1.plot(u_std.flatten(), du_dt_std.flatten(), 'b--', linewidth=2, label='Standard NN', alpha=0.8)\n",
    "ax1.plot(u_pinn.flatten(), du_dt_pinn.flatten(), 'r-', linewidth=2, label='PINN', alpha=0.8)\n",
    "\n",
    "# Mark start and end points\n",
    "ax1.scatter(u_exact[0], v_exact[0], color='green', s=100, marker='o', zorder=5, label='Start (t=0)')\n",
    "ax1.scatter(u_exact[-1], v_exact[-1], color='red', s=100, marker='X', zorder=5, label='End (t=1)')\n",
    "\n",
    "ax1.set_xlabel('Displacement u(t)')\n",
    "ax1.set_ylabel('Velocity du/dt')\n",
    "ax1.set_title('Phase Portrait: du/dt vs u(t)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(0, color='black', linewidth=0.5)\n",
    "ax1.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Plot 2: Energy Analysis\n",
    "# Total energy E = (1/2)m*v² + (1/2)k*u² for undamped case\n",
    "# For damped oscillator, energy should decrease monotonically\n",
    "energy_exact = 0.5 * m * v_exact**2 + 0.5 * k * u_exact**2\n",
    "energy_std = 0.5 * m * du_dt_std.flatten()**2 + 0.5 * k * u_std.flatten()**2\n",
    "energy_pinn = 0.5 * m * du_dt_pinn.flatten()**2 + 0.5 * k * u_pinn.flatten()**2\n",
    "\n",
    "ax2.plot(t_exact, energy_exact, 'k-', linewidth=3, label='Exact energy', alpha=0.8)\n",
    "ax2.plot(t_exact, energy_std, 'b--', linewidth=2, label='Standard NN energy', alpha=0.8)\n",
    "ax2.plot(t_exact, energy_pinn, 'r-', linewidth=2, label='PINN energy', alpha=0.8)\n",
    "ax2.set_xlabel('Time t')\n",
    "ax2.set_ylabel('Total Energy E(t)')\n",
    "ax2.set_title('Energy Dissipation Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy2fjtI11SMp"
   },
   "source": [
    "## Summary: Why PINNs Work\n",
    "\n",
    "### The Revolutionary Insight\n",
    "\n",
    "**Traditional ML:** Learn patterns from data alone\n",
    "\n",
    "**PINNs:** Learn patterns from data AND physics simultaneously\n",
    "\n",
    "### Key Advantages of PINNs\n",
    "\n",
    "1. **Regularization Effect:** Physics constraints prevent overfitting\n",
    "    - Standard NN can fit any function through the data points\n",
    "    - PINN is constrained to solutions that satisfy the ODE\n",
    "\n",
    "2. **Better Interpolation:** Smooth, physically meaningful predictions between data points\n",
    "    - Standard NN: arbitrary interpolation\n",
    "    - PINN: physics-guided interpolation\n",
    "\n",
    "3. **Accurate Derivatives:** Natural consequence of physics enforcement\n",
    "    - Automatic differentiation + physics loss = correct derivatives\n",
    "    - Critical for applications requiring gradients (optimization, control)\n",
    "\n",
    "4. **Data Efficiency:** Less training data needed\n",
    "    - Physics provides strong inductive bias\n",
    "    - Can generalize from very sparse measurements\n",
    "\n",
    "### The Universal Approximation Foundation\n",
    "\n",
    "**Why this works theoretically:**\n",
    "- Neural networks can approximate functions in Sobolev spaces $H^k$\n",
    "- Sobolev spaces include both functions AND their derivatives\n",
    "- With smooth activation functions ($\\tanh$, $\\sin$), we can approximate solutions to differential equations\n",
    "- Automatic differentiation makes this practical\n",
    "\n",
    "### When to Use PINNs\n",
    "\n",
    "**Ideal scenarios:**\n",
    "- ✅ Known governing equations (PDEs/ODEs)\n",
    "- ✅ Sparse, noisy data\n",
    "- ✅ Need physically consistent solutions\n",
    "- ✅ Require accurate derivatives\n",
    "- ✅ Complex geometries (where finite elements struggle)\n",
    "\n",
    "**Limitations:**\n",
    "- ❌ Unknown physics\n",
    "- ❌ Highly nonlinear/chaotic systems\n",
    "- ❌ Large-scale problems (computational cost)\n",
    "- ❌ Discontinuous solutions\n",
    "\n",
    "### Extensions and Applications\n",
    "\n",
    "**This framework extends to:**\n",
    "- **Partial Differential Equations:** Heat equation, wave equation, Navier-Stokes\n",
    "- **Inverse Problems:** Estimate unknown parameters from data\n",
    "- **Multi-physics:** Coupled systems (fluid-structure interaction)\n",
    "- **High Dimensions:** Curse of dimensionality breaking\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "**PINNs = Universal Function Approximation + Physics Constraints + Automatic Differentiation**\n",
    "\n",
    "This combination creates a powerful method for solving differential equations with neural networks, particularly when data is sparse and physics is well-understood.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:** Try this approach on the 1D Poisson equation with hard constraints!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
