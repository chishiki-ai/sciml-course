{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e219b8",
   "metadata": {},
   "source": [
    "# CNN Part 3: Introduction to DDP with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a3f7d",
   "metadata": {},
   "source": [
    "Check for GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85079b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPUs are available = {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of GPUs available are {}'.format(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You are currently using GPU with local rank = {}'.format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5f5f2",
   "metadata": {},
   "source": [
    "## Introduction to DDP with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a788c0",
   "metadata": {},
   "source": [
    "### Create Process Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c230af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "def init_distributed(local_rank, world_size):\n",
    "    '''\n",
    "    local_rank: identifier for pariticular GPU on one node\n",
    "    world: total number of process in a the group\n",
    "    '''\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
    "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
    "    torch.cuda.set_device(local_rank)                 \n",
    "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
    "                            rank=local_rank,          # rank of the current process being used\n",
    "                            world_size=world_size)    # total number of processors being used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d45e96",
   "metadata": {},
   "source": [
    "### Create Data DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def load_dataset(train_dataset):\n",
    "    train_data = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,\n",
    "        #################################################\n",
    "        shuffle=False,                             # shuffle should be set to False when using DistributedSampler\n",
    "        sampler=DistributedSampler(train_dataset), # passing the distributed loader\n",
    "        ################################################\n",
    "    )\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81dd6a",
   "metadata": {},
   "source": [
    "## MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697029b",
   "metadata": {},
   "source": [
    "### Non-Distributed Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f8c27",
   "metadata": {},
   "source": [
    "#### Get Data\n",
    "Load the MNIST dataset and pass it to a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size=32):\n",
    "\n",
    "    # download MNIST dataset\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "                            root=os.path.join(os.environ['SCRATCH'], \"data\"),      # path to where data is stored\n",
    "                            train=True,                                         # specifies if data is train or test\n",
    "                            download=True,                                      # downloads data if not available at root\n",
    "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
    "                            )\n",
    "    # pass dataset to the dataloader\n",
    "    train_dataloader = DataLoader(trainset,\n",
    "                                  shuffle=False,\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "trainloader=prepare_data(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13c3fb",
   "metadata": {},
   "source": [
    "Visualize a few images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26697fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80949c99",
   "metadata": {},
   "source": [
    "#### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        prob = self.linear_relu_stack(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769bcdf",
   "metadata": {},
   "source": [
    "#### Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # transfer data to GPU if available\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def main(device):\n",
    "\n",
    "    # Setup Dataloader\n",
    "    train_dataloader=prepare_data(batch_size=4)\n",
    "    \n",
    "    # Instantiate Model \n",
    "    model = Net().to(device)\n",
    "    \n",
    "    # instantiate loss and optimizer \n",
    "    loss_fn = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train Model \n",
    "    epochs = 3\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, model, loss_fn, optimizer)\n",
    "        \n",
    "    print(\"Done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92989f",
   "metadata": {},
   "source": [
    "Train the model by calling `main`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = main(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf5793",
   "metadata": {},
   "source": [
    "### Distributed Code for Multiple GPUs on One Node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 1. Create a process group (function)\n",
    "def init_distributed(local_rank, world_size):\n",
    "    '''\n",
    "    local_rank: identifier for pariticular GPU on one node\n",
    "    world: total number of process in a the group\n",
    "    '''\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
    "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
    "    torch.cuda.set_device(local_rank)                 \n",
    "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
    "                            rank=local_rank,          # rank of the current process being used\n",
    "                            world_size=world_size)    # total number of processors being used\n",
    "#################################################  \n",
    "    \n",
    "def prepare_data(local_rank, world_size, batch_size=32):\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "                            root=os.path.join(os.environ['SCRATCH'], \"data\"),      # path to where data is stored\n",
    "                            train=True,                                         # specifies if data is train or test\n",
    "                            download=True,                                      # downloads data if not available at root\n",
    "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
    "                            )\n",
    "\n",
    "    # pass data to the distributed sampler and dataloader\n",
    "    train_dataloader = DataLoader(trainset,\n",
    "                                  ################################################\n",
    "                                  # 2. Setup Dataloader with Distributed Sampler\n",
    "                                  shuffle=False,\n",
    "                                  sampler=DistributedSampler(trainset, num_replicas=world_size, rank=local_rank),\n",
    "                                  ################################################\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "# training loop for one epoch\n",
    "def train_loop(local_rank, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # transfer data to GPU if available\n",
    "        X = X.to(local_rank)\n",
    "        y = y.to(local_rank)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ################################################\n",
    "        # 4. Only write/print model information on one GPU\n",
    "        if local_rank == 0:\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        ################################################\n",
    "\n",
    "def main(local_rank, world_size):\n",
    "    ################################################\n",
    "    # 1. Set up Process Group\n",
    "    init_distributed(local_rank, world_size)\n",
    "    ################################################\n",
    "\n",
    "    ################################################\n",
    "    # 2. Setup Dataloader with Distributed Sampler\n",
    "    train_dataloader = prepare_data(local_rank, world_size)\n",
    "    ################################################\n",
    "\n",
    "    ################################################\n",
    "    # 3. Wrap Model with DDP\n",
    "    model = DDP(Net().to(local_rank),\n",
    "        device_ids=[local_rank],                  # list of gpu that model lives on\n",
    "        output_device=local_rank,                 # where to output model\n",
    "    )\n",
    "    ################################################\n",
    "\n",
    "    # instantiate loss and optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train Model\n",
    "    epochs = 10\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(local_rank, train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    #################################################\n",
    "    # 5. Close Process Group\n",
    "    dist.destroy_process_group()\n",
    "    #################################################\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb2b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# helper functions to display python scripts in Markdown\n",
    "display_python = lambda x: Markdown(f\"```python\\n{Path(x).read_text()}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905113a",
   "metadata": {},
   "source": [
    "`display_python` displays `cnn_part3/mnist_parallel.py` with Python syntax highlighting as the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8db5613c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "import torchvision\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "\n",
       "\n",
       "class Net(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(Net, self).__init__()\n",
       "        self.flatten = torch.nn.Flatten()\n",
       "        self.linear_relu_stack = torch.nn.Sequential(\n",
       "            torch.nn.Linear(28*28, 128),\n",
       "            torch.nn.ReLU(),\n",
       "            torch.nn.Dropout(0.2),\n",
       "            torch.nn.Linear(128, 10),\n",
       "        )\n",
       "\n",
       "    def forward(self, x):\n",
       "        x = self.flatten(x)\n",
       "        prob = self.linear_relu_stack(x)\n",
       "        return prob\n",
       "\n",
       "def prepare_data(local_rank, world_size, batch_size=32):\n",
       "        \n",
       "    trainset = torchvision.datasets.MNIST(\n",
       "                            root=os.path.join(os.environ['SCRATCH'], \"data\"),   # path to where data is stored\n",
       "                            train=True,                                         # specifies if data is train or test\n",
       "                            download=True,                                      # downloads data if not available at root\n",
       "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
       "                            )\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(trainset,\n",
       "                                  shuffle=False,\n",
       "                                  sampler=DistributedSampler(trainset, num_replicas=world_size, rank=local_rank),\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "def init_distributed(local_rank, world_size):\n",
       "    '''\n",
       "    rank: identifier for pariticular GPU\n",
       "    world: total number of process in a the group\n",
       "    '''\n",
       "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
       "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
       "    torch.cuda.set_device(local_rank)                       \n",
       "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
       "                            rank=local_rank,                # rank of the current process being used\n",
       "                            world_size=world_size)    # total number of processors being used\n",
       "\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(local_rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(local_rank)\n",
       "        y = y.to(local_rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "        \n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if local_rank == 0:\n",
       "            if batch % 100 == 0:\n",
       "                loss, current = loss.item(), batch * len(X)\n",
       "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "        ################################################\n",
       "\n",
       "def main(local_rank, world_size):\n",
       "    ################################################\n",
       "    # 1. Set up Process Group \n",
       "    init_distributed(local_rank, world_size)\n",
       "    ################################################\n",
       "\n",
       "    ################################################\n",
       "    # 2. Setup Dataloader with Distributed Sampler\n",
       "    train_dataloader = prepare_data(local_rank, world_size)\n",
       "    ################################################\n",
       "\n",
       "    ################################################                                                 \n",
       "    # 3. Wrap Model with DDP  \n",
       "    model = DDP(Net().to(local_rank),\n",
       "        device_ids=[local_rank],                  # list of gpu that model lives on \n",
       "        output_device=local_rank,                 # where to output model\n",
       "    )        \n",
       "    ################################################\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.CrossEntropyLoss()\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=4*0.001)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 10\n",
       "    for t in range(epochs):\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if local_rank == 0:\n",
       "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        ################################################\n",
       "        train_loop(local_rank, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    #################################################\n",
       "    # 5. Close Process Group\n",
       "    dist.destroy_process_group()\n",
       "    #################################################\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    world_size= torch.cuda.device_count()\n",
       "    print('world_size = {}'.format(world_size))\n",
       "    mp.spawn(main, args=(world_size,) , nprocs=world_size)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_python(\"cnn_part3/mnist_parallel.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd350f",
   "metadata": {},
   "source": [
    "Run `cnn_part3/mnist_parallel.py` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9d7f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 cnn_part3/mnist_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356ca21",
   "metadata": {},
   "source": [
    "## Additional Exercise\n",
    "\n",
    "There is a script called `cnn_part3/simple_linear_regression_serial.py` that implements a simple linear regression model with PyTorch. Below, the script is modified to run on multiple GPUs on one node using PyTorch's DDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c951087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "def get_model():\n",
       "    return torch.nn.Sequential(\n",
       "            torch.nn.Linear(1, 1),     # first number specifies input dimension; second number specifies output dimension\n",
       "    ) \n",
       "\n",
       "def prepare_data(batch_size=32):\n",
       "    # Generate random data centered around 10 with noise\n",
       "    X = torch.randn(32*4, 1) * 10\n",
       "    y = X + torch.randn(32*4, 1) * 3\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(list(zip(X,y)),\n",
       "                                  shuffle=True,\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(rank)\n",
       "        y = y.to(rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "\n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        if batch % 2 == 0:\n",
       "            loss, current = loss.item(), batch * len(X)\n",
       "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "\n",
       "def main(device):\n",
       "    train_dataloader = prepare_data()\n",
       "\n",
       "    model = get_model().to(device)\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 20\n",
       "    for t in range(epochs):\n",
       "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        train_loop(device, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "    main(device)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_python(\"cnn_part3/simple_linear_regression_serial.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 cnn_part3/simple_linear_regression_serial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9db95e",
   "metadata": {},
   "source": [
    "`cnn_part3/simple_linear_regression_parallel.py` is the modified code to work with DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ad94c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "\n",
       "##############################################\n",
       "# 1. Create a process group\n",
       "def init_distributed(local_rank, world_size):\n",
       "    '''\n",
       "    local_rank: identifier for pariticular GPU on one node\n",
       "    world: total number of process in a the group\n",
       "    '''\n",
       "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
       "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
       "    torch.cuda.set_device(local_rank)                 \n",
       "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
       "                            rank=local_rank,          # rank of the current process being used\n",
       "                            world_size=world_size)    # total number of processors being used\n",
       "##############################################\n",
       "\n",
       "def get_model():\n",
       "    return torch.nn.Sequential(\n",
       "            torch.nn.Linear(1, 1),     # first number specifies input dimension; second number specifies output dimension\n",
       "            ) \n",
       "\n",
       "def prepare_data(batch_size=32):\n",
       "    # Generate random data centered around 10 with noise\n",
       "    X = torch.randn(32*4, 1) * 10\n",
       "    y = X + torch.randn(32*4, 1) * 3\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(list(zip(X,y)),\n",
       "                                  ##############################################\n",
       "                                  # 2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
       "                                  shuffle=False,\n",
       "                                  sampler=DistributedSampler(list(zip(X,y))),\n",
       "                                  ##############################################\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(rank)\n",
       "        y = y.to(rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "\n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        if batch % 2 == 0:\n",
       "            loss, current = loss.item(), batch * len(X)\n",
       "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "\n",
       "def main(rank, world_size):\n",
       "    init_distributed(rank, world_size)\n",
       "\n",
       "    train_dataloader = prepare_data()\n",
       "\n",
       "    ##############################################\n",
       "    # 3. Wrap Model with Pytorch's DistributedDataParallel\n",
       "    model = DDP(get_model().to(rank), device_ids=[rank], output_device=rank)\n",
       "    ##############################################\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 20\n",
       "    for t in range(epochs):\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if rank == 0:\n",
       "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        ################################################\n",
       "        train_loop(rank, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    #################################################\n",
       "    # 5. Close Process Group\n",
       "    dist.destroy_process_group()\n",
       "    #################################################\n",
       "\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    world_size= torch.cuda.device_count()\n",
       "    mp.spawn(main, args=(world_size,) , nprocs=world_size)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_python(\"cnn_part3/simple_linear_regression_parallel.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 cnn_part3/simple_linear_regression_parallel.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
