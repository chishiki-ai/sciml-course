{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOn82cwe1VYU"
   },
   "source": [
    "# Differentiable Simulation\n",
    "\n",
    "**Slides:** [![View PDF](https://img.shields.io/badge/View-PDF-red?style=flat-square&logo=googledocs&logoColor=white)](https://github.com/kks32-courses/sciml/raw/main/docs/03-diffsim/diffsim-slides.pdf)\n",
    "\n",
    "In the previous MLP notebook we introduced Automatic Differentiation (AD) as the core engine that enables the training of neural networks by computing gradients of a loss function with respect to network parameters.\n",
    "\n",
    "However, the power of AD extends far beyond just neural networks. It allows us to make **entire physical simulations differentiable**. This paradigm, often called **Differentiable Simulation** or **Differentiable Physics**, involves implementing a simulator (e.g., a PDE solver) in a framework that supports AD, such as PyTorch or JAX. By doing so, we can automatically compute the gradient of a final quantity (like a measurement or a loss function) with respect to any initial parameter of the simulation.\n",
    "\n",
    "This notebook demonstrates this powerful concept. We will:\n",
    "1. Briefly recall how gradients are computed in PyTorch.\n",
    "2. Introduce the JAX framework for high-performance differentiable programming.\n",
    "3. Build a differentiable simulator for the 1D acoustic wave equation.\n",
    "4. Use this simulator to solve a challenging inverse problem: Full Waveform Inversion (FWI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEssIQBj1VYW"
   },
   "source": [
    "## What is Differentiable Simulation?\n",
    "\n",
    "Differentiable simulation combines physics-based simulation with automatic differentiation to solve **inverse problems**.\n",
    "\n",
    "Instead of:\n",
    "- **Forward Problem**: Given parameters → predict observations\n",
    "- **Traditional Inverse**: Trial-and-error parameter search\n",
    "\n",
    "We use:\n",
    "- **Differentiable Simulation**: Compute gradients through simulation → gradient-based optimization\n",
    "\n",
    "**Key insight**: If we can compute $\\frac{\\partial \\text{simulation}}{\\partial \\text{parameters}}$, we can use gradient descent to find optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpbOz4nb1VYX"
   },
   "source": [
    "## From PyTorch to JAX: A Quick Review\n",
    "\n",
    "We introduced automatic differentiation in the MLP module.\n",
    "\n",
    "\n",
    "As we saw previously, frameworks like PyTorch keep track of all operations on tensors. When we call `.backward()` on a final scalar output (like a loss), PyTorch uses reverse-mode AD (backpropagation) to compute the gradient of that output with respect to the inputs that have `requires_grad=True`. Here's a quick recap of computing gradients in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_1Khzjc1VYY",
    "outputId": "cf2cd735-275b-4be0-f7b1-c85586a0d570"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Simple function: loss = (prediction - target)^2\n",
    "def simple_loss(params, target):\n",
    "    prediction = params[0] * 2.0 + params[1]  # ax + b\n",
    "    return (prediction - target)**2\n",
    "\n",
    "# PyTorch gradient computation\n",
    "params = torch.tensor([1.0, 0.5], requires_grad=True)\n",
    "target = 5.0\n",
    "\n",
    "loss = simple_loss(params, target)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Gradients: {params.grad}\")\n",
    "print(f\"Parameters: {params.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9I91Gtg1VYZ"
   },
   "source": [
    "## JAX: The Scientific Computing Advantage\n",
    "\n",
    "JAX provides several advantages for scientific computing:\n",
    "- **Functional programming**: Pure functions, no side effects\n",
    "- **JIT compilation**: Fast execution with `jit`\n",
    "- **Vectorization**: Automatic batching with `vmap`\n",
    "- **Transformations**: `grad`, `jit`, `vmap` compose seamlessly\n",
    "\n",
    "Same computation in JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjdt3hlY1VYb",
    "outputId": "920f92e8-6b8a-429b-be66-5b376a97ed0f"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, lax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "# Same function in JAX\n",
    "def simple_loss_jax(params, target):\n",
    "    prediction = params[0] * 2.0 + params[1]\n",
    "    return (prediction - target)**2\n",
    "\n",
    "# JAX gradient computation\n",
    "params_jax = jnp.array([1.0, 0.5])\n",
    "target_jax = 5.0\n",
    "\n",
    "# Create gradient function\n",
    "grad_fn = grad(simple_loss_jax)\n",
    "\n",
    "loss_val = simple_loss_jax(params_jax, target_jax)\n",
    "gradients = grad_fn(params_jax, target_jax)\n",
    "\n",
    "print(f\"Loss: {loss_val:.4f}\")\n",
    "print(f\"Gradients: {gradients}\")\n",
    "print(f\"Parameters: {params_jax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2iNKkdS1VYc",
    "outputId": "be87ef1d-0dc3-447a-cea0-b3be586222f9"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit, grad\n",
    "import time\n",
    "\n",
    "# Complex polynomial operation\n",
    "def f(x):\n",
    "    return x**4 - 3*x**3 + 2*x**2 - x + 1\n",
    "\n",
    "# JIT version\n",
    "@jit\n",
    "def fast_f(x):\n",
    "    return x**4 - 3*x**3 + 2*x**2 - x + 1\n",
    "\n",
    "# Large array\n",
    "x = jnp.linspace(-10, 10, 1_000_000)\n",
    "\n",
    "# Time without JIT\n",
    "start = time.time()\n",
    "result1 = f(x)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "# Time with JIT (includes compilation on first run)\n",
    "start = time.time()\n",
    "result2 = fast_f(x)\n",
    "time_with_jit = time.time() - start\n",
    "\n",
    "# Time JIT on second run (no compilation)\n",
    "start = time.time()\n",
    "result3 = fast_f(x)\n",
    "time_jit_second = time.time() - start\n",
    "\n",
    "print(f\"No JIT: {time_no_jit:.4f}s\")\n",
    "print(f\"JIT (first run): {time_with_jit:.4f}s\")\n",
    "print(f\"JIT (second run): {time_jit_second:.4f}s\")\n",
    "print(f\"Speedup: {time_no_jit/time_jit_second:.2f}x\")\n",
    "\n",
    "# Gradient example\n",
    "df = grad(f)\n",
    "fast_df = jit(grad(f))\n",
    "print(f\"Gradient at x=1.0: {df(1.0)}\")\n",
    "print(f\"JIT gradient at x=1.0: {fast_df(1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHRO0QM11VYe"
   },
   "source": [
    "## JAX Transformations: The Power of Composition\n",
    "\n",
    "JAX's key advantage is that transformations compose. You can combine `grad`, `jit`, and `vmap` in any order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59o8C7xL1VYf",
    "outputId": "3add14f5-e0a9-491a-ea7c-57928f72785e"
   },
   "outputs": [],
   "source": [
    "# vmap example - scalar function that needs vectorization\n",
    "from jax import vmap\n",
    "\n",
    "def scalar_only_func(x):\n",
    "    \"\"\"Function that only works on scalars\"\"\"\n",
    "    return jnp.where(x > 0, x**2, -x**3)\n",
    "\n",
    "# This would fail on array: scalar_only_func(x)\n",
    "# Need vmap to vectorize it\n",
    "vectorized_func = vmap(scalar_only_func)\n",
    "jit_vectorized_func = jit(vmap(scalar_only_func))\n",
    "\n",
    "# Test on smaller array\n",
    "small_x = jnp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "result_vmap = vectorized_func(small_x)\n",
    "result_jit_vmap = jit_vectorized_func(small_x)\n",
    "\n",
    "print(f\"vmap result: {result_vmap}\")\n",
    "print(f\"jit+vmap result: {result_jit_vmap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vaZj5Lj1VYf"
   },
   "source": [
    "## The 1D Wave Equation: Our Physics Model\n",
    "\n",
    "Let's implement a differentiable physics simulation. We'll use the 1D acoustic wave equation:\n",
    "\n",
    "$$\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\frac{\\partial^2 u}{\\partial x^2}$$\n",
    "\n",
    "Where:\n",
    "- $u(x,t)$ is the wavefield (pressure/displacement)\n",
    "- $c(x)$ is the wave speed (what we want to estimate)\n",
    "\n",
    "**Finite difference discretization**:\n",
    "$$u_{i}^{n+1} = 2u_{i}^{n} - u_{i}^{n-1} + \\frac{c_i^2 \\Delta t^2}{\\Delta x^2}(u_{i+1}^{n} - 2u_{i}^{n} + u_{i-1}^{n})$$\n",
    "\n",
    "**Initial conditions**: Gaussian source\n",
    "$$u_0 = \\exp\\left(-5(x - 0.5)^2\\right)$$\n",
    "\n",
    "\n",
    "### The Forward Problem: Simulation\n",
    "The forward problem is to simulate the behavior of $u(x,t)$ given an initial state and the wave speed profile $c(x)$. We will solve this using a finite difference method. By rearranging the central difference approximation, we can find the wave's state at the next timestep based on its two previous states:\n",
    "\n",
    "$$u_i^{n+1} = c_i^2 \\frac{\\Delta t^2}{\\Delta x^2} (u_{i+1}^n - 2u_i^n + u_{i-1}^n) + 2u_i^n - u_i^{n-1} $$\n",
    "\n",
    "We can implement this time-stepping loop in JAX. Using `@jit`, this loop will be compiled for high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3eIky7Q1VYg"
   },
   "source": [
    "![FWI](https://github.com/chishiki-ai/sciml/blob/main/docs/03-diffsim/figs/1dfwi.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpBkoIa41VYg",
    "outputId": "e2024d49-a5c3-47f2-8bfb-49367fd4b638"
   },
   "outputs": [],
   "source": [
    "# Set up the wave equation solver\n",
    "n = 1000  # Number of spatial points\n",
    "dx = 1.0 / (n - 1)\n",
    "x0 = jnp.linspace(0.0, 1.0, n)\n",
    "\n",
    "@jit\n",
    "def wave_propagation(params):\n",
    "    \"\"\"Solve 1D wave equation using finite differences\"\"\"\n",
    "    c = params  # Velocity model\n",
    "    dt = 5e-4   # Time step\n",
    "\n",
    "    # CFL condition check: C = c*dt/dx should be < 1\n",
    "    C = c * dt / dx\n",
    "    C2 = C * C\n",
    "\n",
    "    # Initial conditions: Gaussian source\n",
    "    u0 = jnp.exp(-(5 * (x0 - 0.5))**2)\n",
    "    u1 = jnp.exp(-(5 * (x0 - 0.5 - c * dt))**2)\n",
    "    u2 = jnp.zeros(n)\n",
    "\n",
    "    def step(i, carry):\n",
    "        u0, u1, u2 = carry\n",
    "\n",
    "        # Boundary conditions: u = 0 at boundaries\n",
    "        u1p = jnp.roll(u1, 1).at[0].set(0)      # u[j-1]\n",
    "        u1n = jnp.roll(u1, -1).at[n-1].set(0)   # u[j+1]\n",
    "\n",
    "        # Central difference scheme\n",
    "        u2 = 2 * u1 - u0 + C2 * (u1p - 2 * u1 + u1n)\n",
    "\n",
    "        # Update for next iteration\n",
    "        u0 = u1\n",
    "        u1 = u2\n",
    "        return (u0, u1, u2)\n",
    "\n",
    "    # Time stepping: 5000 steps\n",
    "    u0, u1, u2 = lax.fori_loop(0, 5000, step, (u0, u1, u2))\n",
    "\n",
    "    return u2\n",
    "\n",
    "print(\"Wave propagation solver defined successfully!\")\n",
    "print(f\"Grid points: {n}\")\n",
    "print(f\"Spatial step: {dx:.6f}\")\n",
    "print(f\"Time step: {5e-4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGP0nK781VYh"
   },
   "source": [
    "## Example 1: Constant Velocity Recovery\n",
    "\n",
    "**The Problem**: Given observed wavefield data, can we recover a constant velocity?\n",
    "\n",
    "- **Target**: $c = 1.0$ (constant)\n",
    "- **Initial guess**: $c = 0.8$\n",
    "- **Goal**: Use gradients to optimize $c$ until simulation matches observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxAWXGZL1VYh",
    "outputId": "7f67345e-0ac6-4720-ac3d-d282f05e33a8"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic observed data\n",
    "ctarget = 1.0  # True constant velocity\n",
    "target_data = wave_propagation(ctarget)\n",
    "\n",
    "# Visualize target\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x0, jnp.ones(n) * ctarget, 'c-', linewidth=3, label='Target velocity')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Velocity')\n",
    "plt.title('Target Velocity Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x0, target_data, 'b-', linewidth=2, label='Target wavefield')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Target Wavefield Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target velocity: {ctarget}\")\n",
    "print(f\"Target data shape: {target_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXxlP_RT1VYi",
    "outputId": "819733cc-f925-4237-e82c-61f8c997b83d"
   },
   "outputs": [],
   "source": [
    "# Define loss function for constant velocity inversion\n",
    "@jit\n",
    "def compute_loss_constant(c_scalar):\n",
    "    \"\"\"Loss function for constant velocity model\"\"\"\n",
    "    # Convert scalar to constant array\n",
    "    c_array = jnp.ones(n) * c_scalar\n",
    "    u2 = wave_propagation(c_array)\n",
    "    return jnp.linalg.norm(u2 - target_data)\n",
    "\n",
    "# Gradient function\n",
    "grad_loss_constant = jit(grad(compute_loss_constant))\n",
    "\n",
    "# Initial guess\n",
    "c_initial = 0.8\n",
    "print(f\"Initial guess: {c_initial}\")\n",
    "print(f\"Target: {ctarget}\")\n",
    "print(f\"Initial loss: {compute_loss_constant(c_initial):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huU3hRM01VYi",
    "outputId": "29c95b33-9c2d-4c98-8248-351cf13d508b"
   },
   "outputs": [],
   "source": [
    "# Optimize constant velocity using Adam\n",
    "def optax_adam_constant(c_init, niter):\n",
    "    \"\"\"Optimize constant velocity using Adam optimizer\"\"\"\n",
    "    start_learning_rate = 1e-3\n",
    "    optimizer = optax.adam(start_learning_rate)\n",
    "    c_param = c_init\n",
    "    opt_state = optimizer.init(c_param)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(niter):\n",
    "        loss_val = compute_loss_constant(c_param)\n",
    "        grads = grad_loss_constant(c_param)\n",
    "\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        c_param = optax.apply_updates(c_param, updates)\n",
    "\n",
    "        losses.append(loss_val)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Iteration {i:4d}: Loss = {loss_val:.6f}, Velocity = {c_param:.6f}\")\n",
    "\n",
    "    return c_param, losses\n",
    "\n",
    "# Run optimization\n",
    "print(\"Starting constant velocity optimization...\")\n",
    "c_optimized, losses = optax_adam_constant(c_initial, 1000)\n",
    "\n",
    "print(f\"\\nOptimization Results:\")\n",
    "print(f\"Initial velocity: {c_initial:.6f}\")\n",
    "print(f\"Target velocity:  {ctarget:.6f}\")\n",
    "print(f\"Final velocity:   {c_optimized:.6f}\")\n",
    "print(f\"Error:           {abs(c_optimized - ctarget):.6f}\")\n",
    "print(f\"Loss reduction:   {losses[0]/losses[-1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKnPC2gr1VYk"
   },
   "source": [
    "## Example 2: Linear profile\n",
    "\n",
    "![FWI gif](https://github.com/chishiki-ai/sciml/blob/main/docs/03-diffsim/figs/fwi.gif?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5yXDTJR1VYk",
    "outputId": "786baf87-2e8b-4457-b772-cf6c676493cd"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data for linear velocity profile\n",
    "ctarget_linear = jnp.linspace(0.9, 1.0, n)  # Target: linear increase\n",
    "target_linear = wave_propagation(ctarget_linear)\n",
    "\n",
    "# Initial guess: different linear profile\n",
    "c_initial_linear = jnp.linspace(0.85, 1.0, n)\n",
    "\n",
    "# Define loss function for linear velocity profile\n",
    "@jit\n",
    "def compute_loss_linear(c_array):\n",
    "    \"\"\"Loss function for spatially-varying velocity model\"\"\"\n",
    "    u2 = wave_propagation(c_array)\n",
    "    return jnp.linalg.norm(u2 - target_linear)\n",
    "\n",
    "# Gradient function\n",
    "grad_loss_linear = jit(grad(compute_loss_linear))\n",
    "\n",
    "# Visualize setup\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x0, ctarget_linear, 'r-', linewidth=3, label='Target velocity')\n",
    "plt.plot(x0, c_initial_linear, 'b--', linewidth=2, label='Initial guess')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Velocity')\n",
    "plt.title('Linear Velocity Profiles')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "target_init_linear = wave_propagation(c_initial_linear)\n",
    "plt.plot(x0, target_linear, 'r-', linewidth=2, label='Target wavefield')\n",
    "plt.plot(x0, target_init_linear, 'b--', linewidth=2, label='Initial wavefield')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Wavefield Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target velocity range: [{jnp.min(ctarget_linear):.3f}, {jnp.max(ctarget_linear):.3f}]\")\n",
    "print(f\"Initial guess range:    [{jnp.min(c_initial_linear):.3f}, {jnp.max(c_initial_linear):.3f}]\")\n",
    "print(f\"Number of parameters to optimize: {len(c_initial_linear)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQKD6Wu11VYk",
    "outputId": "0e95132f-ca23-470e-da1a-be6e1c7a4152"
   },
   "outputs": [],
   "source": [
    "# Test initial loss and gradient to verify everything is working\n",
    "initial_loss_linear = compute_loss_linear(c_initial_linear)\n",
    "initial_grad_linear = grad_loss_linear(c_initial_linear)\n",
    "\n",
    "print(f\"Initial loss: {initial_loss_linear:.6f}\")\n",
    "print(f\"Gradient shape: {initial_grad_linear.shape}\")\n",
    "print(f\"Gradient norm: {jnp.linalg.norm(initial_grad_linear):.6f}\")\n",
    "print(f\"Gradient range: [{jnp.min(initial_grad_linear):.6f}, {jnp.max(initial_grad_linear):.6f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLxiJiul1VYk",
    "outputId": "ba03b759-e0fc-4319-d4db-80163653e6d8"
   },
   "outputs": [],
   "source": [
    "# Optimizers for linear profile\n",
    "def optax_adam_linear(params, niter):\n",
    "    \"\"\"Optimize spatially-varying velocity using Adam optimizer\"\"\"\n",
    "    start_learning_rate = 1e-3\n",
    "    optimizer = optax.adam(start_learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    for i in range(niter):\n",
    "        grads = grad_loss_linear(params)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            loss_val = compute_loss_linear(params)\n",
    "            velocity_rmse = jnp.sqrt(jnp.mean((params - ctarget_linear)**2))\n",
    "            print(f\"Iteration {i:4d}: Loss = {loss_val:.6f}, Velocity RMSE = {velocity_rmse:.6f}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "def tfp_lbfgs_linear(params):\n",
    "    \"\"\"Optimize using L-BFGS\"\"\"\n",
    "    # For TFP L-BFGS, we need a function that returns both value and gradients\n",
    "    from jax import value_and_grad\n",
    "\n",
    "    value_and_grad_fn = jit(value_and_grad(compute_loss_linear))\n",
    "\n",
    "    results = tfp.optimizer.lbfgs_minimize(\n",
    "        value_and_grad_fn,\n",
    "        initial_position=params,\n",
    "        tolerance=1e-5\n",
    "    )\n",
    "    return results.position\n",
    "\n",
    "# Initial and Target\n",
    "c_initial_linear = jnp.linspace(0.85, 1.0, n)\n",
    "ctarget_linear = jnp.linspace(0.9, 1.0, n)\n",
    "\n",
    "# Run Adam optimization\n",
    "print(\"Starting Adam optimization...\")\n",
    "result_adam = optax_adam_linear(c_initial_linear, 1000)\n",
    "\n",
    "# Run L-BFGS optimization\n",
    "print(\"\\nStarting L-BFGS optimization...\")\n",
    "result_lbfgs = tfp_lbfgs_linear(c_initial_linear)\n",
    "\n",
    "# Compare results\n",
    "adam_rmse = jnp.sqrt(jnp.mean((result_adam - ctarget_linear)**2))\n",
    "lbfgs_rmse = jnp.sqrt(jnp.mean((result_lbfgs - ctarget_linear)**2))\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Adam RMSE:   {adam_rmse:.8f}\")\n",
    "print(f\"L-BFGS RMSE: {lbfgs_rmse:.8f}\")\n",
    "print(f\"L-BFGS improvement: {adam_rmse/lbfgs_rmse:.1f}x better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jy1jxK0o1VYl",
    "outputId": "76775780-374e-4824-9c3a-99f6658acf80"
   },
   "outputs": [],
   "source": [
    "# Define loss function for linear velocity profile\n",
    "@jit\n",
    "def compute_loss_linear(c_array):\n",
    "    \"\"\"Loss function for spatially-varying velocity model\"\"\"\n",
    "    u2 = wave_propagation(c_array)\n",
    "    return jnp.linalg.norm(u2 - target_linear)\n",
    "\n",
    "# Gradient function\n",
    "grad_loss_linear = jit(grad(compute_loss_linear))\n",
    "\n",
    "# Test initial loss and gradient\n",
    "initial_loss_linear = compute_loss_linear(c_initial_linear)\n",
    "initial_grad_linear = grad_loss_linear(c_initial_linear)\n",
    "\n",
    "print(f\"Initial loss: {initial_loss_linear:.6f}\")\n",
    "print(f\"Gradient shape: {initial_grad_linear.shape}\")\n",
    "print(f\"Gradient norm: {jnp.linalg.norm(initial_grad_linear):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrOnzE9O1VYm",
    "outputId": "8e8011a0-02ac-4bc9-df8f-99e7a72eea4d"
   },
   "outputs": [],
   "source": [
    "# Optimizers for linear profile\n",
    "def optax_adam_linear(params, niter):\n",
    "    \"\"\"Optimize spatially-varying velocity using Adam optimizer\"\"\"\n",
    "    start_learning_rate = 1e-3\n",
    "    optimizer = optax.adam(start_learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    for i in range(niter):\n",
    "        grads = grad_loss_linear(params)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            loss_val = compute_loss_linear(params)\n",
    "            velocity_rmse = jnp.sqrt(jnp.mean((params - ctarget_linear)**2))\n",
    "            print(f\"Iteration {i:4d}: Loss = {loss_val:.6f}, Velocity RMSE = {velocity_rmse:.6f}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "def tfp_lbfgs_linear(params):\n",
    "    \"\"\"Optimize using L-BFGS\"\"\"\n",
    "    # For TFP L-BFGS, we need a function that returns both value and gradients\n",
    "    from jax import value_and_grad\n",
    "\n",
    "    value_and_grad_fn = jit(value_and_grad(compute_loss_linear))\n",
    "\n",
    "    results = tfp.optimizer.lbfgs_minimize(\n",
    "        value_and_grad_fn,\n",
    "        initial_position=params,\n",
    "        tolerance=1e-5\n",
    "    )\n",
    "    return results.position\n",
    "\n",
    "# Run Adam optimization\n",
    "print(\"Starting Adam optimization...\")\n",
    "result_adam = optax_adam_linear(c_initial_linear, 1000)\n",
    "\n",
    "# Run L-BFGS optimization\n",
    "print(\"\\nStarting L-BFGS optimization...\")\n",
    "result_lbfgs = tfp_lbfgs_linear(c_initial_linear)\n",
    "\n",
    "# Compare results\n",
    "adam_rmse = jnp.sqrt(jnp.mean((result_adam - ctarget_linear)**2))\n",
    "lbfgs_rmse = jnp.sqrt(jnp.mean((result_lbfgs - ctarget_linear)**2))\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Adam RMSE:   {adam_rmse:.8f}\")\n",
    "print(f\"L-BFGS RMSE: {lbfgs_rmse:.8f}\")\n",
    "print(f\"L-BFGS improvement: {adam_rmse/lbfgs_rmse:.1f}x better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vj4sidT61VYm",
    "outputId": "87b95924-3745-4ba3-ebdb-6edec96208a2"
   },
   "outputs": [],
   "source": [
    "# Visualize linear velocity optimization results\n",
    "wave_adam = wave_propagation(result_adam)\n",
    "wave_lbfgs = wave_propagation(result_lbfgs)\n",
    "wave_initial = wave_propagation(c_initial_linear)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Velocity model comparison\n",
    "axes[0, 0].plot(x0, ctarget_linear, 'k-', linewidth=3, label='Target velocity', alpha=0.8)\n",
    "axes[0, 0].plot(x0, c_initial_linear, 'gray', linestyle=':', linewidth=2, label='Initial guess')\n",
    "axes[0, 0].plot(x0, result_adam, 'b-', linewidth=2, label='Adam result')\n",
    "axes[0, 0].plot(x0, result_lbfgs, 'r--', linewidth=2, label='L-BFGS result')\n",
    "axes[0, 0].set_xlabel('Position')\n",
    "axes[0, 0].set_ylabel('Velocity')\n",
    "axes[0, 0].set_title('Velocity Model Recovery')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Wavefield comparison\n",
    "axes[0, 1].plot(x0, target_linear, 'k-', linewidth=3, label='Target', alpha=0.8)\n",
    "axes[0, 1].plot(x0, wave_initial, 'gray', linestyle=':', linewidth=2, label='Initial')\n",
    "axes[0, 1].plot(x0, wave_adam, 'b-', linewidth=2, label='Adam prediction')\n",
    "axes[0, 1].plot(x0, wave_lbfgs, 'r--', linewidth=2, label='L-BFGS prediction')\n",
    "axes[0, 1].set_xlabel('Position')\n",
    "axes[0, 1].set_ylabel('Amplitude')\n",
    "axes[0, 1].set_title('Wavefield Predictions')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Error comparison\n",
    "adam_error = jnp.abs(result_adam - ctarget_linear)\n",
    "lbfgs_error = jnp.abs(result_lbfgs - ctarget_linear)\n",
    "\n",
    "axes[1, 0].plot(x0, adam_error, 'b-', linewidth=2, label=f'Adam (RMSE: {adam_rmse:.6f})')\n",
    "axes[1, 0].plot(x0, lbfgs_error, 'r--', linewidth=2, label=f'L-BFGS (RMSE: {lbfgs_rmse:.6f})')\n",
    "axes[1, 0].set_xlabel('Position')\n",
    "axes[1, 0].set_ylabel('|Error|')\n",
    "axes[1, 0].set_title('Velocity Estimation Errors')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Summary statistics\n",
    "summary_text = f'''Algorithm Comparison:\n",
    "\n",
    "Adam Optimizer:\n",
    "• RMSE: {adam_rmse:.8f}\n",
    "• Max error: {jnp.max(adam_error):.8f}\n",
    "• Mean error: {jnp.mean(adam_error):.8f}\n",
    "\n",
    "L-BFGS Optimizer:\n",
    "• RMSE: {lbfgs_rmse:.8f}\n",
    "• Max error: {jnp.max(lbfgs_error):.8f}\n",
    "• Mean error: {jnp.mean(lbfgs_error):.8f}\n",
    "\n",
    "L-BFGS is {adam_rmse/lbfgs_rmse:.1f}× more accurate'''\n",
    "\n",
    "axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,\n",
    "               fontsize=10, ha='left', va='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "axes[1, 1].set_title('Optimization Comparison')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF6IUFxB1VYn"
   },
   "source": [
    "## Advanced Optimization: Newton's Method and BFGS\n",
    "\n",
    "### Newton's Method for Optimization\n",
    "\n",
    "Newton's method finds minima by using second-order information:\n",
    "$$x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k)$$\n",
    "\n",
    "Where:\n",
    "- $x_k$ is the current estimate\n",
    "- $\\nabla^2 f(x_k)$ is the Hessian matrix (second derivatives)\n",
    "- $\\nabla f(x_k)$ is the gradient vector\n",
    "\n",
    "**Challenge**: Computing and inverting the Hessian is expensive for large problems.\n",
    "\n",
    "### BFGS (Broyden-Fletcher-Goldfarb-Shanno)\n",
    "\n",
    "BFGS approximates the inverse Hessian iteratively:\n",
    "$$H_{k+1} = (I - \\rho_k s_k y_k^T) H_k (I - \\rho_k y_k s_k^T) + \\rho_k s_k s_k^T$$\n",
    "\n",
    "Where:\n",
    "- $s_k = x_{k+1} - x_k$ (step difference)\n",
    "- $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ (gradient difference)\n",
    "- $\\rho_k = \\frac{1}{y_k^T s_k}$\n",
    "\n",
    "**L-BFGS** (Limited-memory BFGS) stores only recent history, making it suitable for large problems.\n",
    "\n",
    "### Adam vs L-BFGS Comparison\n",
    "\n",
    "| Aspect | Adam | L-BFGS |\n",
    "|--------|------|--------|\n",
    "| **Type** | First-order (gradients only) | Quasi-Newton (approximate second-order) |\n",
    "| **Memory** | O(1) per parameter | O(m) history vectors |\n",
    "| **Convergence** | Robust, good for noisy functions | Superlinear for smooth functions |\n",
    "| **Best for** | Large-scale, noisy problems | Smooth, deterministic functions |\n",
    "\n",
    "**Our Results**: L-BFGS achieved significantly better accuracy for the smooth wave equation optimization, demonstrating the power of second-order methods for well-conditioned problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6jKSjzA1VYo"
   },
   "source": [
    "## The Power of Differentiable Simulation\n",
    "\n",
    "**What we've demonstrated**:\n",
    "\n",
    "1. **Physics-Based Model**: Realistic wave equation solver with finite differences\n",
    "2. **Automatic Differentiation**: JAX computed exact gradients through entire simulation\n",
    "3. **Scalable Optimization**: From 1 parameter (constant) to 1000 parameters (linear profile)\n",
    "4. **Algorithm Comparison**: Adam vs L-BFGS trade-offs in practice\n",
    "\n",
    "**Key Results**:\n",
    "- **Constant velocity**: Perfect recovery with gradient descent\n",
    "- **Linear profile**: High-fidelity reconstruction of spatially-varying parameters\n",
    "- **L-BFGS advantage**: Superior convergence for smooth optimization landscapes\n",
    "\n",
    "**The Revolution**: Physics simulations are now **learnable components** that can be optimized end-to-end with gradient descent, enabling inverse problems that were previously intractable.\n",
    "\n",
    "### Applications\n",
    "\n",
    "**Geophysics**: Subsurface imaging, earthquake location, Earth structure\n",
    "**Medical imaging**: Ultrasound tomography, photoacoustic imaging\n",
    "**Materials science**: Non-destructive testing, property characterization\n",
    "**Engineering**: Structural health monitoring, design optimization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "\n",
    "\n",
    "- Explore the interactive projectile control demo: [![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](https://cvw.cac.cornell.edu/SciML/diffsim/demo-projectile-control)\n",
    "- Explore the interactive JAX roll demo: [![Button](https://img.shields.io/badge/Go%20to-Interactive%20Demo-blue?style=for-the-badge&logo=airplayvideo&logoColor=white)](https://cvw.cac.cornell.edu/SciML/diffsim/demo-jax-roll)\n",
    "- Try different velocity models (step functions, Gaussian anomalies)\n",
    "- Experiment with other PDEs (heat, elasticity, Maxwell)\n",
    "- Implement multi-objective optimization with regularization\n",
    "\n",
    "**The Future**: Differentiable simulation bridges physics and machine learning, enabling scientific discovery through optimization."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
