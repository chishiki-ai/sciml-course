{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGKCXbWIVjT6"
   },
   "source": [
    "# Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuTNLdaA1WI6"
   },
   "source": [
    "\n",
    "**Slides:** [![View Slides](https://img.shields.io/badge/View-Presentation-yellow?style=flat-square&logo=googleslides&logoColor=white)](https://docs.google.com/presentation/d/1hDcMFqbOXPehlxjaCBAPCxdmiMJSzLj5FYeuXAo6Dg4/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2-6pWyq1WI8"
   },
   "source": [
    "## Graph Neural Networks (GNNs) and message passing\n",
    "\n",
    "Although traditional physics simulators are powerful, there are some important drawbacks with them: (1) it is expensive and time-consuming to get high-quality results of large-scale simulations for traditional physical simulators; (2) to set up physical simulators, we need to have full knowledge of the physics parameters of the object and the environment, which are extremely hard to know in some cases.\n",
    "\n",
    "Graph Neural Network (GNN) is a special subset of neural networks that take less structured data, such as a graph, as input, while other neural networks like Convolutional Neural Network (CNN) and Transformer, can only accept more structured data (e.g., grid and sequence). By “less structured”, it means that the input can have arbitrary shapes and sizes and can have complex topological relations.\n",
    "\n",
    "In particle-based physics simulation, we have the unstructured position information of all the particles as the input, which inspires the idea of using a GNN.\n",
    "\n",
    "*Permutation equivariance*\n",
    "\n",
    "One key characteristic of GNN which distinguishes it from other neural networks is permutation equivalence. That is to say, the nodes in a graph do not have a canonical order, so how we “order” the nodes in a graph does not impact the results produced by GNNs.\n",
    "\n",
    "Since particles of an object are “identical” in the particle-based simulation, they are permutation-equivariant when applying physics laws on them. Therefore, a permutation-equivariant model such as a GNN is suitable to simulate the interactions between particles.\n",
    "\n",
    "### Graphs\n",
    "Graphs are powerful means of representing interactions between physical systems.\n",
    "A granular material media can be represented as a graph $ G=\\left(V,E\\right) $ consisting of a set of vertices ($\\mathbf{v}_i \\in\\ V$) representing the soil grains and edges ($\\mathbf{e}_{i,j} \\in\\ E$) connecting a pair of vertices ($\\mathbf{v}_i$ and $\\mathbf{v}_j$) representing the interaction relationship between grains.\n",
    "We describe how graphs work by showing a simple example involving interaction between balls in a box (Figure 1a).\n",
    "The state of the physical system (Figure 1a and 1d) can be encoded as a graph (Figure 1b and 1c). The vertices describe the balls, and the edges describe the directional interaction between them, shown as arrows in Figure 1b and 1c.\n",
    "The state of the ball i is represented as a vertex feature vector $\\mathbf{v}_i$ at $i$. The feature vector includes properties such as velocities, mass, and distance to the boundary.\n",
    "The edge feature vector $\\mathbf{e}_{i,j}$ includes the information about the interaction between balls $i$ and $j$ such as the relative distance between the balls.\n",
    "\n",
    "Graphs offer a permutation invariant form of encoding data, where the interaction between vertices is independent of the order of vertices or their position in Euclidean space.\n",
    "Rather,  graphs represent the interactions through the edge connection, not affected by the permutation of the vertices.\n",
    "Therefore, graphs can efficiently represent the physical state of granular flow where numerous orderless particles interact by using vertices to represent particles and edges to their interaction.\n",
    "\n",
    "### Graph neural networks (GNNs)\n",
    "GNNs are a state-of-the-art deep learning architecture that can operate on a graph and learn the local interactions.\n",
    "GNNs take a graph $G=\\left(\\mathbf{V},\\mathbf{E}\\right)$ at time t as an input, compute properties and propagate information through the network, termed as message passing, and output an updated graph $G^\\prime=\\left(\\mathbf{V}^\\prime,\\mathbf{E}^\\prime\\right)$ with an identical structure, where $\\mathbf{V}^\\prime$ and $\\mathbf{E}^\\prime$ are the set of updated vertex and edge features ($\\mathbf{v}_i^\\prime$ and $\\mathbf{e}_{i,\\ j}^\\prime$).\n",
    "In the balls-in-a-box example, the GNN first takes the original graph $G=\\left(\\mathbf{V},\\mathbf{E}\\right)$  (Figure 1b) that describes the current state of the physical system ($\\mathbf{X}^t$).\n",
    "The GNN then updates the state of the physical system through message passing, which models the exchange of energy and momentum between the balls communicating through the edges, and returns an updated graph $G^\\prime=\\left(\\mathbf{V}^\\prime,\\mathbf{E}^\\prime\\right)$ (Figure 1c).\n",
    "After the GNN computation, we may decode G^\\prime to extract useful information related to the future state of the physical system ($\\mathbf{X}^{t+1}$) such as the next position or acceleration of the balls (Figure 1d).\n",
    "\n",
    "![balls-in-a-box](https://github.com/chishiki-ai/sciml/blob/main/docs/04-gns/figs/balls-in-a-box.svg?raw=1)\n",
    "*Figure. 1. An example of a graph and graph neural network (GNN) that process the graph (modified from Battaglia et al. (2018)):\n",
    "(a) A state of the current physical system ($\\mathbf{X}^t$) where the balls are bouncing in a box boundary;\n",
    "(b) Graph representation of the physical system ($G$).\n",
    "There are three vertices representing balls and six edges representing their directional interaction shown as arrows;\n",
    "(c) The updated graph ($G^\\prime$) that GNN outputs through message passing; (d) The predicted future state of the physical system ($\\mathbf{X}^{t+1}$) (i.e., the positions of the balls at the next timestep) decoded from the updated graph.*\n",
    "\n",
    "### Message passing\n",
    "Message passing consists of three operations: message construction (Eq. 1), message aggregation (Eq. 2), and the vertex update function (Eq. 3).\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{e}_{i,j}^\\prime=\\phi_{\\mathbf{\\Theta}_\\phi}\\left(\\mathbf{v}_i,\\mathbf{v}_j,\\mathbf{e}_{i,\\ j}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    {\\bar{\\mathbf{v}}}_i=\\Sigma_{j \\in N\\left(i\\right)}\\ \\mathbf{e}_{i,j}^\\prime\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{v}_i^\\prime=\\gamma_{\\mathbf{\\Theta}_\\gamma}\\left(\\mathbf{v}_i,{\\bar{\\mathbf{v}}}_i\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The subscript $\\mathbf{\\Theta}_\\phi$ and $\\mathbf{\\Theta}_\\gamma$ represent a set of learnable parameters in each computation.\n",
    "The message construction function $\\phi_{\\Theta_{\\phi}}$ (Eq. 1) takes the feature vector of the receiver and sender vertices ($\\mathbf{v}_i$ and $\\mathbf{v}_j$) and the feature vector of the edge connecting them ($\\mathbf{e}_{i,\\ j}$) and returns an updated edge feature vector $\\mathbf{e}_{i,j}^\\prime$ as the output.\n",
    "$\\phi_{\\Theta_{\\phi}}$ is a matrix operation including the learnable parameter $\\mathbf{\\Theta}_\\phi$.\n",
    "The updated edge feature vector $\\mathbf{e}_{i,j}^\\prime$ is the message sent from vertex $j$ to $i$.\n",
    "Figure 2a shows an example of constructing messages on edges directing to vertex 0 originating from vertices 1, 2, and 3 ($\\mathbf{e}_{0,1}^\\prime, \\mathbf{e}_{0,2}^\\prime, \\mathbf{e}_{0,3}^\\prime$).\n",
    "Here, we define the message construction function $\\phi_{\\Theta_{\\phi}}$ as $\\left(\\left(\\mathbf{v}_i+\\mathbf{v}_j\\right)\\times\\mathbf{e}_{i,j}\\right)\\times\\mathbf{\\Theta}_\\phi$.\n",
    "The updated feature vector $\\mathbf{e}_{0,\\ 1}^\\prime$ is computed as $\\left(\\left(\\mathbf{v}_0+\\mathbf{v}_1\\right)\\times\\mathbf{e}_{0,1}\\right)\\times\\mathbf{\\Theta}_\\phi$, where $\\mathbf{v}_0$ and $\\mathbf{v}_1$ are the receiver and sender vertex feature vectors, and $\\mathbf{e}_{0,1}$ is their edge feature vector.\n",
    "If we assume that all values of $\\mathbf{\\Theta}_\\phi$ are 1.0 for simplicity, we obtain $\\mathbf{e}_{0,\\ 1}^\\prime=(\\left(\\left[1,\\ 0,\\ 2\\right]\\right)+\\left[1,\\ 3,\\ 2\\right])\\times\\left[2,\\ 1,\\ 0\\right]^T)\\times1=[4,\\ 3,\\ 0]$.\n",
    "Similarly, we compute the messages $\\mathbf{e}_{0,\\ 2}^\\prime=\\left[0,\\ 3,\\ 9\\right]$ and $\\mathbf{e}_{0,\\ 3}^\\prime=\\left[3,\\ 4,\\ 9\\right]$.\n",
    "\n",
    "The next step in message passing is the message aggregation $\\Sigma_{j \\in N\\left(i\\right)}$ (Eq. 2), where $N\\left(i\\right)$ is the set of sender vertices j related to vertex $i$.\n",
    "It collects all the messages directing to vertex $i$ and aggregates those into a single vector with the same dimension as the aggregated message (${\\bar{\\mathbf{v}}}_i$).\n",
    "The aggregation rule can be element-wise vector summation or averaging, hence it is a permutation invariant computation.\n",
    "In Figure 2a, the aggregated message $\\bar{\\mathbf{v}_0}=\\left[7,10,18\\right]$ is the element-wise summation of the messages directing to vertex 0 as $\\bar{\\mathbf{v}_o}=\\mathbf{e}_{0,\\ 1}^\\prime+\\ \\mathbf{e}_{0,\\ 2}^\\prime+\\ \\mathbf{e}_{0,\\ 3}^\\prime$.\n",
    "\n",
    "The final step of the message passing is updating vertex features using Eq. 3.\n",
    "It takes the aggregated message (${\\bar{\\mathbf{v}}}_i$) and the current vertex feature vector $\\mathbf{v}_i$, and returns an updated vertex feature vector $\\mathbf{v}_i^\\prime$, using predefined vector operations including the learnable parameter $\\mathbf{\\Theta}_\\gamma$. Figure 2b shows an example of the update at vertex 0.\n",
    "Here, we define the update function $\\gamma_{\\Theta_{\\gamma}}$ as $\\mathbf{\\Theta}_\\gamma\\left(\\mathbf{v}_i+{\\bar{\\mathbf{v}}}_i\\right)$.\n",
    "The updated feature vector $\\mathbf{v}_0^\\prime$ is computed as $\\mathbf{\\Theta}_\\gamma\\left(\\mathbf{v}_0+{\\bar{\\mathbf{v}}}_\\mathbf{0}\\right)$.\n",
    "Assuming all parameters in $\\mathbf{\\Theta}_\\gamma$ are 1.0 for simpliticy, we obtain $\\mathbf{v}_0^\\prime=\\left[1,\\ 0,\\ 2\\right]+\\left[7,\\ 10,\\ 18\\right]=\\left[8,10,20\\right]$. Similarly, we update the other vertex features $(\\mathbf{v}_1^\\prime, \\mathbf{v}_2^\\prime, \\mathbf{v}_3^\\prime)$.\n",
    "\n",
    "At the end of the message passing, the graph vertex and edge features ($\\mathbf{v}_i$ and $\\mathbf{e}_{i,\\ j}$) are updated to $\\mathbf{v}_i^\\prime$ and $\\mathbf{e}_{i,\\ j}^\\prime$.\n",
    "The GNN may include multiple message passing steps to propagate the information further through the network.\n",
    "\n",
    "![message_construction](https://github.com/chishiki-ai/sciml/blob/main/docs/04-gns/figs/message_construction.svg?raw=1)\n",
    "(a)\n",
    "![update](https://github.com/chishiki-ai/sciml/blob/main/docs/04-gns/figs/message_aggregate.svg?raw=1)\n",
    "(b)\n",
    "\n",
    "*Figure 2. An example of message passing on a graph:\n",
    "(a) message construction directing to receiver vertex 0 $(\\mathbf{e}_{0,\\ 1}^\\prime, \\mathbf{e}_{0,\\ 2}^\\prime, \\mathbf{e}_{0,\\ 3}^\\prime)$ and the resultant aggregated message $({\\bar{\\mathbf{v}}}_0)$;\n",
    "(b) feature update at vertex 0 using ${\\bar{\\mathbf{v}}}_0$. Note that we assume $\\mathbf{\\Theta}_\\phi$ and $\\mathbf{\\Theta}_r$ are 1.0 for the convenience of calculation.*\n",
    "\n",
    "Unlike the example shown above, where we assume a constant value of 1.0 for the learnable parameters, in a supervised learning environment, the optimization algorithm will find a set of the best learnable parameters ($\\mathbf{\\Theta}_\\phi, \\mathbf{\\Theta}_\\gamma$) in the message passing operation.\n",
    "\n",
    "## Graph Neural Network-based Simulator (GNS)\n",
    "\n",
    "In this study, we use GNN as a surrogate simulator to model granular flow behavior.\n",
    "Figure 3 shows an overview of the general concepts and structure of the GNN-based simulator (GNS).\n",
    "Consider a granular flow domain represented as particles (Figure 3a).\n",
    "In GNS, we represent the physical state of the granular domain at time t with a set of $\\mathbf{x}_i^t$ describing the state and properties of each particle.\n",
    "The GNS takes the current state of the granular flow $\\mathbf{x}_t^i \\in \\mathbf{X}_t$ and predicts its next state ${\\mathbf{x}_{i+1}^i \\in\\ bm{X}}_{t+1}$ (Figure 3a).\n",
    "The GNS consists of two components: a parameterized function approximator $\\ d_\\mathbf{\\Theta}$ and an updater function (Figure 3b).\n",
    "The approximator $d_\\theta$ take takes $\\mathbf{X}_t$ as an input and outputs dynamics information ${\\mathbf{y}_i^t \\in \\mathbf{Y}}_t$.\n",
    "The updater then computes $\\mathbf{X}_{t+1}$ using $\\mathbf{Y}_t$ and $\\mathbf{X}_t$.\n",
    "Figure 3c shows the details of $d_\\theta$ which consists of an encoder, a processor, and a decoder.\n",
    "The encoder (Figure 3c-1) takes the state of the system $\\mathbf{X}^t$ and embed it into a latent graph $G_0=\\left(\\mathbf{V}_0,\\ \\mathbf{E}_0\\right)$ to represent the relationship between particles, where the vertices $\\mathbf{v}_i^t \\in \\mathbf{V}_0$ contain latent information of the current particle state, and the edges $\\mathbf{e}_{i,j}^t \\in \\mathbf{E}_0$ contain latent information of the pair-wise relationship between particles.\n",
    "Next, the processer (Figure 3c-2) converts $G_0$ to $G_M$ with $M$ stacks of message passing GNN ($G_0\\rightarrow\\ G_1\\rightarrow\\cdots\\rightarrow\\ G_M$)  to compute the interaction between particles.\n",
    "Finally, the decoder (Figure 3c-3) extracts dynamics of the particles ($\\mathbf{Y}^t$) from $G_M$, such as the acceleration of the physical system.\n",
    "The entire simulation (Figure 3a) involves running GNS surrogate model through $K$ timesteps predicting from the initial state $\\mathbf{X}_0$ to $\\mathbf{X}_K$ $(\\mathbf{X}_0,\\ \\ \\mathbf{X}_1,\\ \\ \\ldots,\\ \\ \\mathbf{X}_K$), updating at each step ($\\mathbf{X}_t\\rightarrow\\mathbf{X}_{t+1}$)\n",
    "\n",
    "![GNS](https://github.com/chishiki-ai/sciml/blob/main/docs/04-gns/figs/gns_structure.svg?raw=1)\n",
    "*Figure 3. The structure of the graph neural network (GNN)-based physics simulator (GNS) for granular flow (modified from Sanchez-Gonzalez et al. (2020)):\n",
    "(a) The entire simulation procedure using the GNS,\n",
    "(b) The computation procedure of GNS and its composition, (c) The computation procedure of the parameterized function approximator $d_\\theta$ and its composition.*\n",
    "\n",
    "### Input\n",
    "The input to the GNS, $\\mathbf{x}_i^t \\in \\mathbf{X}^t$, is a vector consisting of the current particle position $\\mathbf{p}_i^t$, the particle velocity context ${\\dot{\\mathbf{p}}}_i^{\\le t}$, information on boundaries $\\mathbf{b}_i^t$, and particle type embedding ${\\mathbf{f}}$ (Eq. 4).\n",
    "$\\mathbf{x}_i^t$ will be used to construct vertex feature ($\\mathbf{v}_i^t$) (Eq. 6).\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}_i^t=\\left[\\mathbf{p}_i^t,{\\dot{\\mathbf{p}}}_i^{\\le t},\\mathbf{b}_i^t,\\mathbf{f}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The velocity context ${\\dot{\\mathbf{p}}}_i^{\\le t}$ includes the current and previous particle velocities for n timesteps $\\left[{\\dot{\\mathbf{p}}}_i^{t-n},\\cdots,\\ {\\dot{\\mathbf{p}}}_i^t\\right]$.\n",
    "We use $n$=4 to include sufficient velocity context in the vertex feature $\\mathbf{x}_i^t$.\n",
    "Sanchez-Gonzalez et al. (2020) show that having $n$>1 significantly improves the model performance.\n",
    "The velocities are computed using the finite difference of the position sequence (i.e.,  ${\\dot{\\mathbf{p}}}_i^t=\\left(\\mathbf{p}_i^t-\\mathbf{p}_i^{t-1}\\right)/\\Delta t$).\n",
    "For a 2D problem, $\\mathbf{b}_i^t$ has four components each of which indicates the distance between particles and the four walls.\n",
    "We normalize $\\mathbf{b}_i^t$ by the connectivity radius, which is explained in the next section, and restrict it between 1.0 to 1.0. $\\mathbf{b}_i^t$ is used to evaluate boundary interaction for a particle.\n",
    "${\\mathbf{f}}$ is a vector embedding describing a particle type.\n",
    "\n",
    "In addition to $\\mathbf{x}_i^t$, we define the interaction relationship between particles $i$ and $j$ as $\\mathbf{r}_{i,\\ j}^t$ using the distance and displacement of the particles in the current timestep (see Eq. 5).\n",
    "The former reflects the level of interaction, and the latter reflects its spatial direction.\n",
    "$\\mathbf{r}_{i,\\ j}^t$ will be used to construct edge features ($\\mathbf{e}_{i,j}^t$).\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{r}_{i,j}^t=\\left[(\\mathbf{p}_i^t-\\mathbf{p}_j^t),||\\mathbf{p}_i^t-\\mathbf{p}_j^t||\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### Encoder\n",
    "The vertex and edge encoders ($\\varepsilon_\\Theta^v$ and $\\varepsilon_\\Theta^e$) convert $\\mathbf{x}_i^t$ and $\\mathbf{r}_{i,\\ j}^t$ into the vertex and edge feature vectors ($\\mathbf{v}_i^t$ and $\\mathbf{e}_{i,j}^t$) (Eq. 6) and embed them into a latent graph $G_0=\\left(\\mathbf{V}_0, \\mathbf{E}_0\\right)$,  $\\mathbf{v}_i^t \\in \\mathbf{V}_0$, $\\mathbf{e}_{i,j}^t \\in \\mathbf{E}_0$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{v}_i^t=\\varepsilon_\\Theta^v\\left(\\mathbf{x}_i^t\\right),\\ \\ \\mathbf{e}_{r,s}^t=\\varepsilon_\\Theta^e\\left(\\mathbf{r}_{r,s}^t\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We use a two-layered 128-dimensional multi-layer perceptron (MLP) for the $\\varepsilon_\\Theta^v$ and $\\varepsilon_\\Theta^e$.\n",
    "The MLP and optimization algorithm search for the best candidate for the parameter set $\\Theta$ that estimates a proper way of representing the physical state of the particles and their relationship which will be embedded into $G_0$.\n",
    "\n",
    "The edge encoder $\\varepsilon_\\Theta^v$ uses $\\mathbf{x}_i^t$ (Eq. 4) without the current position of the particle ($\\mathbf{p}_i^t$), but still with its velocities (${\\dot{\\mathbf{p}}}_i^{\\le t}$), since velocity governs the momentum, and the interaction dynamics is independent of the absolute position of the particles.\n",
    "Rubanova et al. (2022) confirmed that including position causes poorer model performance.\n",
    "We only use $\\mathbf{p}_i^t$ to predict the next position $\\mathbf{p}_i^{t+1}$ based on the predicted velocity ${\\dot{\\mathbf{p}}}_i^{t+1}$ (Eq. 9).\n",
    "\n",
    "We consider the interaction between two particles by constructing the edges between them only if vertices are located within a certain distance called connectivity radius $R$ (see the shaded circular area in Figure 3b).\n",
    "The connectivity radius is a critical hyperparameter that governs how effectively the model learns the local interaction.\n",
    "$R$ should be sufficiently large to include the local interaction as edges between particles but also to capture the global dynamics of the simulation domain.\n",
    "\n",
    "### Processor\n",
    "The processor performs message passing (based on Eq. 1-3) on the initial latent graph ($G_0$) from the encoder for M times ($G_0\\rightarrow\\ G_1\\rightarrow\\cdots\\rightarrow\\ G_M$) and returns a final updated graph $G_M$.\n",
    "We use two-layered 128-dimensional MLPs for both message construction function $\\phi_{\\mathbf{\\Theta}_\\phi}$ and vertex update function $\\gamma_{\\mathbf{\\Theta}_r}$, and element-wise summation for the message aggregation function $\\mathbf{\\Sigma}_{j \\in N\\left(i\\right)}$ in Eq. 1-3.\n",
    "We set $M$=10 to ensure sufficient message propagation through the network.\n",
    "These stacks of message passing models the propagation of information through the network of particles.\n",
    "\n",
    "### Decoder\n",
    "The decoder $\\delta_\\Theta^v$ extracts the dynamics $\\mathbf{y}_i^t \\in \\mathbf{Y}^t$ of the particles from the vertices $\\mathbf{v}_i^t$ (Eq. 7) using the final graph $G_M$.\n",
    "We use a two-layered 128-dimensional MLP for $\\delta_\\Theta^v$ which learns to extract the relevant particle dynamics from $G_M$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{y}_i^t=\\delta_\\Theta^v\\left(\\mathbf{v}_i^t\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "### Updater\n",
    "We use the dynamics $\\mathbf{y}_i^t$ to predict the velocity and position of the particles at the next timestep (${\\dot{\\mathbf{p}}}_i^{t+1}$ and  $\\mathbf{p}_i^{t+1}$) based on Euler integration (Eq. 8 and Eq. 9), which makes $\\mathbf{y}_i^t$ analogous to acceleration  ${\\ddot{\\mathbf{p}}}_i^t$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\dot{\\mathbf{p}}}_i^{t+1}={\\dot{\\mathbf{p}}}_i^t+\\mathbf{y}_i^t\\Delta t\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{p}_i^{t+1}=\\mathbf{p}_i^t+{\\dot{\\mathbf{p}}}_i^{t+1}\\Delta t\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Based on the new particle position and velocity, we update $\\mathbf{x}_i^t \\in \\mathbf{X}^t$ (Eq. 5) to $\\mathbf{x}_i^{t+1} \\in \\mathbf{X}^{t+1}$. The updated physical state $\\mathbf{X}^{t+1}$ is then used to predict the position and velocity for the next timestep.\n",
    "\n",
    "The updater imposes inductive biases to GNS to improve learning efficiency.\n",
    "GNS does not directly predict the next position from the current position and velocity (i.e., $\\mathbf{p}_i^{t+1}=GNS\\left(\\mathbf{p}_i^t,\\ {\\dot{\\mathbf{p}}}_i^t\\right)$) which has to learn the static motion and inertial motion.\n",
    "Instead, it uses (1) the inertial prior (Eq. 8) where the prediction of next velocity ${\\dot{\\mathbf{p}}}_i^{t+1}$ should be based on the current velocity ${\\dot{\\mathbf{p}}}_i^t$  and (2) the static prior (Eq. 9) where the prediction of the next position $\\mathbf{p}_i^{t+1}$ should be based on the current position $\\mathbf{p}_i^t$.\n",
    "These make GNS to be trivial to learn static and inertial motions that is already certain and focus on learning dynamics which is uncertain.\n",
    "In addition, since the dynamics of particles are not controlled by their absolute position, GNS prediction can be generalizable to other geometric conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NCNMVt71WI9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch_geometric as pyg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wGYlXhv1WI-"
   },
   "source": [
    "GNN layers such as a IN layer can be easily implemented in PyTorch Geometric (PyG). In PyG, a GNN layer is generally implemented as a subclass of the MessagePassing class. We follow this convention and define the InteractionNetwork Class as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJAbDoxp1WI-"
   },
   "outputs": [],
   "source": [
    "class InteractionNetwork(pyg.nn.MessagePassing):\n",
    "   def __init__(self, hidden_size, layers=3):\n",
    "       super().__init__()\n",
    "       self.lin_edge = MLP(hidden_size * 3, hidden_size, layers)\n",
    "       self.lin_node = MLP(hidden_size * 2, hidden_size, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfr-UDuQ1WI-"
   },
   "source": [
    "(1) Construct a message for each edge of the graph. The message is generated by concatenating the features of the edge’s two nodes and the feature of the edge itself, and transforming the concatenated vector with an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V1nbCfo1WI-"
   },
   "outputs": [],
   "source": [
    "def message(self, x_i, x_j, edge_feature):\n",
    "    x = torch.cat((x_i, x_j, edge_feature), dim=-1)\n",
    "    x = self.lin_edge(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83yowEsJ1WI_"
   },
   "source": [
    "(2) Aggregate (sum up) the messages of all the incoming edges for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzbDHSbw1WI_"
   },
   "outputs": [],
   "source": [
    "def aggregate(self, inputs, index):\n",
    "    out = torch_scatter.scatter(inputs, index, dim=self.node_dim, reduce=\"sum\")\n",
    "    return (inputs, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjWW1cGF1WI_"
   },
   "source": [
    "(3) Update node features and edge features. Each edge’s new feature is the sum of its old feature and the message on the edge. Each node’s new feature is determined by its old feature and the aggregation of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7qLq2Kc1WI_"
   },
   "outputs": [],
   "source": [
    "def forward(self, x, edge_index, edge_feature):\n",
    "    edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
    "    node_out = self.lin_node(torch.cat((x, aggr), dim=-1))\n",
    "    edge_out = edge_feature + edge_out\n",
    "    node_out = x + node_out\n",
    "    return node_out, edge_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05viSpWa1WI_"
   },
   "source": [
    "Let’s include the encoder, the processor and the decoder together! Before GNN layers, input features are transformed by MLP so that the expressiveness of GNN is improved without increasing GNN layers. After GNN layers, final outputs (accelerations of particles in our case) are extracted from features generated by GNN layers to meet the requirement of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeBELMxT1WI_"
   },
   "outputs": [],
   "source": [
    "class LearnedSimulator(torch.nn.Module):\n",
    "   \"\"\"Graph Network-based Simulators(GNS)\"\"\"\n",
    "   def __init__(\n",
    "       self,\n",
    "       hidden_size=128,\n",
    "       n_mp_layers=10, # number of GNN layers\n",
    "       node_feature_dim=30,\n",
    "       edge_feature_dim=3,\n",
    "       dim=2, # dimension of the world, typically 2D or 3D\n",
    "   ):\n",
    "       super().__init__()\n",
    "       self.node_in = MLP(node_feature_dim, hidden_size, 3)\n",
    "       self.edge_in = MLP(edge_feature_dim, hidden_size, 3)\n",
    "       self.node_out = MLP(hidden_size, dim, 3)\n",
    "       self.layers = torch.nn.ModuleList([InteractionNetwork(hidden_size, 3) for _ in range(n_mp_layers)])\n",
    "\n",
    "   def forward(self, edge_index, node_feature, edge_feature):\n",
    "       # encoder\n",
    "       node_feature = self.node_in(node_feature)\n",
    "       edge_feature = self.edge_in(edge_feature)\n",
    "       # processor\n",
    "       for layer in self.layers:\n",
    "           node_feature, edge_feature = layer(node_feature, edge_index, edge_feature=edge_feature)\n",
    "       # decoder\n",
    "       out = self.node_out(node_feature)\n",
    "       return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1E_PEIxaHr8"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**Before we get started:**\n",
    "\n",
    "- This notebook includes a concise PyG implementation of the paper ***Learning to Simulate Complex Physics with Graph Networks*. We adapted our code from the open-source tensorflow implementation by DeepMind.\n",
    "    - Link to the pdf of this paper: https://arxiv.org/abs/2002.09405\n",
    "    - Link to Deepmind's implementation: https://github.com/deepmind/deepmind-research/tree/master/learning_to_simulate\n",
    "    - Link to the video site by DeepMind: https://sites.google.com/view/learning-to-simulate\n",
    "- Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell.\n",
    "- Feel free to make a copy to your own drive to play around with it! Have fun with this tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z4HbK-OPUK8"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset WaterDropSmall includes simulations of dropping water to the ground rendered in a particle-based physics simulator. We will download this dataset to the folder `temp/datasets` in the file system. You can inspect the downloaded files on the **Files** menu on the left of this notebook.\n",
    "\n",
    "The `metadata.json` file in the dataset includes the following information:\n",
    "1. The sequence length of each video data point\n",
    "2. The dimensionality, 2d or 3d\n",
    "3. The box bounds, which specify the bounding box for the scene\n",
    "4. The default connectivity radius, which defines the size of each particle's neighborhood\n",
    "5. The statistics for normalization, such as the mean and standard deviation of the velocity and acceleration of particles\n",
    "\n",
    "\n",
    "Each data point in the dataset includes the following information:\n",
    "1. The type of the particles, such as water\n",
    "2. The particle positions at each frame in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxevKO1yPb4d"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"WaterDropSample\"\n",
    "OUTPUT_DIR = \"./WaterDropSample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW0_YsEPG68T"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Since we cannot apply the raw data in the dataset to train the GNN model directly, we need to go through the following steps to convert the raw data into graphs with descriptive node features and edge features:\n",
    "1. Apply noise to the trajectory to have more diverse training examples\n",
    "1. Construct the graph based on the distance between particles\n",
    "1. Extract node-level features: particle velocities and their distance to the boundary\n",
    "1. Extract edge-level features: displacement and distance between particles\n",
    "\n",
    "If you are not interested in the data pipeline, your can skip to the end of this section. There is a detailed explanation and visualization of one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCy3zaaOGrrS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch_geometric as pyg\n",
    "\n",
    "def generate_noise(position_seq, noise_std):\n",
    "    \"\"\"Generate noise for a trajectory\"\"\"\n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "    time_steps = velocity_seq.size(1)\n",
    "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std / time_steps ** 0.5)\n",
    "    velocity_noise = velocity_noise.cumsum(dim=1)\n",
    "    position_noise = velocity_noise.cumsum(dim=1)\n",
    "    position_noise = torch.cat((torch.zeros_like(position_noise)[:, 0:1], position_noise), dim=1)\n",
    "    return position_noise\n",
    "\n",
    "\n",
    "def preprocess(particle_type, position_seq, target_position, metadata, noise_std):\n",
    "    \"\"\"Preprocess a trajectory and construct the graph\"\"\"\n",
    "    # apply noise to the trajectory\n",
    "    position_noise = generate_noise(position_seq, noise_std)\n",
    "    position_seq = position_seq + position_noise\n",
    "\n",
    "    # calculate the velocities of particles\n",
    "    recent_position = position_seq[:, -1]\n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "\n",
    "    # construct the graph based on the distances between particles\n",
    "    n_particle = recent_position.size(0)\n",
    "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"], loop=True, max_num_neighbors=n_particle)\n",
    "\n",
    "    # node-level features: velocity, distance to the boundary\n",
    "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
    "    boundary = torch.tensor(metadata[\"bounds\"])\n",
    "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
    "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
    "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
    "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
    "\n",
    "    # edge-level features: displacement, distance\n",
    "    dim = recent_position.size(-1)\n",
    "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) -\n",
    "                   torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
    "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
    "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
    "\n",
    "    # ground truth for training\n",
    "    if target_position is not None:\n",
    "        last_velocity = velocity_seq[:, -1]\n",
    "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
    "        acceleration = next_velocity - last_velocity\n",
    "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
    "    else:\n",
    "        acceleration = None\n",
    "\n",
    "    # return the graph with features\n",
    "    graph = pyg.data.Data(\n",
    "        x=particle_type,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
    "        y=acceleration,\n",
    "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
    "    )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhp0soer1WJA"
   },
   "source": [
    "## Operation Modes of GNS\n",
    "\n",
    "The GNS works in two modes: one-step mode and rollout mode. In one-step mode, the GNS always makes predictions with ground-truth inputs. In rollout mode, the GNS predicts positions of particles in the next step based on its own predictions in the previous step. As a result, errors accumulate over time for rollout mode.\n",
    "\n",
    "![gns-modes](https://github.com/chishiki-ai/sciml/blob/main/docs/04-gns/figs/gns-modes.webp?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqfx4NcguDEY"
   },
   "source": [
    "### One Step Dataset\n",
    "\n",
    "Each datapoint in this dataset contains trajectories sliced to short time windows. We will use this dataset in the training phase because the history of particles' states are necessary for the model to make predictions. But in the meantime, since long-horizon prediction is usually inaccurate and time-consuming, we sliced the trajectories to short time windows to improve the perfomance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2HrUjPnsF_4"
   },
   "outputs": [],
   "source": [
    "class OneStepDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7, noise_std=0.0, return_pos=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # load dataset from the disk\n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        self.window_length = window_length\n",
    "        self.noise_std = noise_std\n",
    "        self.return_pos = return_pos\n",
    "\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "\n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            break\n",
    "\n",
    "        # cut particle trajectories according to time slices\n",
    "        self.windows = []\n",
    "        for traj in self.offset.values():\n",
    "            size = traj[\"position\"][\"shape\"][1]\n",
    "            length = traj[\"position\"][\"shape\"][0] - window_length + 1\n",
    "            for i in range(length):\n",
    "                desc = {\n",
    "                    \"size\": size,\n",
    "                    \"type\": traj[\"particle_type\"][\"offset\"],\n",
    "                    \"pos\": traj[\"position\"][\"offset\"] + i * size * self.dim,\n",
    "                }\n",
    "                self.windows.append(desc)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # load corresponding data for this time slice\n",
    "        window = self.windows[idx]\n",
    "        size = window[\"size\"]\n",
    "        particle_type = self.particle_type[window[\"type\"]: window[\"type\"] + size].copy()\n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        position_seq = self.position[window[\"pos\"]: window[\"pos\"] + self.window_length * size * self.dim].copy()\n",
    "        position_seq.resize(self.window_length, size, self.dim)\n",
    "        position_seq = position_seq.transpose(1, 0, 2)\n",
    "        target_position = position_seq[:, -1]\n",
    "        position_seq = position_seq[:, :-1]\n",
    "        target_position = torch.from_numpy(target_position)\n",
    "        position_seq = torch.from_numpy(position_seq)\n",
    "\n",
    "        # construct the graph\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise_std)\n",
    "        if self.return_pos:\n",
    "          return graph, position_seq[:, -1]\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqhPcZeKthYq"
   },
   "source": [
    "### Rollout Dataset\n",
    "\n",
    "Each datapoint in this dataset contains trajectories of particles over 1000 time frames. This dataset is used in the evaluation phase to measure the model's ability to makie long-horizon predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kuk2Z-I8sFv7"
   },
   "outputs": [],
   "source": [
    "class RolloutDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # load data from the disk\n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        self.window_length = window_length\n",
    "\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "\n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            break\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.offset)\n",
    "\n",
    "    def get(self, idx):\n",
    "        traj = self.offset[idx]\n",
    "        size = traj[\"position\"][\"shape\"][1]\n",
    "        time_step = traj[\"position\"][\"shape\"][0]\n",
    "        particle_type = self.particle_type[traj[\"particle_type\"][\"offset\"]: traj[\"particle_type\"][\"offset\"] + size].copy()\n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        position = self.position[traj[\"position\"][\"offset\"]: traj[\"position\"][\"offset\"] + time_step * size * self.dim].copy()\n",
    "        position.resize(traj[\"position\"][\"shape\"])\n",
    "        position = torch.from_numpy(position)\n",
    "        data = {\"particle_type\": particle_type, \"position\": position}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVjrldn4kD-P"
   },
   "source": [
    "### Visualize a graph in the dataset\n",
    "\n",
    "Each data point in the dataset is a `pyg.data.Data` object which describes a graph. We explain the contents of the first data point, and visualize the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "_4MJuOhjkTjx",
    "outputId": "729728e0-44ce-41a1-e74f-1dde8d42511b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "dataset_sample = OneStepDataset(OUTPUT_DIR, \"valid\", return_pos=True)\n",
    "graph, position = dataset_sample[0]\n",
    "\n",
    "print(f\"The first item in the valid set is a graph: {graph}\")\n",
    "print(f\"This graph has {graph.num_nodes} nodes and {graph.num_edges} edges.\")\n",
    "print(f\"Each node is a particle and each edge is the interaction between two particles.\")\n",
    "print(f\"Each node has {graph.num_node_features} categorial feature (Data.x), which represents the type of the node.\")\n",
    "print(f\"Each node has a {graph.pos.size(1)}-dim feature vector (Data.pos), which represents the positions and velocities of the particle (node) in several frames.\")\n",
    "print(f\"Each edge has a {graph.num_edge_features}-dim feature vector (Data.edge_attr), which represents the relative distance and displacement between particles.\")\n",
    "print(f\"The model is expected to predict a {graph.y.size(1)}-dim vector for each node (Data.y), which represents the acceleration of the particle.\")\n",
    "\n",
    "# remove directions of edges, because it is a symmetric directed graph.\n",
    "nx_graph = pyg.utils.to_networkx(graph).to_undirected()\n",
    "# remove self loops, because every node has a self loop.\n",
    "nx_graph.remove_edges_from(nx.selfloop_edges(nx_graph))\n",
    "plt.figure(figsize=(7, 7))\n",
    "nx.draw(nx_graph, pos={i: tuple(v) for i, v in enumerate(position)}, node_size=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlnk5URCF7pZ"
   },
   "source": [
    "## GNN Model\n",
    "\n",
    "We will walk through the implementation of the GNN model in this section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vsVI6epJcsn"
   },
   "source": [
    "### Helper class\n",
    "\n",
    "We first define a class for Multi-Layer Perceptron (MLP). This class generates an MLP given the width and the depth of it. Because MLPs are used in several places of the GNN, this helper class will make the code cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht-upXnRo0dV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch_scatter\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"Multi-Layer perceptron\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, layers, layernorm=True):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(layers):\n",
    "            self.layers.append(torch.nn.Linear(\n",
    "                input_size if i == 0 else hidden_size,\n",
    "                output_size if i == layers - 1 else hidden_size,\n",
    "            ))\n",
    "            if i != layers - 1:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "        if layernorm:\n",
    "            self.layers.append(torch.nn.LayerNorm(output_size))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                layer.weight.data.normal_(0, 1 / math.sqrt(layer.in_features))\n",
    "                layer.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_pkzDgqJ_ED"
   },
   "source": [
    "### GNN layers\n",
    "\n",
    "In the following code block, we implement one type of GNN layer named `InteractionNetwork` (IN), which is proposed by the paper *Interaction Networks for Learning about Objects,\n",
    "Relations and Physics*.\n",
    "\n",
    "For a graph $G$, let the feature of node $i$ be $v_i$, and the feature of edge $(i, j)$ be $e_{i, j}$. There are three stages for IN to generate new features of nodes and edges.\n",
    "\n",
    "1. **Message generation.** If there is an edge pointing from node $i$ to node $j$, node $i$ sends a message to node $j$. The message carries the information of the edge and its two nodes, so it is generated by the following equation $\\mathrm{Msg}_{i,j} = \\mathrm{MLP}(v_i, v_j, e_{i,j})$.\n",
    "\n",
    "1. **Message aggregation.** In this stage, each node of the graph aggregates all the messages that it received to a fixed-sized representation. In the IN, aggregation means summing all the messages up, i.e., $\\mathrm{Agg}_i=\\sum_{(j,i)\\in G}\\mathrm{Msg}_{i,j}$.\n",
    "\n",
    "1. **Update.** Finally, we update features of nodes and edges with the results of previous stages. For each edge, its new feature is simply the sum of its old feature and the correspond message, i.e., $e'_{i,j}=e_{i,j}+\\mathrm{Msg}_{i,j}$. For each node, the new feature is determined by its old feature and the aggregated message, i.e., $v'_i=v_i+\\mathrm{MLP}(v_i, \\mathrm{Agg}_i)$.\n",
    "\n",
    "In PyG, GNN layers are implemented as subclass of `MessagePassing`. We need to override three critical functions to implement our `InteractionNetwork` GNN layer. Each function corresponds to one stage of the GNN layer.\n",
    "\n",
    "1. `message()` -> message generation\n",
    "\n",
    "  This function controls how a message is generated on each edge of the graph. It takes three arguments: (1) `x_i`, features of the source nodes; (2) `x_j`, features of the target nodes; and (3) `edge_feature`, features of the edges themselves. In the IN, we simply concatenate all these features and generate the messages with an MLP.\n",
    "\n",
    "1. `aggregate()` -> message aggregation\n",
    "\n",
    "  This function aggregates messages for nodes. It depends on two arguments: (1) `inputs`, messages; and (2) `index`, the graph structure. We handle over the task of message aggregation to the function `torch_scatter.scatter` and specifies in the argument `reduce` that we want to sum messages up. Because we want to retain messages themselves to update edge features, we return both messages and aggregated messages.\n",
    "\n",
    "1. `forward()` -> update\n",
    "\n",
    "  This function puts everything together. `x` is the node features, `edge_index` is the graph structure and `edge_feature` is edge features. The function`MessagePassing.propagate` invokes functions `message` and `aggregate` for us. Then, we update node features and edge features and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nobE0LcXJ6RR"
   },
   "outputs": [],
   "source": [
    "class InteractionNetwork(pyg.nn.MessagePassing):\n",
    "    \"\"\"Interaction Network as proposed in this paper:\n",
    "    https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html\"\"\"\n",
    "    def __init__(self, hidden_size, layers):\n",
    "        super().__init__()\n",
    "        self.lin_edge = MLP(hidden_size * 3, hidden_size, hidden_size, layers)\n",
    "        self.lin_node = MLP(hidden_size * 2, hidden_size, hidden_size, layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_feature):\n",
    "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
    "        node_out = self.lin_node(torch.cat((x, aggr), dim=-1))\n",
    "        edge_out = edge_feature + edge_out\n",
    "        node_out = x + node_out\n",
    "        return node_out, edge_out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_feature):\n",
    "        x = torch.cat((x_i, x_j, edge_feature), dim=-1)\n",
    "        x = self.lin_edge(x)\n",
    "        return x\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "        out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\")\n",
    "        return (inputs, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Aa9znXKH40"
   },
   "source": [
    "### The GNN\n",
    "\n",
    "Now its time to stack GNN layers to a GNN. Besides GNN layers, there are pre-processing and post-processing blocks in the GNN. Before GNN layers, input features are transformed by MLP so that the expressiveness of GNN is improved without increasing GNN layers. After GNN layers, final outputs (accelerations of particles in our case) are extracted from features generated by GNN layers to meet the requirement of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoB4_A6YJ7FP"
   },
   "outputs": [],
   "source": [
    "class LearnedSimulator(torch.nn.Module):\n",
    "    \"\"\"Graph Network-based Simulators(GNS)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=128,\n",
    "        n_mp_layers=10, # number of GNN layers\n",
    "        num_particle_types=9,\n",
    "        particle_type_dim=16, # embedding dimension of particle types\n",
    "        dim=2, # dimension of the world, typical 2D or 3D\n",
    "        window_size=5, # the model looks into W frames before the frame to be predicted\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embed_type = torch.nn.Embedding(num_particle_types, particle_type_dim)\n",
    "        self.node_in = MLP(particle_type_dim + dim * (window_size + 2), hidden_size, hidden_size, 3)\n",
    "        self.edge_in = MLP(dim + 1, hidden_size, hidden_size, 3)\n",
    "        self.node_out = MLP(hidden_size, hidden_size, dim, 3, layernorm=False)\n",
    "        self.n_mp_layers = n_mp_layers\n",
    "        self.layers = torch.nn.ModuleList([InteractionNetwork(\n",
    "            hidden_size, 3\n",
    "        ) for _ in range(n_mp_layers)])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embed_type.weight)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # pre-processing\n",
    "        # node feature: combine categorial feature data.x and contiguous feature data.pos.\n",
    "        node_feature = torch.cat((self.embed_type(data.x), data.pos), dim=-1)\n",
    "        node_feature = self.node_in(node_feature)\n",
    "        edge_feature = self.edge_in(data.edge_attr)\n",
    "        # stack of GNN layers\n",
    "        for i in range(self.n_mp_layers):\n",
    "            node_feature, edge_feature = self.layers[i](node_feature, data.edge_index, edge_feature=edge_feature)\n",
    "        # post-processing\n",
    "        out = self.node_out(node_feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7LUIouahvhW"
   },
   "source": [
    "## Training\n",
    "\n",
    "Before we start training the model, let's configure the hyperparameters! Since the accessible computaion power is limited in a notebook, we will only run 1 epoch of training, which takes about 1.5 hour. Consequently, we won't be able to produce as accurate results as shown in the original paper in this notebook. Alternatively, we provide a checkpoint of training the model on the entire WaterDrop dataset for 5 epochs, which takes about 14 hours with a GeForce RTX 3080 Ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Czpr3hJTiCuC"
   },
   "outputs": [],
   "source": [
    "data_path = OUTPUT_DIR\n",
    "model_path = os.path.join(\"temp\", \"models\", DATASET_NAME)\n",
    "rollout_path = os.path.join(\"temp\", \"rollouts\", DATASET_NAME)\n",
    "\n",
    "!mkdir -p \"$model_path\"\n",
    "!mkdir -p \"$rollout_path\"\n",
    "\n",
    "params = {\n",
    "    \"epoch\": 1,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"noise\": 3e-4,\n",
    "    \"save_interval\": 1000,\n",
    "    \"eval_interval\": 1000,\n",
    "    \"rollout_interval\": 200000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gN_P6D4tK7FQ"
   },
   "source": [
    "Below are some helper functions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSIAlxN7KvuZ"
   },
   "outputs": [],
   "source": [
    "def rollout(model, data, metadata, noise_std):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    window_size = model.window_size + 1\n",
    "    total_time = data[\"position\"].size(0)\n",
    "    traj = data[\"position\"][:window_size]\n",
    "    traj = traj.permute(1, 0, 2)\n",
    "    particle_type = data[\"particle_type\"]\n",
    "\n",
    "    for time in range(total_time - window_size):\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, traj[:, -window_size:], None, metadata, 0.0)\n",
    "            graph = graph.to(device)\n",
    "            acceleration = model(graph).cpu()\n",
    "            acceleration = acceleration * torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2) + torch.tensor(metadata[\"acc_mean\"])\n",
    "\n",
    "            recent_position = traj[:, -1]\n",
    "            recent_velocity = recent_position - traj[:, -2]\n",
    "            new_velocity = recent_velocity + acceleration\n",
    "            new_position = recent_position + new_velocity\n",
    "            traj = torch.cat((traj, new_position.unsqueeze(1)), dim=1)\n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "def oneStepMSE(simulator, dataloader, metadata, noise):\n",
    "    \"\"\"Returns two values, loss and MSE\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    batch_count = 0\n",
    "    simulator.eval()\n",
    "    with torch.no_grad():\n",
    "        scale = torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise ** 2).cuda()\n",
    "        for data in valid_loader:\n",
    "            data = data.cuda()\n",
    "            pred = simulator(data)\n",
    "            mse = ((pred - data.y) * scale) ** 2\n",
    "            mse = mse.sum(dim=-1).mean()\n",
    "            loss = ((pred - data.y) ** 2).mean()\n",
    "            total_mse += mse.item()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "    return total_loss / batch_count, total_mse / batch_count\n",
    "\n",
    "\n",
    "def rolloutMSE(simulator, dataset, noise):\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    simulator.eval()\n",
    "    with torch.no_grad():\n",
    "        for rollout_data in dataset:\n",
    "            rollout_out = rollout(simulator, rollout_data, dataset.metadata, noise)\n",
    "            rollout_out = rollout_out.permute(1, 0, 2)\n",
    "            loss = (rollout_out - rollout_data[\"position\"]) ** 2\n",
    "            loss = loss.sum(dim=-1).mean()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "    return total_loss / batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwvseROOFqt0"
   },
   "source": [
    "Here is the main training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRsKEIX6XAwN"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(params, simulator, train_loader, valid_loader, valid_rollout_dataset):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(simulator.parameters(), lr=params[\"lr\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1 ** (1 / 5e6))\n",
    "\n",
    "    # recording loss curve\n",
    "    train_loss_list = []\n",
    "    eval_loss_list = []\n",
    "    onestep_mse_list = []\n",
    "    rollout_mse_list = []\n",
    "    total_step = 0\n",
    "\n",
    "    for i in range(params[\"epoch\"]):\n",
    "        simulator.train()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {i}\")\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        for data in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.cuda()\n",
    "            pred = simulator(data)\n",
    "            loss = loss_fn(pred, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count, \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "            total_step += 1\n",
    "            train_loss_list.append((total_step, loss.item()))\n",
    "\n",
    "            # evaluation\n",
    "            if total_step % params[\"eval_interval\"] == 0:\n",
    "                simulator.eval()\n",
    "                eval_loss, onestep_mse = oneStepMSE(simulator, valid_loader, valid_dataset.metadata, params[\"noise\"])\n",
    "                eval_loss_list.append((total_step, eval_loss))\n",
    "                onestep_mse_list.append((total_step, onestep_mse))\n",
    "                tqdm.write(f\"\\nEval: Loss: {eval_loss}, One Step MSE: {onestep_mse}\")\n",
    "                simulator.train()\n",
    "\n",
    "            # do rollout on valid set\n",
    "            if total_step % params[\"rollout_interval\"] == 0:\n",
    "                simulator.eval()\n",
    "                rollout_mse = rolloutMSE(simulator, valid_rollout_dataset, params[\"noise\"])\n",
    "                rollout_mse_list.append((total_step, rollout_mse))\n",
    "                tqdm.write(f\"\\nEval: Rollout MSE: {rollout_mse}\")\n",
    "                simulator.train()\n",
    "\n",
    "            # save model\n",
    "            if total_step % params[\"save_interval\"] == 0:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model\": simulator.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                    },\n",
    "                    os.path.join(model_path, f\"checkpoint_{total_step}.pt\")\n",
    "                )\n",
    "    return train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ-U-nlEakHF"
   },
   "source": [
    "Finally, let's load the dataset and train the model! It takes roughly 1.5 hour to run this block of the notebook with the default parameters. **If you are impatient, we highly recommend you to skip the next 2 blocks and load the checkpoint we provided to save some time; otherwise, make a cup of tea/coffee and come back later to see the results of training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1HWNWqbE6db",
    "outputId": "64f8d3db-f2da-44ff-8ba8-5b5c080c70ae"
   },
   "outputs": [],
   "source": [
    "# Training the model is time-consuming. We highly recommend you to skip this block and load the checkpoint in the next block.\n",
    "\n",
    "# load dataset\n",
    "train_dataset = OneStepDataset(data_path, \"train\", noise_std=params[\"noise\"])\n",
    "valid_dataset = OneStepDataset(data_path, \"valid\", noise_std=params[\"noise\"])\n",
    "train_loader = pyg.loader.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, pin_memory=True, num_workers=2)\n",
    "valid_loader = pyg.loader.DataLoader(valid_dataset, batch_size=params[\"batch_size\"], shuffle=False, pin_memory=True, num_workers=2)\n",
    "valid_rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
    "\n",
    "# build model\n",
    "simulator = LearnedSimulator()\n",
    "simulator = simulator.cuda()\n",
    "\n",
    "# train the model\n",
    "train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list = train(params, simulator, train_loader, valid_loader, valid_rollout_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "MBw_qGjz62_S",
    "outputId": "fec750a7-acc8-4c1c-da71-55458631349e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize the loss curve\n",
    "plt.figure()\n",
    "plt.plot(*zip(*train_loss_list), label=\"train\")\n",
    "plt.plot(*zip(*eval_loss_list), label=\"valid\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p temp/models/WaterDropSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y2Pf_TcbR0K"
   },
   "source": [
    "Load the checkpoint trained by us. Do **not** run this block if you have trained your model in the previous block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw26wx-9B_Q3",
    "outputId": "70170ae5-9d90-496d-82da-aee78b8dd9e6"
   },
   "outputs": [],
   "source": [
    "simulator = LearnedSimulator()\n",
    "simulator = simulator.cuda()\n",
    "\n",
    "checkpoint = torch.load(\"WaterDropSample/checkpoint_100000.pt\")\n",
    "simulator.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eDD-SySxKFt"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Since the video is 1000 frames long, it might take a few minutes to rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNhrML1SrWIe"
   },
   "outputs": [],
   "source": [
    "rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
    "simulator.eval()\n",
    "rollout_data = rollout_dataset[0]\n",
    "rollout_out = rollout(simulator, rollout_data, rollout_dataset.metadata, params[\"noise\"])\n",
    "rollout_out = rollout_out.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "H0W5Tm4rws0_",
    "outputId": "ddd2094a-daab-4548-aef8-2d9dd3818f42"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "TYPE_TO_COLOR = {\n",
    "    3: \"black\",\n",
    "    0: \"green\",\n",
    "    7: \"magenta\",\n",
    "    6: \"gold\",\n",
    "    5: \"blue\",\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_prepare(ax, particle_type, position, metadata):\n",
    "    bounds = metadata[\"bounds\"]\n",
    "    ax.set_xlim(bounds[0][0], bounds[0][1])\n",
    "    ax.set_ylim(bounds[1][0], bounds[1][1])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(1.0)\n",
    "    points = {type_: ax.plot([], [], \"o\", ms=2, color=color)[0] for type_, color in TYPE_TO_COLOR.items()}\n",
    "    return ax, position, points\n",
    "\n",
    "\n",
    "def visualize_pair(particle_type, position_pred, position_gt, metadata):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    plot_info = [\n",
    "        visualize_prepare(axes[0], particle_type, position_gt, metadata),\n",
    "        visualize_prepare(axes[1], particle_type, position_pred, metadata),\n",
    "    ]\n",
    "    axes[0].set_title(\"Ground truth\")\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    def update(step_i):\n",
    "        outputs = []\n",
    "        for _, position, points in plot_info:\n",
    "            for type_, line in points.items():\n",
    "                mask = particle_type == type_\n",
    "                line.set_data(position[step_i, mask, 0], position[step_i, mask, 1])\n",
    "            outputs.append(line)\n",
    "        return outputs\n",
    "\n",
    "    return animation.FuncAnimation(fig, update, frames=np.arange(0, position_gt.size(0)), interval=10, blit=True)\n",
    "\n",
    "anim = visualize_pair(rollout_data[\"particle_type\"], rollout_out, rollout_data[\"position\"], rollout_dataset.metadata)\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
