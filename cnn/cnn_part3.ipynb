{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e219b8",
   "metadata": {},
   "source": [
    "# CNN Part 3: Introduction to DDP with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a3f7d",
   "metadata": {},
   "source": [
    "Check for GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85079b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPUs are available = {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of GPUs available are {}'.format(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You are currently using GPU with local rank = {}'.format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5f5f2",
   "metadata": {},
   "source": [
    "## Introduction to DDP with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a788c0",
   "metadata": {},
   "source": [
    "### Create Process Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c230af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "def init_distributed(local_rank, world_size):\n",
    "    '''\n",
    "    local_rank: identifier for pariticular GPU on one node\n",
    "    world: total number of process in a the group\n",
    "    '''\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
    "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
    "    torch.cuda.set_device(local_rank)                 \n",
    "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
    "                            rank=local_rank,          # rank of the current process being used\n",
    "                            world_size=world_size)    # total number of processors being used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d45e96",
   "metadata": {},
   "source": [
    "### Create Data DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def load_dataset(train_dataset):\n",
    "    train_data = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,\n",
    "        #################################################\n",
    "        shuffle=False,                             # shuffle should be set to False when using DistributedSampler\n",
    "        sampler=DistributedSampler(train_dataset), # passing the distributed loader\n",
    "        ################################################\n",
    "    )\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81dd6a",
   "metadata": {},
   "source": [
    "## MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697029b",
   "metadata": {},
   "source": [
    "### Non-Distributed Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f8c27",
   "metadata": {},
   "source": [
    "#### Get Data\n",
    "Load the MNIST dataset and pass it to a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size=32):\n",
    "\n",
    "    # download MNIST dataset\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "                            root=os.path.join(os.environ['SCRATCH'], \"data\"),      # path to where data is stored\n",
    "                            train=True,                                         # specifies if data is train or test\n",
    "                            download=True,                                      # downloads data if not available at root\n",
    "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
    "                            )\n",
    "    # pass dataset to the dataloader\n",
    "    train_dataloader = DataLoader(trainset,\n",
    "                                  shuffle=False,\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "trainloader=prepare_data(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13c3fb",
   "metadata": {},
   "source": [
    "Visualize a few images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26697fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80949c99",
   "metadata": {},
   "source": [
    "#### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        prob = self.linear_relu_stack(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769bcdf",
   "metadata": {},
   "source": [
    "#### Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # transfer data to GPU if available\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def main(device):\n",
    "\n",
    "    # Setup Dataloader\n",
    "    train_dataloader=prepare_data(batch_size=4)\n",
    "    \n",
    "    # Instantiate Model \n",
    "    model = Net().to(device)\n",
    "    \n",
    "    # instantiate loss and optimizer \n",
    "    loss_fn = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train Model \n",
    "    epochs = 3\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, model, loss_fn, optimizer)\n",
    "        \n",
    "    print(\"Done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92989f",
   "metadata": {},
   "source": [
    "Train the model by calling `main`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = main(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf5793",
   "metadata": {},
   "source": [
    "### Distributed Code for Multiple GPUs on One Node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 1. Create a process group (function)\n",
    "def init_distributed(local_rank, world_size):\n",
    "    '''\n",
    "    local_rank: identifier for pariticular GPU on one node\n",
    "    world: total number of process in a the group\n",
    "    '''\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
    "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
    "    torch.cuda.set_device(local_rank)                 \n",
    "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
    "                            rank=local_rank,          # rank of the current process being used\n",
    "                            world_size=world_size)    # total number of processors being used\n",
    "#################################################  \n",
    "    \n",
    "def prepare_data(local_rank, world_size, batch_size=32):\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "                            root=os.path.join(os.environ['SCRATCH'], \"data\"),      # path to where data is stored\n",
    "                            train=True,                                         # specifies if data is train or test\n",
    "                            download=True,                                      # downloads data if not available at root\n",
    "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
    "                            )\n",
    "\n",
    "    # pass data to the distributed sampler and dataloader\n",
    "    train_dataloader = DataLoader(trainset,\n",
    "                                  ################################################\n",
    "                                  # 2. Setup Dataloader with Distributed Sampler\n",
    "                                  shuffle=False,\n",
    "                                  sampler=DistributedSampler(trainset, num_replicas=world_size, rank=local_rank),\n",
    "                                  ################################################\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "# training loop for one epoch\n",
    "def train_loop(local_rank, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # transfer data to GPU if available\n",
    "        X = X.to(local_rank)\n",
    "        y = y.to(local_rank)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ################################################\n",
    "        # 4. Only write/print model information on one GPU\n",
    "        if local_rank == 0:\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        ################################################\n",
    "\n",
    "def main(local_rank, world_size):\n",
    "    ################################################\n",
    "    # 1. Set up Process Group\n",
    "    init_distributed(local_rank, world_size)\n",
    "    ################################################\n",
    "\n",
    "    ################################################\n",
    "    # 2. Setup Dataloader with Distributed Sampler\n",
    "    train_dataloader = prepare_data(local_rank, world_size)\n",
    "    ################################################\n",
    "\n",
    "    ################################################\n",
    "    # 3. Wrap Model with DDP\n",
    "    model = DDP(Net().to(local_rank),\n",
    "        device_ids=[local_rank],                  # list of gpu that model lives on\n",
    "        output_device=local_rank,                 # where to output model\n",
    "    )\n",
    "    ################################################\n",
    "\n",
    "    # instantiate loss and optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train Model\n",
    "    epochs = 10\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(local_rank, train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    #################################################\n",
    "    # 5. Close Process Group\n",
    "    dist.destroy_process_group()\n",
    "    #################################################\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eda4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb2b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905113a",
   "metadata": {},
   "source": [
    "`display_python` displays `cnn_part3/mnist_parallel.py` with Python syntax highlighting as the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db5613c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #F00 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #04D } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #00F; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #800 } /* Name.Constant */\n",
       ".output_html .nd { color: #A2F } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #00F } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #BBB } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #00F } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.distributed</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">dist</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span> \n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.utils.data</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span> \n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.multiprocessing</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">mp</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.utils.data.distributed</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DistributedSampler</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torchvision</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.nn</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">nn</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.nn.functional</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">F</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n",
       "    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">flatten</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Flatten</span><span class=\"p\">()</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear_relu_stack</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"o\">*</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">),</span>\n",
       "        <span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n",
       "        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear_relu_stack</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">prob</span>\n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">prepare_data</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">):</span>\n",
       "        \n",
       "    <span class=\"n\">trainset</span> <span class=\"o\">=</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">MNIST</span><span class=\"p\">(</span>\n",
       "                            <span class=\"n\">root</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;SCRATCH&#39;</span><span class=\"p\">],</span> <span class=\"s2\">&quot;data&quot;</span><span class=\"p\">),</span>   <span class=\"c1\"># path to where data is stored</span>\n",
       "                            <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>                                         <span class=\"c1\"># specifies if data is train or test</span>\n",
       "                            <span class=\"n\">download</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>                                      <span class=\"c1\"># downloads data if not available at root</span>\n",
       "                            <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ToTensor</span><span class=\"p\">()</span>         <span class=\"c1\"># trasforms both features and targets accordingly</span>\n",
       "                            <span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"c1\"># pass data to the distributed sampler and dataloader </span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"n\">trainset</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"n\">trainset</span><span class=\"p\">,</span> <span class=\"n\">num_replicas</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">,</span> <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">),</span>\n",
       "                                  <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataloader</span>\n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">init_distributed</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">    </span><span class=\"sd\">&#39;&#39;&#39;</span>\n",
       "<span class=\"sd\">    rank: identifier for pariticular GPU</span>\n",
       "<span class=\"sd\">    world: total number of process in a the group</span>\n",
       "<span class=\"sd\">    &#39;&#39;&#39;</span>\n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;MASTER_ADDR&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;localhost&#39;</span>           <span class=\"c1\"># IP address of rank 0 process</span>\n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;MASTER_PORT&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;12355&#39;</span>               <span class=\"c1\"># a free port used to communicate amongst processors</span>\n",
       "    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">set_device</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>                       \n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"s2\">&quot;nccl&quot;</span><span class=\"p\">,</span>                   <span class=\"c1\"># backend being used; nccl typically used with distributed GPU training</span>\n",
       "                            <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>                <span class=\"c1\"># rank of the current process being used</span>\n",
       "                            <span class=\"n\">world_size</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>    <span class=\"c1\"># total number of processors being used</span>\n",
       "\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.nn.parallel</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DistributedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">DDP</span>\n",
       "\n",
       "<span class=\"c1\"># training loop for one epoch</span>\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">train_loop</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\"># transfer data to GPU if available</span>\n",
       "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"c1\"># Compute prediction and loss</span>\n",
       "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"c1\"># Backpropagation</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"c1\"># 4. Only write/print model information on one GPU</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"k\">if</span> <span class=\"n\">batch</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "                <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">(),</span> <span class=\"n\">batch</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"si\">:</span><span class=\"s2\">&gt;7f</span><span class=\"si\">}</span><span class=\"s2\">  [</span><span class=\"si\">{</span><span class=\"n\">current</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">size</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">]&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">):</span>\n",
       "    <span class=\"c1\">################################################</span>\n",
       "    <span class=\"c1\"># 1. Set up Process Group </span>\n",
       "    <span class=\"n\">init_distributed</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">)</span>\n",
       "    <span class=\"c1\">################################################</span>\n",
       "\n",
       "    <span class=\"c1\">################################################</span>\n",
       "    <span class=\"c1\"># 2. Setup Dataloader with Distributed Sampler</span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">prepare_data</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">)</span>\n",
       "    <span class=\"c1\">################################################</span>\n",
       "\n",
       "    <span class=\"c1\">################################################                                                 </span>\n",
       "    <span class=\"c1\"># 3. Wrap Model with DDP  </span>\n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DDP</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">),</span>\n",
       "        <span class=\"n\">device_ids</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">local_rank</span><span class=\"p\">],</span>                  <span class=\"c1\"># list of gpu that model lives on </span>\n",
       "        <span class=\"n\">output_device</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>                 <span class=\"c1\"># where to output model</span>\n",
       "    <span class=\"p\">)</span>        \n",
       "    <span class=\"c1\">################################################</span>\n",
       "    \n",
       "    <span class=\"c1\"># instantiate loss and optimizer </span>\n",
       "    <span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">CrossEntropyLoss</span><span class=\"p\">()</span>\n",
       "    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"o\">*</span><span class=\"mf\">0.001</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># Train Model </span>\n",
       "    <span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"c1\"># 4. Only write/print model information on one GPU</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">local_rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Epoch </span><span class=\"si\">{</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">-------------------------------&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"n\">train_loop</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "    <span class=\"c1\"># 5. Close Process Group</span>\n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">destroy_process_group</span><span class=\"p\">()</span>\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Done!&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">model</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">world_size</span><span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;world_size = </span><span class=\"si\">{}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">world_size</span><span class=\"p\">))</span>\n",
       "    <span class=\"n\">mp</span><span class=\"o\">.</span><span class=\"n\">spawn</span><span class=\"p\">(</span><span class=\"n\">main</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">world_size</span><span class=\"p\">,)</span> <span class=\"p\">,</span> <span class=\"n\">nprocs</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{dist}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch} \n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DataLoader}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{numpy}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{np} \n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{matplotlib}\\PY{n+nn}{.}\\PY{n+nn}{pyplot}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{plt}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{multiprocessing}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{mp}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DistributedSampler}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torchvision}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{nn}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{F}\n",
       "\n",
       "\n",
       "\\PY{k}{class}\\PY{+w}{ }\\PY{n+nc}{Net}\\PY{p}{(}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Module}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{def}\\PY{+w}{ }\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb}{super}\\PY{p}{(}\\PY{n}{Net}\\PY{p}{,} \\PY{n+nb+bp}{self}\\PY{p}{)}\\PY{o}{.}\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{flatten} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Flatten}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{linear\\PYZus{}relu\\PYZus{}stack} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{28}\\PY{o}{*}\\PY{l+m+mi}{28}\\PY{p}{,} \\PY{l+m+mi}{128}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{ReLU}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Dropout}\\PY{p}{(}\\PY{l+m+mf}{0.2}\\PY{p}{)}\\PY{p}{,}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{10}\\PY{p}{)}\\PY{p}{,}\n",
       "        \\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{forward}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n}{x} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{flatten}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{n}{prob} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{linear\\PYZus{}relu\\PYZus{}stack}\\PY{p}{(}\\PY{n}{x}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{prob}\n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{,} \\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{32}\\PY{p}{)}\\PY{p}{:}\n",
       "        \n",
       "    \\PY{n}{trainset} \\PY{o}{=} \\PY{n}{torchvision}\\PY{o}{.}\\PY{n}{datasets}\\PY{o}{.}\\PY{n}{MNIST}\\PY{p}{(}\n",
       "                            \\PY{n}{root}\\PY{o}{=}\\PY{n}{os}\\PY{o}{.}\\PY{n}{path}\\PY{o}{.}\\PY{n}{join}\\PY{p}{(}\\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SCRATCH}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{data}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,}   \\PY{c+c1}{\\PYZsh{} path to where data is stored}\n",
       "                            \\PY{n}{train}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{,}                                         \\PY{c+c1}{\\PYZsh{} specifies if data is train or test}\n",
       "                            \\PY{n}{download}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{,}                                      \\PY{c+c1}{\\PYZsh{} downloads data if not available at root}\n",
       "                            \\PY{n}{transform}\\PY{o}{=}\\PY{n}{torchvision}\\PY{o}{.}\\PY{n}{transforms}\\PY{o}{.}\\PY{n}{ToTensor}\\PY{p}{(}\\PY{p}{)}         \\PY{c+c1}{\\PYZsh{} trasforms both features and targets accordingly}\n",
       "                            \\PY{p}{)}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} pass data to the distributed sampler and dataloader }\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{DataLoader}\\PY{p}{(}\\PY{n}{trainset}\\PY{p}{,}\n",
       "                                  \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{False}\\PY{p}{,}\n",
       "                                  \\PY{n}{sampler}\\PY{o}{=}\\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n}{trainset}\\PY{p}{,} \\PY{n}{num\\PYZus{}replicas}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{,} \\PY{n}{rank}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataloader}\n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{    }\\PY{l+s+sd}{\\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "\\PY{l+s+sd}{    rank: identifier for pariticular GPU}\n",
       "\\PY{l+s+sd}{    world: total number of process in a the group}\n",
       "\\PY{l+s+sd}{    \\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MASTER\\PYZus{}ADDR}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{localhost}\\PY{l+s+s1}{\\PYZsq{}}           \\PY{c+c1}{\\PYZsh{} IP address of rank 0 process}\n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MASTER\\PYZus{}PORT}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{12355}\\PY{l+s+s1}{\\PYZsq{}}               \\PY{c+c1}{\\PYZsh{} a free port used to communicate amongst processors}\n",
       "    \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{set\\PYZus{}device}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}                       \n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{init\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{nccl}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}                   \\PY{c+c1}{\\PYZsh{} backend being used; nccl typically used with distributed GPU training}\n",
       "                            \\PY{n}{rank}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}                \\PY{c+c1}{\\PYZsh{} rank of the current process being used}\n",
       "                            \\PY{n}{world\\PYZus{}size}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}    \\PY{c+c1}{\\PYZsh{} total number of processors being used}\n",
       "\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{parallel}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DistributedDataParallel} \\PY{k}{as} \\PY{n}{DDP}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} training loop for one epoch}\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{size} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\n",
       "    \\PY{k}{for} \\PY{n}{batch}\\PY{p}{,} \\PY{p}{(}\\PY{n}{X}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)} \\PY{o+ow}{in} \\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{dataloader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} transfer data to GPU if available}\n",
       "        \\PY{n}{X} \\PY{o}{=} \\PY{n}{X}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\n",
       "        \\PY{n}{y} \\PY{o}{=} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} Compute prediction and loss}\n",
       "        \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} Backpropagation}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} 4. Only write/print model information on one GPU}\n",
       "        \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{k}{if} \\PY{n}{batch} \\PY{o}{\\PYZpc{}} \\PY{l+m+mi}{100} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "                \\PY{n}{loss}\\PY{p}{,} \\PY{n}{current} \\PY{o}{=} \\PY{n}{loss}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{batch} \\PY{o}{*} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "                \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{loss}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}7f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  [}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{size}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{]}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{main}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 1. Set up Process Group }\n",
       "    \\PY{n}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 2. Setup Dataloader with Distributed Sampler}\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}                                                 }\n",
       "    \\PY{c+c1}{\\PYZsh{} 3. Wrap Model with DDP  }\n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{DDP}\\PY{p}{(}\\PY{n}{Net}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}\\PY{p}{,}\n",
       "        \\PY{n}{device\\PYZus{}ids}\\PY{o}{=}\\PY{p}{[}\\PY{n}{local\\PYZus{}rank}\\PY{p}{]}\\PY{p}{,}                  \\PY{c+c1}{\\PYZsh{} list of gpu that model lives on }\n",
       "        \\PY{n}{output\\PYZus{}device}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}                 \\PY{c+c1}{\\PYZsh{} where to output model}\n",
       "    \\PY{p}{)}        \n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} instantiate loss and optimizer }\n",
       "    \\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{CrossEntropyLoss}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{n}{optimizer} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{Adam}\\PY{p}{(}\\PY{n}{model}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{lr}\\PY{o}{=}\\PY{l+m+mi}{4}\\PY{o}{*}\\PY{l+m+mf}{0.001}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Train Model }\n",
       "    \\PY{n}{epochs} \\PY{o}{=} \\PY{l+m+mi}{10}\n",
       "    \\PY{k}{for} \\PY{n}{t} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epochs}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} 4. Only write/print model information on one GPU}\n",
       "        \\PY{k}{if} \\PY{n}{local\\PYZus{}rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{t}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{n}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 5. Close Process Group}\n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{destroy\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Done!}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{model}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{n}{world\\PYZus{}size}\\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{device\\PYZus{}count}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{world\\PYZus{}size = }\\PY{l+s+si}{\\PYZob{}\\PYZcb{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{o}{.}\\PY{n}{format}\\PY{p}{(}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}\\PY{p}{)}\n",
       "    \\PY{n}{mp}\\PY{o}{.}\\PY{n}{spawn}\\PY{p}{(}\\PY{n}{main}\\PY{p}{,} \\PY{n}{args}\\PY{o}{=}\\PY{p}{(}\\PY{n}{world\\PYZus{}size}\\PY{p}{,}\\PY{p}{)} \\PY{p}{,} \\PY{n}{nprocs}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "import torchvision\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "\n",
       "\n",
       "class Net(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(Net, self).__init__()\n",
       "        self.flatten = torch.nn.Flatten()\n",
       "        self.linear_relu_stack = torch.nn.Sequential(\n",
       "            torch.nn.Linear(28*28, 128),\n",
       "            torch.nn.ReLU(),\n",
       "            torch.nn.Dropout(0.2),\n",
       "            torch.nn.Linear(128, 10),\n",
       "        )\n",
       "\n",
       "    def forward(self, x):\n",
       "        x = self.flatten(x)\n",
       "        prob = self.linear_relu_stack(x)\n",
       "        return prob\n",
       "\n",
       "def prepare_data(local_rank, world_size, batch_size=32):\n",
       "        \n",
       "    trainset = torchvision.datasets.MNIST(\n",
       "                            root=os.path.join(os.environ['SCRATCH'], \"data\"),   # path to where data is stored\n",
       "                            train=True,                                         # specifies if data is train or test\n",
       "                            download=True,                                      # downloads data if not available at root\n",
       "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
       "                            )\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(trainset,\n",
       "                                  shuffle=False,\n",
       "                                  sampler=DistributedSampler(trainset, num_replicas=world_size, rank=local_rank),\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "def init_distributed(local_rank, world_size):\n",
       "    '''\n",
       "    rank: identifier for pariticular GPU\n",
       "    world: total number of process in a the group\n",
       "    '''\n",
       "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
       "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
       "    torch.cuda.set_device(local_rank)                       \n",
       "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
       "                            rank=local_rank,                # rank of the current process being used\n",
       "                            world_size=world_size)    # total number of processors being used\n",
       "\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(local_rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(local_rank)\n",
       "        y = y.to(local_rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "        \n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if local_rank == 0:\n",
       "            if batch % 100 == 0:\n",
       "                loss, current = loss.item(), batch * len(X)\n",
       "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "        ################################################\n",
       "\n",
       "def main(local_rank, world_size):\n",
       "    ################################################\n",
       "    # 1. Set up Process Group \n",
       "    init_distributed(local_rank, world_size)\n",
       "    ################################################\n",
       "\n",
       "    ################################################\n",
       "    # 2. Setup Dataloader with Distributed Sampler\n",
       "    train_dataloader = prepare_data(local_rank, world_size)\n",
       "    ################################################\n",
       "\n",
       "    ################################################                                                 \n",
       "    # 3. Wrap Model with DDP  \n",
       "    model = DDP(Net().to(local_rank),\n",
       "        device_ids=[local_rank],                  # list of gpu that model lives on \n",
       "        output_device=local_rank,                 # where to output model\n",
       "    )        \n",
       "    ################################################\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.CrossEntropyLoss()\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=4*0.001)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 10\n",
       "    for t in range(epochs):\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if local_rank == 0:\n",
       "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        ################################################\n",
       "        train_loop(local_rank, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    #################################################\n",
       "    # 5. Close Process Group\n",
       "    dist.destroy_process_group()\n",
       "    #################################################\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    world_size= torch.cuda.device_count()\n",
       "    print('world_size = {}'.format(world_size))\n",
       "    mp.spawn(main, args=(world_size,) , nprocs=world_size)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to show syntax highlighting\n",
    "display.Code(\"cnn_part3/mnist_parallel.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd350f",
   "metadata": {},
   "source": [
    "Run `cnn_part3/mnist_parallel.py` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9d7f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 cnn_part3/mnist_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356ca21",
   "metadata": {},
   "source": [
    "## Additional Exercise\n",
    "\n",
    "There is a script called `cnn_part3/simple_linear_regression_serial.py` that implements a simple linear regression model with PyTorch. Below, the script is modified to run on multiple GPUs on one node using PyTorch's DDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c951087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #F00 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #04D } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #00F; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #800 } /* Name.Constant */\n",
       ".output_html .nd { color: #A2F } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #00F } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #BBB } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #00F } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.distributed</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">dist</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span> \n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.utils.data</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span> \n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.multiprocessing</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">mp</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.utils.data.distributed</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DistributedSampler</span>\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.nn.parallel</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DistributedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">DDP</span>\n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_model</span><span class=\"p\">():</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>     <span class=\"c1\"># first number specifies input dimension; second number specifies output dimension</span>\n",
       "    <span class=\"p\">)</span> \n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">prepare_data</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">):</span>\n",
       "    <span class=\"c1\"># Generate random data centered around 10 with noise</span>\n",
       "    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>\n",
       "    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">3</span>\n",
       "    \n",
       "    <span class=\"c1\"># pass data to the distributed sampler and dataloader </span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)),</span>\n",
       "                                  <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataloader</span>\n",
       "\n",
       "<span class=\"c1\"># training loop for one epoch</span>\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">train_loop</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\"># transfer data to GPU if available</span>\n",
       "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"c1\"># Compute prediction and loss</span>\n",
       "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Backpropagation</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "        <span class=\"k\">if</span> <span class=\"n\">batch</span> <span class=\"o\">%</span> <span class=\"mi\">2</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">(),</span> <span class=\"n\">batch</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"si\">:</span><span class=\"s2\">&gt;7f</span><span class=\"si\">}</span><span class=\"s2\">  [</span><span class=\"si\">{</span><span class=\"n\">current</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">size</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">]&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">prepare_data</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">get_model</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"c1\"># instantiate loss and optimizer </span>\n",
       "    <span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MSELoss</span><span class=\"p\">(</span><span class=\"n\">reduction</span><span class=\"o\">=</span><span class=\"s1\">&#39;mean&#39;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># Train Model </span>\n",
       "    <span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Epoch </span><span class=\"si\">{</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">-------------------------------&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">train_loop</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Done!&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">model</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s1\">&#39;cuda&#39;</span> <span class=\"k\">if</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">is_available</span><span class=\"p\">()</span> <span class=\"k\">else</span> <span class=\"s1\">&#39;cpu&#39;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">main</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{dist}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch} \n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DataLoader}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{numpy}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{np} \n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{matplotlib}\\PY{n+nn}{.}\\PY{n+nn}{pyplot}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{plt}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{multiprocessing}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{mp}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DistributedSampler}\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{parallel}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DistributedDataParallel} \\PY{k}{as} \\PY{n}{DDP}\n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{return} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)}\\PY{p}{,}     \\PY{c+c1}{\\PYZsh{} first number specifies input dimension; second number specifies output dimension}\n",
       "    \\PY{p}{)} \n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{32}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{} Generate random data centered around 10 with noise}\n",
       "    \\PY{n}{X} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{10}\n",
       "    \\PY{n}{y} \\PY{o}{=} \\PY{n}{X} \\PY{o}{+} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{3}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} pass data to the distributed sampler and dataloader }\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{DataLoader}\\PY{p}{(}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{zip}\\PY{p}{(}\\PY{n}{X}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{,}\n",
       "                                  \\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataloader}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} training loop for one epoch}\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{size} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\n",
       "    \\PY{k}{for} \\PY{n}{batch}\\PY{p}{,} \\PY{p}{(}\\PY{n}{X}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)} \\PY{o+ow}{in} \\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{dataloader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} transfer data to GPU if available}\n",
       "        \\PY{n}{X} \\PY{o}{=} \\PY{n}{X}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \\PY{n}{y} \\PY{o}{=} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} Compute prediction and loss}\n",
       "        \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Backpropagation}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "        \\PY{k}{if} \\PY{n}{batch} \\PY{o}{\\PYZpc{}} \\PY{l+m+mi}{2} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n}{loss}\\PY{p}{,} \\PY{n}{current} \\PY{o}{=} \\PY{n}{loss}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{batch} \\PY{o}{*} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{loss}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}7f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  [}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{size}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{]}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{main}\\PY{p}{(}\\PY{n}{device}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{device}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} instantiate loss and optimizer }\n",
       "    \\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{MSELoss}\\PY{p}{(}\\PY{n}{reduction}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{mean}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "    \\PY{n}{optimizer} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{Adam}\\PY{p}{(}\\PY{n}{model}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{lr}\\PY{o}{=}\\PY{l+m+mf}{0.1}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Train Model }\n",
       "    \\PY{n}{epochs} \\PY{o}{=} \\PY{l+m+mi}{20}\n",
       "    \\PY{k}{for} \\PY{n}{t} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epochs}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{t}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{n}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{device}\\PY{p}{,} \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Done!}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{model}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{n}{device} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{cuda}\\PY{l+s+s1}{\\PYZsq{}} \\PY{k}{if} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{is\\PYZus{}available}\\PY{p}{(}\\PY{p}{)} \\PY{k}{else} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{cpu}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "    \\PY{n}{main}\\PY{p}{(}\\PY{n}{device}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "def get_model():\n",
       "    return torch.nn.Sequential(\n",
       "            torch.nn.Linear(1, 1),     # first number specifies input dimension; second number specifies output dimension\n",
       "    ) \n",
       "\n",
       "def prepare_data(batch_size=32):\n",
       "    # Generate random data centered around 10 with noise\n",
       "    X = torch.randn(32*4, 1) * 10\n",
       "    y = X + torch.randn(32*4, 1) * 3\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(list(zip(X,y)),\n",
       "                                  shuffle=True,\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(rank)\n",
       "        y = y.to(rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "\n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        if batch % 2 == 0:\n",
       "            loss, current = loss.item(), batch * len(X)\n",
       "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "\n",
       "def main(device):\n",
       "    train_dataloader = prepare_data()\n",
       "\n",
       "    model = get_model().to(device)\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 20\n",
       "    for t in range(epochs):\n",
       "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        train_loop(device, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "    main(device)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to show syntax highlighting\n",
    "display.Code(\"cnn_part3/simple_linear_regression_serial.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 cnn_part3/simple_linear_regression_serial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9db95e",
   "metadata": {},
   "source": [
    "`cnn_part3/simple_linear_regression_parallel.py` is the modified code to work with DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ad94c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #F00 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #04D } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #00F; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #800 } /* Name.Constant */\n",
       ".output_html .nd { color: #A2F } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #00F } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #BBB } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #00F } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.distributed</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">dist</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch</span> \n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.utils.data</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DataLoader</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span> \n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">matplotlib.pyplot</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">plt</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">torch.multiprocessing</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">mp</span>\n",
       "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">os</span>\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.utils.data.distributed</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DistributedSampler</span>\n",
       "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.nn.parallel</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">DistributedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">DDP</span>\n",
       "\n",
       "\n",
       "<span class=\"c1\">##############################################</span>\n",
       "<span class=\"c1\"># 1. Create a process group</span>\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">init_distributed</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">    </span><span class=\"sd\">&#39;&#39;&#39;</span>\n",
       "<span class=\"sd\">    local_rank: identifier for pariticular GPU on one node</span>\n",
       "<span class=\"sd\">    world: total number of process in a the group</span>\n",
       "<span class=\"sd\">    &#39;&#39;&#39;</span>\n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;MASTER_ADDR&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;localhost&#39;</span>           <span class=\"c1\"># IP address of rank 0 process</span>\n",
       "    <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s1\">&#39;MASTER_PORT&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;12355&#39;</span>               <span class=\"c1\"># a free port used to communicate amongst processors</span>\n",
       "    <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">set_device</span><span class=\"p\">(</span><span class=\"n\">local_rank</span><span class=\"p\">)</span>                 \n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">init_process_group</span><span class=\"p\">(</span><span class=\"s2\">&quot;nccl&quot;</span><span class=\"p\">,</span>                   <span class=\"c1\"># backend being used; nccl typically used with distributed GPU training</span>\n",
       "                            <span class=\"n\">rank</span><span class=\"o\">=</span><span class=\"n\">local_rank</span><span class=\"p\">,</span>          <span class=\"c1\"># rank of the current process being used</span>\n",
       "                            <span class=\"n\">world_size</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>    <span class=\"c1\"># total number of processors being used</span>\n",
       "<span class=\"c1\">##############################################</span>\n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_model</span><span class=\"p\">():</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n",
       "            <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span>     <span class=\"c1\"># first number specifies input dimension; second number specifies output dimension</span>\n",
       "            <span class=\"p\">)</span> \n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">prepare_data</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">):</span>\n",
       "    <span class=\"c1\"># Generate random data centered around 10 with noise</span>\n",
       "    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>\n",
       "    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">+</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">3</span>\n",
       "    \n",
       "    <span class=\"c1\"># pass data to the distributed sampler and dataloader </span>\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">DataLoader</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)),</span>\n",
       "                                  <span class=\"c1\">##############################################</span>\n",
       "                                  <span class=\"c1\"># 2. Use Pytorch&#39;s DistributedSampler to ensure that data passed to each GPU is different</span>\n",
       "                                  <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span>\n",
       "                                  <span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"n\">DistributedSampler</span><span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">))),</span>\n",
       "                                  <span class=\"c1\">##############################################</span>\n",
       "                                  <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n",
       "    \n",
       "    <span class=\"k\">return</span> <span class=\"n\">train_dataloader</span>\n",
       "\n",
       "<span class=\"c1\"># training loop for one epoch</span>\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">train_loop</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">dataloader</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\"># transfer data to GPU if available</span>\n",
       "        <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "        \n",
       "        <span class=\"c1\"># Compute prediction and loss</span>\n",
       "        <span class=\"n\">pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Backpropagation</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n",
       "\n",
       "        <span class=\"k\">if</span> <span class=\"n\">batch</span> <span class=\"o\">%</span> <span class=\"mi\">2</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">(),</span> <span class=\"n\">batch</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;loss: </span><span class=\"si\">{</span><span class=\"n\">loss</span><span class=\"si\">:</span><span class=\"s2\">&gt;7f</span><span class=\"si\">}</span><span class=\"s2\">  [</span><span class=\"si\">{</span><span class=\"n\">current</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">/</span><span class=\"si\">{</span><span class=\"n\">size</span><span class=\"si\">:</span><span class=\"s2\">&gt;5d</span><span class=\"si\">}</span><span class=\"s2\">]&quot;</span><span class=\"p\">)</span>\n",
       "\n",
       "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">init_distributed</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">world_size</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"n\">train_dataloader</span> <span class=\"o\">=</span> <span class=\"n\">prepare_data</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"c1\">##############################################</span>\n",
       "    <span class=\"c1\"># 3. Wrap Model with Pytorch&#39;s DistributedDataParallel</span>\n",
       "    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">DDP</span><span class=\"p\">(</span><span class=\"n\">get_model</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">),</span> <span class=\"n\">device_ids</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">rank</span><span class=\"p\">],</span> <span class=\"n\">output_device</span><span class=\"o\">=</span><span class=\"n\">rank</span><span class=\"p\">)</span>\n",
       "    <span class=\"c1\">##############################################</span>\n",
       "    \n",
       "    <span class=\"c1\"># instantiate loss and optimizer </span>\n",
       "    <span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">MSELoss</span><span class=\"p\">(</span><span class=\"n\">reduction</span><span class=\"o\">=</span><span class=\"s1\">&#39;mean&#39;</span><span class=\"p\">)</span>\n",
       "    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\"># Train Model </span>\n",
       "    <span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">20</span>\n",
       "    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">epochs</span><span class=\"p\">):</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"c1\"># 4. Only write/print model information on one GPU</span>\n",
       "        <span class=\"k\">if</span> <span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n",
       "            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Epoch </span><span class=\"si\">{</span><span class=\"n\">t</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">-------------------------------&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"c1\">################################################</span>\n",
       "        <span class=\"n\">train_loop</span><span class=\"p\">(</span><span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"n\">train_dataloader</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">loss_fn</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "    <span class=\"c1\"># 5. Close Process Group</span>\n",
       "    <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">destroy_process_group</span><span class=\"p\">()</span>\n",
       "    <span class=\"c1\">#################################################</span>\n",
       "\n",
       "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Done!&quot;</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">return</span> <span class=\"n\">model</span>\n",
       "\n",
       "<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n",
       "    <span class=\"n\">world_size</span><span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n",
       "    <span class=\"n\">mp</span><span class=\"o\">.</span><span class=\"n\">spawn</span><span class=\"p\">(</span><span class=\"n\">main</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">world_size</span><span class=\"p\">,)</span> <span class=\"p\">,</span> <span class=\"n\">nprocs</span><span class=\"o\">=</span><span class=\"n\">world_size</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{dist}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch} \n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DataLoader}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{numpy}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{np} \n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{matplotlib}\\PY{n+nn}{.}\\PY{n+nn}{pyplot}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{plt}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{multiprocessing}\\PY{+w}{ }\\PY{k}{as}\\PY{+w}{ }\\PY{n+nn}{mp}\n",
       "\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{os}\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{utils}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DistributedSampler}\n",
       "\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{parallel}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{DistributedDataParallel} \\PY{k}{as} \\PY{n}{DDP}\n",
       "\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{} 1. Create a process group}\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{    }\\PY{l+s+sd}{\\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "\\PY{l+s+sd}{    local\\PYZus{}rank: identifier for pariticular GPU on one node}\n",
       "\\PY{l+s+sd}{    world: total number of process in a the group}\n",
       "\\PY{l+s+sd}{    \\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MASTER\\PYZus{}ADDR}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{localhost}\\PY{l+s+s1}{\\PYZsq{}}           \\PY{c+c1}{\\PYZsh{} IP address of rank 0 process}\n",
       "    \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{MASTER\\PYZus{}PORT}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]} \\PY{o}{=} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{12355}\\PY{l+s+s1}{\\PYZsq{}}               \\PY{c+c1}{\\PYZsh{} a free port used to communicate amongst processors}\n",
       "    \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{set\\PYZus{}device}\\PY{p}{(}\\PY{n}{local\\PYZus{}rank}\\PY{p}{)}                 \n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{init\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{nccl}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,}                   \\PY{c+c1}{\\PYZsh{} backend being used; nccl typically used with distributed GPU training}\n",
       "                            \\PY{n}{rank}\\PY{o}{=}\\PY{n}{local\\PYZus{}rank}\\PY{p}{,}          \\PY{c+c1}{\\PYZsh{} rank of the current process being used}\n",
       "                            \\PY{n}{world\\PYZus{}size}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}    \\PY{c+c1}{\\PYZsh{} total number of processors being used}\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{k}{return} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Sequential}\\PY{p}{(}\n",
       "            \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{Linear}\\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)}\\PY{p}{,}     \\PY{c+c1}{\\PYZsh{} first number specifies input dimension; second number specifies output dimension}\n",
       "            \\PY{p}{)} \n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{l+m+mi}{32}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{c+c1}{\\PYZsh{} Generate random data centered around 10 with noise}\n",
       "    \\PY{n}{X} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{10}\n",
       "    \\PY{n}{y} \\PY{o}{=} \\PY{n}{X} \\PY{o}{+} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{randn}\\PY{p}{(}\\PY{l+m+mi}{32}\\PY{o}{*}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{)} \\PY{o}{*} \\PY{l+m+mi}{3}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} pass data to the distributed sampler and dataloader }\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{DataLoader}\\PY{p}{(}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{zip}\\PY{p}{(}\\PY{n}{X}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "                                  \\PY{c+c1}{\\PYZsh{} 2. Use Pytorch\\PYZsq{}s DistributedSampler to ensure that data passed to each GPU is different}\n",
       "                                  \\PY{n}{shuffle}\\PY{o}{=}\\PY{k+kc}{False}\\PY{p}{,}\n",
       "                                  \\PY{n}{sampler}\\PY{o}{=}\\PY{n}{DistributedSampler}\\PY{p}{(}\\PY{n+nb}{list}\\PY{p}{(}\\PY{n+nb}{zip}\\PY{p}{(}\\PY{n}{X}\\PY{p}{,}\\PY{n}{y}\\PY{p}{)}\\PY{p}{)}\\PY{p}{)}\\PY{p}{,}\n",
       "                                  \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "                                  \\PY{n}{batch\\PYZus{}size}\\PY{o}{=}\\PY{n}{batch\\PYZus{}size}\\PY{p}{)}\n",
       "    \n",
       "    \\PY{k}{return} \\PY{n}{train\\PYZus{}dataloader}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} training loop for one epoch}\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{size} \\PY{o}{=} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{dataset}\\PY{p}{)}\n",
       "    \\PY{k}{for} \\PY{n}{batch}\\PY{p}{,} \\PY{p}{(}\\PY{n}{X}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)} \\PY{o+ow}{in} \\PY{n+nb}{enumerate}\\PY{p}{(}\\PY{n}{dataloader}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} transfer data to GPU if available}\n",
       "        \\PY{n}{X} \\PY{o}{=} \\PY{n}{X}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \\PY{n}{y} \\PY{o}{=} \\PY{n}{y}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\n",
       "        \n",
       "        \\PY{c+c1}{\\PYZsh{} Compute prediction and loss}\n",
       "        \\PY{n}{pred} \\PY{o}{=} \\PY{n}{model}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "        \\PY{n}{loss} \\PY{o}{=} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{(}\\PY{n}{pred}\\PY{p}{,} \\PY{n}{y}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Backpropagation}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{zero\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{loss}\\PY{o}{.}\\PY{n}{backward}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{optimizer}\\PY{o}{.}\\PY{n}{step}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "        \\PY{k}{if} \\PY{n}{batch} \\PY{o}{\\PYZpc{}} \\PY{l+m+mi}{2} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n}{loss}\\PY{p}{,} \\PY{n}{current} \\PY{o}{=} \\PY{n}{loss}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{batch} \\PY{o}{*} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{X}\\PY{p}{)}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{loss: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{loss}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}7f}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{  [}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{current}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{/}\\PY{l+s+si}{\\PYZob{}}\\PY{n}{size}\\PY{l+s+si}{:}\\PY{l+s+s2}{\\PYZgt{}5d}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{]}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "\n",
       "\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{main}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{init\\PYZus{}distributed}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{world\\PYZus{}size}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n}{train\\PYZus{}dataloader} \\PY{o}{=} \\PY{n}{prepare\\PYZus{}data}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 3. Wrap Model with Pytorch\\PYZsq{}s DistributedDataParallel}\n",
       "    \\PY{n}{model} \\PY{o}{=} \\PY{n}{DDP}\\PY{p}{(}\\PY{n}{get\\PYZus{}model}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{)}\\PY{p}{,} \\PY{n}{device\\PYZus{}ids}\\PY{o}{=}\\PY{p}{[}\\PY{n}{rank}\\PY{p}{]}\\PY{p}{,} \\PY{n}{output\\PYZus{}device}\\PY{o}{=}\\PY{n}{rank}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \n",
       "    \\PY{c+c1}{\\PYZsh{} instantiate loss and optimizer }\n",
       "    \\PY{n}{loss\\PYZus{}fn} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{MSELoss}\\PY{p}{(}\\PY{n}{reduction}\\PY{o}{=}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{mean}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)}\n",
       "    \\PY{n}{optimizer} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{optim}\\PY{o}{.}\\PY{n}{Adam}\\PY{p}{(}\\PY{n}{model}\\PY{o}{.}\\PY{n}{parameters}\\PY{p}{(}\\PY{p}{)}\\PY{p}{,} \\PY{n}{lr}\\PY{o}{=}\\PY{l+m+mf}{0.1}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{} Train Model }\n",
       "    \\PY{n}{epochs} \\PY{o}{=} \\PY{l+m+mi}{20}\n",
       "    \\PY{k}{for} \\PY{n}{t} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{epochs}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{c+c1}{\\PYZsh{} 4. Only write/print model information on one GPU}\n",
       "        \\PY{k}{if} \\PY{n}{rank} \\PY{o}{==} \\PY{l+m+mi}{0}\\PY{p}{:}\n",
       "            \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Epoch }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{t}\\PY{o}{+}\\PY{l+m+mi}{1}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+se}{\\PYZbs{}n}\\PY{l+s+s2}{\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}\\PYZhy{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "        \\PY{n}{train\\PYZus{}loop}\\PY{p}{(}\\PY{n}{rank}\\PY{p}{,} \\PY{n}{train\\PYZus{}dataloader}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{loss\\PYZus{}fn}\\PY{p}{,} \\PY{n}{optimizer}\\PY{p}{)}\n",
       "\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "    \\PY{c+c1}{\\PYZsh{} 5. Close Process Group}\n",
       "    \\PY{n}{dist}\\PY{o}{.}\\PY{n}{destroy\\PYZus{}process\\PYZus{}group}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Done!}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "    \\PY{k}{return} \\PY{n}{model}\n",
       "\n",
       "\\PY{k}{if} \\PY{n+nv+vm}{\\PYZus{}\\PYZus{}name\\PYZus{}\\PYZus{}} \\PY{o}{==} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{\\PYZus{}\\PYZus{}main\\PYZus{}\\PYZus{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:}\n",
       "    \\PY{n}{world\\PYZus{}size}\\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{cuda}\\PY{o}{.}\\PY{n}{device\\PYZus{}count}\\PY{p}{(}\\PY{p}{)}\n",
       "    \\PY{n}{mp}\\PY{o}{.}\\PY{n}{spawn}\\PY{p}{(}\\PY{n}{main}\\PY{p}{,} \\PY{n}{args}\\PY{o}{=}\\PY{p}{(}\\PY{n}{world\\PYZus{}size}\\PY{p}{,}\\PY{p}{)} \\PY{p}{,} \\PY{n}{nprocs}\\PY{o}{=}\\PY{n}{world\\PYZus{}size}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "\n",
       "import torch.distributed as dist\n",
       "import torch \n",
       "from torch.utils.data import DataLoader\n",
       "import numpy as np \n",
       "import matplotlib.pyplot as plt\n",
       "import torch.multiprocessing as mp\n",
       "import os\n",
       "from torch.utils.data.distributed import DistributedSampler\n",
       "from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "\n",
       "\n",
       "##############################################\n",
       "# 1. Create a process group\n",
       "def init_distributed(local_rank, world_size):\n",
       "    '''\n",
       "    local_rank: identifier for pariticular GPU on one node\n",
       "    world: total number of process in a the group\n",
       "    '''\n",
       "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
       "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
       "    torch.cuda.set_device(local_rank)                 \n",
       "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
       "                            rank=local_rank,          # rank of the current process being used\n",
       "                            world_size=world_size)    # total number of processors being used\n",
       "##############################################\n",
       "\n",
       "def get_model():\n",
       "    return torch.nn.Sequential(\n",
       "            torch.nn.Linear(1, 1),     # first number specifies input dimension; second number specifies output dimension\n",
       "            ) \n",
       "\n",
       "def prepare_data(batch_size=32):\n",
       "    # Generate random data centered around 10 with noise\n",
       "    X = torch.randn(32*4, 1) * 10\n",
       "    y = X + torch.randn(32*4, 1) * 3\n",
       "    \n",
       "    # pass data to the distributed sampler and dataloader \n",
       "    train_dataloader = DataLoader(list(zip(X,y)),\n",
       "                                  ##############################################\n",
       "                                  # 2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
       "                                  shuffle=False,\n",
       "                                  sampler=DistributedSampler(list(zip(X,y))),\n",
       "                                  ##############################################\n",
       "                                  batch_size=batch_size)\n",
       "    \n",
       "    return train_dataloader\n",
       "\n",
       "# training loop for one epoch\n",
       "def train_loop(rank, dataloader, model, loss_fn, optimizer):\n",
       "    size = len(dataloader.dataset)\n",
       "    for batch, (X, y) in enumerate(dataloader):\n",
       "        # transfer data to GPU if available\n",
       "        X = X.to(rank)\n",
       "        y = y.to(rank)\n",
       "        \n",
       "        # Compute prediction and loss\n",
       "        pred = model(X)\n",
       "        loss = loss_fn(pred, y)\n",
       "\n",
       "        # Backpropagation\n",
       "        optimizer.zero_grad()\n",
       "        loss.backward()\n",
       "        optimizer.step()\n",
       "\n",
       "        if batch % 2 == 0:\n",
       "            loss, current = loss.item(), batch * len(X)\n",
       "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
       "\n",
       "def main(rank, world_size):\n",
       "    init_distributed(rank, world_size)\n",
       "\n",
       "    train_dataloader = prepare_data()\n",
       "\n",
       "    ##############################################\n",
       "    # 3. Wrap Model with Pytorch's DistributedDataParallel\n",
       "    model = DDP(get_model().to(rank), device_ids=[rank], output_device=rank)\n",
       "    ##############################################\n",
       "    \n",
       "    # instantiate loss and optimizer \n",
       "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
       "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
       "\n",
       "    # Train Model \n",
       "    epochs = 20\n",
       "    for t in range(epochs):\n",
       "        ################################################\n",
       "        # 4. Only write/print model information on one GPU\n",
       "        if rank == 0:\n",
       "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
       "        ################################################\n",
       "        train_loop(rank, train_dataloader, model, loss_fn, optimizer)\n",
       "\n",
       "    #################################################\n",
       "    # 5. Close Process Group\n",
       "    dist.destroy_process_group()\n",
       "    #################################################\n",
       "\n",
       "    print(\"Done!\")\n",
       "    return model\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    world_size= torch.cuda.device_count()\n",
       "    mp.spawn(main, args=(world_size,) , nprocs=world_size)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to show syntax highlighting\n",
    "display.Code(\"cnn_part3/simple_linear_regression_parallel.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 cnn_part3/simple_linear_regression_parallel.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
