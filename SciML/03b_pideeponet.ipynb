{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbNRpxLW1Uo_"
   },
   "source": [
    "# Physics-Informed DeepONet for 1D Poisson Equation\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how to incorporate physics constraints into DeepONet training\n",
    "- Implement soft-constrained physics-informed DeepONet (PI-DeepONet)\n",
    "- Apply PI-DeepONet to the 1D Poisson equation with Dirichlet boundary conditions\n",
    "- Compare physics-informed vs data-driven approaches\n",
    "\n",
    "**Slides:** [![View PDF](https://img.shields.io/badge/View-PDF-red?style=flat-square&logo=googledocs&logoColor=white)](https://github.com/chishiki-ai/sciml/raw/main/docs/02-deeponet/deeponet-slides.pdf)\n",
    "\n",
    "**Problem:** Learn the solution operator for the 1D Poisson equation:\n",
    "$$\\frac{d^2u}{dx^2} = -f(x), \\quad x \\in [0,1]$$\n",
    "with Dirichlet boundary conditions: $u(0) = 0$, $u(1) = 0$\n",
    "\n",
    "**Physics-Informed Approach:** Incorporate the differential equation and boundary conditions directly into the loss function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Bky-NJ11UpA",
    "outputId": "13c8b921-5e6d-4838-96e4-2ec61a03e004"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.integrate import solve_bvp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style and random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device selection\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWqijJLc1UpB"
   },
   "source": [
    "## Problem Setup: 1D Poisson Equation\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "We want to learn the solution operator $\\mathcal{G}$ that maps source functions $f(x)$ to solutions $u(x)$:\n",
    "\n",
    "$$\\mathcal{G}[f] = u$$\n",
    "\n",
    "where $u(x)$ satisfies:\n",
    "- **PDE:** $\\frac{d^2u}{dx^2} = -f(x)$ for $x \\in [0,1]$\n",
    "- **Boundary conditions:** $u(0) = 0$, $u(1) = 0$\n",
    "\n",
    "### Physics-Informed Loss Components\n",
    "\n",
    "1. **Data Loss:** $\\mathcal{L}_{data} = \\frac{1}{N} \\sum_{i=1}^N \\|u_{pred}^{(i)} - u_{true}^{(i)}\\|^2$\n",
    "\n",
    "2. **Physics Loss:** $\\mathcal{L}_{physics} = \\frac{1}{N} \\sum_{i=1}^N \\left\\|\\frac{d^2u_{pred}^{(i)}}{dx^2} + f^{(i)}\\right\\|^2$\n",
    "\n",
    "3. **Boundary Loss:** $\\mathcal{L}_{boundary} = \\frac{1}{N} \\sum_{i=1}^N \\left[|u_{pred}^{(i)}(0)|^2 + |u_{pred}^{(i)}(1)|^2\\right]$\n",
    "\n",
    "4. **Total Loss:** $\\mathcal{L} = \\mathcal{L}_{data} + \\lambda_{physics} \\mathcal{L}_{physics} + \\lambda_{boundary} \\mathcal{L}_{boundary}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoD3lFdq1UpB"
   },
   "source": [
    "![PI DeepONet](https://github.com/chishiki-ai/sciml/blob/main/docs/02-deeponet/figs/pideeponet-arch.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-596Suy1UpC",
    "outputId": "52cf1967-91ac-453b-fd28-21b58f42457a"
   },
   "outputs": [],
   "source": [
    "# ========================= DATA GENERATION =========================\n",
    "def generate_source_functions(num_functions=1000, num_points=100):\n",
    "    \"\"\"Generate random source functions f(x) using Fourier series\"\"\"\n",
    "    x = np.linspace(0, 1, num_points)\n",
    "    functions = np.zeros((num_functions, num_points))\n",
    "\n",
    "    for i in range(num_functions):\n",
    "        # Create f(x) using Fourier series with random coefficients\n",
    "        f = np.zeros_like(x)\n",
    "\n",
    "        # Add sine modes (automatically satisfy boundary conditions)\n",
    "        for n in range(1, 6):  # Use first 5 modes\n",
    "            coeff = np.random.normal(0, 1/(n**2))  # Decay with mode number\n",
    "            f += coeff * np.sin(n * np.pi * x)\n",
    "\n",
    "        functions[i] = f\n",
    "\n",
    "    return x, functions\n",
    "\n",
    "def solve_poisson_1d(x, f):\n",
    "    \"\"\"Solve 1D Poisson equation using finite differences\"\"\"\n",
    "    n = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "\n",
    "    # Create finite difference matrix for d²u/dx²\n",
    "    # Interior points: u[i-1] - 2*u[i] + u[i+1] = dx² * (-f[i])\n",
    "    A = np.zeros((n-2, n-2))\n",
    "    b = np.zeros(n-2)\n",
    "\n",
    "    for i in range(n-2):\n",
    "        A[i, i] = -2.0 / (dx**2)\n",
    "        if i > 0:\n",
    "            A[i, i-1] = 1.0 / (dx**2)\n",
    "        if i < n-3:\n",
    "            A[i, i+1] = 1.0 / (dx**2)\n",
    "        b[i] = -f[i+1]  # RHS: -f(x)\n",
    "\n",
    "    # Solve for interior points\n",
    "    u_interior = np.linalg.solve(A, b)\n",
    "\n",
    "    # Construct full solution (boundary conditions u(0)=0, u(1)=0)\n",
    "    u = np.zeros(n)\n",
    "    u[1:-1] = u_interior\n",
    "\n",
    "    return u\n",
    "\n",
    "def generate_poisson_data(num_functions=1000, num_points=100):\n",
    "    \"\"\"Generate dataset for 1D Poisson equation\"\"\"\n",
    "    x, source_functions = generate_source_functions(num_functions, num_points)\n",
    "    solutions = np.zeros_like(source_functions)\n",
    "\n",
    "    print(f\"Generating {num_functions} Poisson solutions...\")\n",
    "    for i in tqdm(range(num_functions)):\n",
    "        solutions[i] = solve_poisson_1d(x, source_functions[i])\n",
    "\n",
    "    return x, source_functions, solutions\n",
    "\n",
    "print(\"=== GENERATING POISSON DATA ===\")\n",
    "x, source_functions, solutions = generate_poisson_data(num_functions=1000, num_points=100)\n",
    "\n",
    "print(f\"Generated {len(source_functions)} function pairs\")\n",
    "print(f\"Domain: x ∈ [0, 1] with {len(x)} points\")\n",
    "print(f\"Source functions shape: {source_functions.shape}\")\n",
    "print(f\"Solutions shape: {solutions.shape}\")\n",
    "\n",
    "# Visualize sample data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i in range(6):\n",
    "    ax = axes[i//3, i%3]\n",
    "\n",
    "    ax.plot(x, source_functions[i], 'r-', linewidth=2, label='Source f(x)', alpha=0.8)\n",
    "    ax.plot(x, solutions[i], 'b-', linewidth=2, label='Solution u(x)', alpha=0.8)\n",
    "\n",
    "    ax.set_title(f'Sample {i+1}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "    # Verify boundary conditions\n",
    "    u_boundary_error = abs(solutions[i][0]) + abs(solutions[i][-1])\n",
    "    ax.text(0.5, 0.95, f'BC error: {u_boundary_error:.2e}',\n",
    "            transform=ax.transAxes, ha='center', va='top', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Boundary conditions satisfied: u(0) = {solutions[0][0]:.6f}, u(1) = {solutions[0][-1]:.6f}\")\n",
    "print(f\"Max solution value: {np.max(np.abs(solutions)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-JrU9Fx1UpC"
   },
   "source": [
    "## Physics-Informed DeepONet Architecture\n",
    "\n",
    "### Key Modifications from Standard DeepONet\n",
    "\n",
    "1. **Automatic Differentiation:** Use PyTorch's autograd to compute derivatives\n",
    "2. **Physics Loss:** Incorporate PDE residual into training\n",
    "3. **Boundary Enforcement:** Soft constraints for boundary conditions\n",
    "4. **Multi-objective Loss:** Balance data fidelity and physics consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zti1o9If1UpD",
    "outputId": "2a41c1a6-1958-4585-bce9-6abb1e9d529d"
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedDeepONet(nn.Module):\n",
    "    def __init__(self, branch_input_dim, trunk_input_dim, hidden_dim=64, num_basis=50):\n",
    "        super(PhysicsInformedDeepONet, self).__init__()\n",
    "        self.num_basis = num_basis\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Branch network - processes source functions f(x)\n",
    "        self.branch_net = nn.Sequential(\n",
    "            nn.Linear(branch_input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, num_basis)\n",
    "        )\n",
    "\n",
    "        # Trunk network - processes spatial coordinates\n",
    "        self.trunk_net = nn.Sequential(\n",
    "            nn.Linear(trunk_input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, num_basis)\n",
    "        )\n",
    "\n",
    "        # Bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, branch_input, trunk_input):\n",
    "        \"\"\"Forward pass through PI-DeepONet\"\"\"\n",
    "        # Branch network output\n",
    "        branch_out = self.branch_net(branch_input)  # [batch_size, num_basis]\n",
    "\n",
    "        # Trunk network output\n",
    "        if trunk_input.dim() == 3:  # [batch_size, num_points, 1]\n",
    "            batch_size, num_points, _ = trunk_input.shape\n",
    "            trunk_input_flat = trunk_input.view(-1, 1)\n",
    "            trunk_out = self.trunk_net(trunk_input_flat)\n",
    "            trunk_out = trunk_out.view(batch_size, num_points, self.num_basis)\n",
    "        else:  # [num_points, 1]\n",
    "            trunk_out = self.trunk_net(trunk_input)\n",
    "            trunk_out = trunk_out.unsqueeze(0)\n",
    "\n",
    "        # Compute inner product: sum over basis functions\n",
    "        output = torch.einsum('bi,bpi->bp', branch_out, trunk_out)\n",
    "\n",
    "        return output + self.bias\n",
    "\n",
    "    def compute_derivatives(self, branch_input, trunk_input):\n",
    "        \"\"\"Compute second derivative d²u/dx² using automatic differentiation\"\"\"\n",
    "        # Ensure trunk_input requires gradient\n",
    "        trunk_input = trunk_input.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Forward pass\n",
    "        u = self.forward(branch_input, trunk_input)\n",
    "\n",
    "        # Compute first derivative du/dx\n",
    "        du_dx = torch.autograd.grad(\n",
    "            outputs=u,\n",
    "            inputs=trunk_input,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        # Compute second derivative d²u/dx²\n",
    "        d2u_dx2 = torch.autograd.grad(\n",
    "            outputs=du_dx,\n",
    "            inputs=trunk_input,\n",
    "            grad_outputs=torch.ones_like(du_dx),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        return u, du_dx, d2u_dx2\n",
    "\n",
    "print(\"\\n=== PHYSICS-INFORMED DEEPONET ARCHITECTURE ===\")\n",
    "pi_model = PhysicsInformedDeepONet(\n",
    "    branch_input_dim=len(x),  # Source function f(x) sampled at all points\n",
    "    trunk_input_dim=1,        # Spatial coordinate x\n",
    "    hidden_dim=64,\n",
    "    num_basis=50\n",
    ")\n",
    "pi_model = pi_model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in pi_model.parameters())\n",
    "print(f\"PI-DeepONet Architecture:\")\n",
    "print(f\"- Branch input: {len(x)} (source function values)\")\n",
    "print(f\"- Trunk input: 1 (spatial coordinate)\")\n",
    "print(f\"- Hidden dimension: {pi_model.hidden_dim}\")\n",
    "print(f\"- Number of basis functions: {pi_model.num_basis}\")\n",
    "print(f\"- Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XN8MX0z41UpE"
   },
   "source": [
    "## Physics-Informed Training\n",
    "\n",
    "### Multi-Component Loss Function\n",
    "\n",
    "The physics-informed loss combines three components:\n",
    "\n",
    "1. **Data Loss:** Ensures predictions match true solutions\n",
    "2. **Physics Loss:** Enforces PDE: $\\frac{d^2u}{dx^2} + f(x) = 0$\n",
    "3. **Boundary Loss:** Enforces boundary conditions: $u(0) = 0$, $u(1) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBkozkNQ1UpE",
    "outputId": "81517cd5-0175-4e5c-d1d5-273679be1be4"
   },
   "outputs": [],
   "source": [
    "def physics_informed_loss(model, source_batch, x_batch, u_true_batch,\n",
    "                         lambda_physics=1.0, lambda_boundary=1.0, compute_physics=True):\n",
    "    \"\"\"Compute physics-informed loss with multiple components\"\"\"\n",
    "\n",
    "    if compute_physics:\n",
    "        # Ensure x_batch requires gradients for automatic differentiation\n",
    "        x_batch = x_batch.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Forward pass with derivatives\n",
    "        u_pred, du_dx, d2u_dx2 = model.compute_derivatives(source_batch, x_batch)\n",
    "\n",
    "        # 1. Data loss: ||u_pred - u_true||²\n",
    "        data_loss = torch.mean((u_pred - u_true_batch)**2)\n",
    "\n",
    "        # 2. Physics loss: ||d²u/dx² + f||²\n",
    "        # source_batch is [batch_size, num_points], u_pred is [batch_size, num_points]\n",
    "        # d2u_dx2 is [batch_size, num_points, 1], need to squeeze last dimension\n",
    "        d2u_dx2_squeezed = d2u_dx2.squeeze(-1)  # Remove last dimension to match source_batch\n",
    "        physics_residual = d2u_dx2_squeezed + source_batch\n",
    "        physics_loss = torch.mean(physics_residual**2)\n",
    "\n",
    "        # 3. Boundary loss: ||u(0)||² + ||u(1)||²\n",
    "        u_boundary_0 = u_pred[:, 0]   # u(x=0)\n",
    "        u_boundary_1 = u_pred[:, -1]  # u(x=1)\n",
    "        boundary_loss = torch.mean(u_boundary_0**2 + u_boundary_1**2)\n",
    "    else:\n",
    "        # During validation, only compute data loss to avoid gradient computation issues\n",
    "        u_pred = model(source_batch, x_batch)\n",
    "        data_loss = torch.mean((u_pred - u_true_batch)**2)\n",
    "        physics_loss = torch.tensor(0.0, device=u_pred.device)\n",
    "        boundary_loss = torch.tensor(0.0, device=u_pred.device)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = data_loss + lambda_physics * physics_loss + lambda_boundary * boundary_loss\n",
    "\n",
    "    return total_loss, data_loss, physics_loss, boundary_loss\n",
    "\n",
    "def setup_pi_data_loaders(source_functions, solutions, x, batch_size=32):\n",
    "    \"\"\"Setup data loaders for physics-informed training\"\"\"\n",
    "    # Split data\n",
    "    train_size = int(0.8 * len(source_functions))\n",
    "    val_size = int(0.1 * len(source_functions))\n",
    "\n",
    "    train_sources = source_functions[:train_size]\n",
    "    train_solutions = solutions[:train_size]\n",
    "\n",
    "    val_sources = source_functions[train_size:train_size+val_size]\n",
    "    val_solutions = solutions[train_size:train_size+val_size]\n",
    "\n",
    "    test_sources = source_functions[train_size+val_size:]\n",
    "    test_solutions = solutions[train_size+val_size:]\n",
    "\n",
    "    # Convert to tensors\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Create datasets\n",
    "    train_x = x_tensor.unsqueeze(0).repeat(len(train_sources), 1, 1)\n",
    "    val_x = x_tensor.unsqueeze(0).repeat(len(val_sources), 1, 1)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_sources, dtype=torch.float32),\n",
    "        train_x,\n",
    "        torch.tensor(train_solutions, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(val_sources, dtype=torch.float32),\n",
    "        val_x,\n",
    "        torch.tensor(val_solutions, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, (test_sources, test_solutions)\n",
    "\n",
    "def train_physics_informed_deeponet(model, train_loader, val_loader,\n",
    "                                  num_epochs=1000, lr=0.001,\n",
    "                                  lambda_physics=1.0, lambda_boundary=10.0):\n",
    "    \"\"\"Train physics-informed DeepONet\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5)\n",
    "\n",
    "    # Storage for losses\n",
    "    train_losses = {'total': [], 'data': [], 'physics': [], 'boundary': []}\n",
    "    val_losses = {'total': [], 'data': [], 'physics': [], 'boundary': []}\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 200\n",
    "    patience_counter = 0\n",
    "\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training PI-DeepONet\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss_epoch = {'total': 0, 'data': 0, 'physics': 0, 'boundary': 0}\n",
    "\n",
    "        for batch_sources, batch_x, batch_solutions in train_loader:\n",
    "            batch_sources = batch_sources.to(device)\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_solutions = batch_solutions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute full physics-informed loss during training\n",
    "            total_loss, data_loss, physics_loss, boundary_loss = physics_informed_loss(\n",
    "                model, batch_sources, batch_x, batch_solutions,\n",
    "                lambda_physics=lambda_physics, lambda_boundary=lambda_boundary,\n",
    "                compute_physics=True\n",
    "            )\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_epoch['total'] += total_loss.item()\n",
    "            train_loss_epoch['data'] += data_loss.item()\n",
    "            train_loss_epoch['physics'] += physics_loss.item()\n",
    "            train_loss_epoch['boundary'] += boundary_loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_epoch = {'total': 0, 'data': 0, 'physics': 0, 'boundary': 0}\n",
    "\n",
    "        # Compute validation loss with physics terms (but allow gradients)\n",
    "        for batch_sources, batch_x, batch_solutions in val_loader:\n",
    "            batch_sources = batch_sources.to(device)\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_solutions = batch_solutions.to(device)\n",
    "\n",
    "            # Don't use torch.no_grad() here so we can compute derivatives\n",
    "            total_loss, data_loss, physics_loss, boundary_loss = physics_informed_loss(\n",
    "                model, batch_sources, batch_x, batch_solutions,\n",
    "                lambda_physics=lambda_physics, lambda_boundary=lambda_boundary,\n",
    "                compute_physics=True\n",
    "            )\n",
    "\n",
    "            val_loss_epoch['total'] += total_loss.item()\n",
    "            val_loss_epoch['data'] += data_loss.item()\n",
    "            val_loss_epoch['physics'] += physics_loss.item()\n",
    "            val_loss_epoch['boundary'] += boundary_loss.item()\n",
    "\n",
    "        # Average losses\n",
    "        for key in train_loss_epoch:\n",
    "            train_loss_epoch[key] /= len(train_loader)\n",
    "            val_loss_epoch[key] /= len(val_loader)\n",
    "            train_losses[key].append(train_loss_epoch[key])\n",
    "            val_losses[key].append(val_loss_epoch[key])\n",
    "\n",
    "        scheduler.step(val_loss_epoch['total'])\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Total': f'{train_loss_epoch[\"total\"]:.2e}',\n",
    "            'Data': f'{train_loss_epoch[\"data\"]:.2e}',\n",
    "            'Physics': f'{train_loss_epoch[\"physics\"]:.2e}',\n",
    "            'Boundary': f'{train_loss_epoch[\"boundary\"]:.2e}',\n",
    "            'LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss_epoch['total'] < best_val_loss:\n",
    "            best_val_loss = val_loss_epoch['total']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'\\nEarly stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    pbar.close()\n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"\\n=== TRAINING SETUP ===\")\n",
    "train_loader, val_loader, test_data = setup_pi_data_loaders(\n",
    "    source_functions, solutions, x, batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test samples: {len(test_data[0])}\")\n",
    "\n",
    "print(\"\\nStarting physics-informed training...\")\n",
    "train_losses, val_losses = train_physics_informed_deeponet(\n",
    "    pi_model, train_loader, val_loader,\n",
    "    num_epochs=1000, lr=0.001,\n",
    "    lambda_physics=1.0, lambda_boundary=10.0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final losses:\")\n",
    "print(f\"  Total: {train_losses['total'][-1]:.2e}\")\n",
    "print(f\"  Data: {train_losses['data'][-1]:.2e}\")\n",
    "print(f\"  Physics: {train_losses['physics'][-1]:.2e}\")\n",
    "print(f\"  Boundary: {train_losses['boundary'][-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCpAD1Zq1UpF"
   },
   "source": [
    "## Training Analysis and Visualization\n",
    "\n",
    "### Loss Component Analysis\n",
    "\n",
    "Understanding how each loss component contributes to the overall training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqcjKWNR1UpF",
    "outputId": "db2bd33a-4768-4f7b-83a7-6464151b143c"
   },
   "outputs": [],
   "source": [
    "# Plot comprehensive training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Total loss\n",
    "axes[0, 0].plot(train_losses['total'], label='Training', alpha=0.8)\n",
    "axes[0, 0].plot(val_losses['total'], label='Validation', alpha=0.8)\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Data loss\n",
    "axes[0, 1].plot(train_losses['data'], label='Training', alpha=0.8)\n",
    "axes[0, 1].plot(val_losses['data'], label='Validation', alpha=0.8)\n",
    "axes[0, 1].set_title('Data Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Physics loss\n",
    "axes[1, 0].plot(train_losses['physics'], label='Training', alpha=0.8)\n",
    "axes[1, 0].plot(val_losses['physics'], label='Validation', alpha=0.8)\n",
    "axes[1, 0].set_title('Physics Loss (PDE Residual)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Boundary loss\n",
    "axes[1, 1].plot(train_losses['boundary'], label='Training', alpha=0.8)\n",
    "axes[1, 1].plot(val_losses['boundary'], label='Validation', alpha=0.8)\n",
    "axes[1, 1].set_title('Boundary Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Loss composition at the end of training\n",
    "final_losses = {\n",
    "    'Data': train_losses['data'][-1],\n",
    "    'Physics': train_losses['physics'][-1],\n",
    "    'Boundary': train_losses['boundary'][-1]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(final_losses.keys(), final_losses.values(), alpha=0.7)\n",
    "plt.title('Final Loss Components')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (key, value) in enumerate(final_losses.items()):\n",
    "    plt.text(i, value, f'{value:.2e}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fAtJtSC1UpG"
   },
   "source": [
    "## Model Evaluation and Physics Verification\n",
    "\n",
    "### Comprehensive Testing\n",
    "\n",
    "1. **Prediction Accuracy:** How well does the model predict solutions?\n",
    "2. **Physics Consistency:** Does the model satisfy the PDE?\n",
    "3. **Boundary Conditions:** Are boundary conditions satisfied?\n",
    "4. **Generalization:** Performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPUtwjTc1UpG",
    "outputId": "8cd0ecda-946a-4f31-8959-5687436d4f60"
   },
   "outputs": [],
   "source": [
    "def evaluate_physics_informed_model(model, test_data, x):\n",
    "    \"\"\"Comprehensive evaluation of physics-informed DeepONet\"\"\"\n",
    "    test_sources, test_solutions = test_data\n",
    "    model.eval()\n",
    "\n",
    "    n_test = len(test_sources)\n",
    "    predictions = []\n",
    "    physics_residuals = []\n",
    "    boundary_errors = []\n",
    "    l2_errors = []\n",
    "\n",
    "    print(f\"Evaluating {n_test} test samples...\")\n",
    "\n",
    "    # Don't use torch.no_grad() so we can compute derivatives\n",
    "    for i in tqdm(range(n_test)):\n",
    "        # Prepare input\n",
    "        source_tensor = torch.tensor(test_sources[i:i+1], dtype=torch.float32).to(device)\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        x_tensor = x_tensor.requires_grad_(True)\n",
    "\n",
    "        # Predict solution and compute derivatives\n",
    "        u_pred, du_dx, d2u_dx2 = model.compute_derivatives(source_tensor, x_tensor)\n",
    "\n",
    "        # Convert to numpy\n",
    "        u_pred_np = u_pred.detach().cpu().numpy().flatten()\n",
    "        d2u_dx2_np = d2u_dx2.detach().cpu().numpy().flatten()\n",
    "\n",
    "        predictions.append(u_pred_np)\n",
    "\n",
    "        # Compute physics residual: d²u/dx² + f(x)\n",
    "        physics_residual = d2u_dx2_np + test_sources[i]\n",
    "        physics_residuals.append(physics_residual)\n",
    "\n",
    "        # Compute boundary errors\n",
    "        boundary_error = abs(u_pred_np[0]) + abs(u_pred_np[-1])  # |u(0)| + |u(1)|\n",
    "        boundary_errors.append(boundary_error)\n",
    "\n",
    "        # Compute L2 error\n",
    "        l2_error = np.sqrt(np.mean((u_pred_np - test_solutions[i])**2))\n",
    "        l2_errors.append(l2_error)\n",
    "\n",
    "    # Statistics\n",
    "    avg_l2_error = np.mean(l2_errors)\n",
    "    avg_physics_residual = np.mean([np.mean(np.abs(res)) for res in physics_residuals])\n",
    "    avg_boundary_error = np.mean(boundary_errors)\n",
    "\n",
    "    print(f\" \\nEvaluation Results:\")\n",
    "    print(f\"  Average L2 Error: {avg_l2_error:.6f}\")\n",
    "    print(f\"  Average Physics Residual: {avg_physics_residual:.6f}\")\n",
    "    print(f\"  Average Boundary Error: {avg_boundary_error:.6f}\")\n",
    "\n",
    "    return predictions, physics_residuals, boundary_errors, l2_errors\n",
    "\n",
    "print(\"\\n=== MODEL EVALUATION ===\")\n",
    "predictions, physics_residuals, boundary_errors, l2_errors = evaluate_physics_informed_model(\n",
    "    pi_model, test_data, x\n",
    ")\n",
    "\n",
    "test_sources, test_solutions = test_data\n",
    "\n",
    "# Visualization of results\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# Plot sample predictions\n",
    "for i in range(6):\n",
    "    if i < 4:\n",
    "        ax = axes[i//2, i%2]\n",
    "    else:\n",
    "        ax = axes[2, i-4]\n",
    "\n",
    "    # Plot source, true solution, and prediction\n",
    "    ax.plot(x, test_sources[i], 'g-', linewidth=2, label='Source f(x)', alpha=0.7)\n",
    "    ax.plot(x, test_solutions[i], 'b-', linewidth=2, label='True u(x)', alpha=0.8)\n",
    "    ax.plot(x, predictions[i], 'r--', linewidth=2, label='Predicted u(x)', alpha=0.8)\n",
    "\n",
    "    # Plot physics residual\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x, physics_residuals[i], 'orange', linewidth=1, alpha=0.6, label='Physics Residual')\n",
    "    ax2.set_ylabel('Physics Residual', color='orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "    # Error metrics\n",
    "    l2_err = l2_errors[i]\n",
    "    bc_err = boundary_errors[i]\n",
    "    phys_err = np.mean(np.abs(physics_residuals[i]))\n",
    "\n",
    "    ax.set_title(f'Test {i+1}\\nL2: {l2_err:.4f}, BC: {bc_err:.2e}, Phys: {phys_err:.2e}')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error distribution analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# L2 error distribution\n",
    "axes[0].hist(l2_errors, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(np.mean(l2_errors), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(l2_errors):.4f}')\n",
    "axes[0].set_title('L2 Error Distribution')\n",
    "axes[0].set_xlabel('L2 Error')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Physics residual distribution\n",
    "physics_residual_means = [np.mean(np.abs(res)) for res in physics_residuals]\n",
    "axes[1].hist(physics_residual_means, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(np.mean(physics_residual_means), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(physics_residual_means):.4f}')\n",
    "axes[1].set_title('Physics Residual Distribution')\n",
    "axes[1].set_xlabel('Mean |Physics Residual|')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Boundary error distribution\n",
    "axes[2].hist(boundary_errors, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[2].axvline(np.mean(boundary_errors), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(boundary_errors):.2e}')\n",
    "axes[2].set_title('Boundary Error Distribution')\n",
    "axes[2].set_xlabel('Boundary Error')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BisUQ5Ns1UpG"
   },
   "source": [
    "## Comparison: Standard vs Physics-Informed DeepONet\n",
    "\n",
    "### Comparative Analysis\n",
    "\n",
    "Let's train a standard DeepONet (data-only) and compare it with our physics-informed version to understand the benefits of incorporating physics constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFrf99HB1UpG",
    "outputId": "7417aa1f-10d7-4748-bbe7-e0459a3df5bf"
   },
   "outputs": [],
   "source": [
    "# Train standard DeepONet for comparison\n",
    "print(\"\\n=== TRAINING STANDARD DEEPONET FOR COMPARISON ===\")\n",
    "\n",
    "# Create standard DeepONet (same architecture, different training)\n",
    "standard_model = PhysicsInformedDeepONet(\n",
    "    branch_input_dim=len(x),\n",
    "    trunk_input_dim=1,\n",
    "    hidden_dim=64,\n",
    "    num_basis=50\n",
    ").to(device)\n",
    "\n",
    "# Train with data loss only\n",
    "def train_standard_deeponet(model, train_loader, val_loader, num_epochs=1000, lr=0.001):\n",
    "    \"\"\"Train standard DeepONet with data loss only\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=100, factor=0.5)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 200\n",
    "    patience_counter = 0\n",
    "\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training Standard DeepONet\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_sources, batch_x, batch_solutions in train_loader:\n",
    "            batch_sources = batch_sources.to(device)\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_solutions = batch_solutions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Only data loss\n",
    "            u_pred = model(batch_sources, batch_x)\n",
    "            loss = torch.mean((u_pred - batch_solutions)**2)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_sources, batch_x, batch_solutions in val_loader:\n",
    "                batch_sources = batch_sources.to(device)\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_solutions = batch_solutions.to(device)\n",
    "\n",
    "                u_pred = model(batch_sources, batch_x)\n",
    "                loss = torch.mean((u_pred - batch_solutions)**2)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Train': f'{avg_train_loss:.2e}',\n",
    "            'Val': f'{avg_val_loss:.2e}',\n",
    "            'LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'\\nEarly stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    pbar.close()\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train standard model\n",
    "standard_train_losses, standard_val_losses = train_standard_deeponet(\n",
    "    standard_model, train_loader, val_loader, num_epochs=1000, lr=0.001\n",
    ")\n",
    "\n",
    "print(\"\\nStandard DeepONet training completed!\")\n",
    "print(f\"Final training loss: {standard_train_losses[-1]:.2e}\")\n",
    "print(f\"Final validation loss: {standard_val_losses[-1]:.2e}\")\n",
    "\n",
    "# Evaluate standard model\n",
    "def evaluate_standard_model(model, test_data, x):\n",
    "    \"\"\"Evaluate standard DeepONet\"\"\"\n",
    "    test_sources, test_solutions = test_data\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    l2_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_sources)):\n",
    "            source_tensor = torch.tensor(test_sources[i:i+1], dtype=torch.float32).to(device)\n",
    "            x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            u_pred = model(source_tensor, x_tensor)\n",
    "            u_pred_np = u_pred.cpu().numpy().flatten()\n",
    "\n",
    "            predictions.append(u_pred_np)\n",
    "\n",
    "            l2_error = np.sqrt(np.mean((u_pred_np - test_solutions[i])**2))\n",
    "            l2_errors.append(l2_error)\n",
    "\n",
    "    avg_l2_error = np.mean(l2_errors)\n",
    "    print(f\"Standard DeepONet - Average L2 Error: {avg_l2_error:.6f}\")\n",
    "\n",
    "    return predictions, l2_errors\n",
    "\n",
    "standard_predictions, standard_l2_errors = evaluate_standard_model(standard_model, test_data, x)\n",
    "\n",
    "# Comparative visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i//3, i%3]\n",
    "\n",
    "    # Plot true solution and both predictions\n",
    "    ax.plot(x, test_solutions[i], 'k-', linewidth=2, label='True u(x)', alpha=0.8)\n",
    "    ax.plot(x, standard_predictions[i], 'b--', linewidth=2, label='Standard DeepONet', alpha=0.8)\n",
    "    ax.plot(x, predictions[i], 'r--', linewidth=2, label='PI-DeepONet', alpha=0.8)\n",
    "\n",
    "    # Error comparison\n",
    "    std_err = standard_l2_errors[i]\n",
    "    pi_err = l2_errors[i]\n",
    "\n",
    "    ax.set_title(f'Test {i+1}\\nStandard L2: {std_err:.4f}, PI L2: {pi_err:.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('u(x)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== COMPARISON SUMMARY ===\")\n",
    "print(f\"Standard DeepONet:\")\n",
    "print(f\"  Mean L2 Error: {np.mean(standard_l2_errors):.6f} ± {np.std(standard_l2_errors):.6f}\")\n",
    "print(f\"  Final Training Loss: {standard_train_losses[-1]:.2e}\")\n",
    "\n",
    "print(f\"\\nPhysics-Informed DeepONet:\")\n",
    "print(f\"  Mean L2 Error: {np.mean(l2_errors):.6f} ± {np.std(l2_errors):.6f}\")\n",
    "print(f\"  Mean Physics Residual: {np.mean([np.mean(np.abs(res)) for res in physics_residuals]):.6f}\")\n",
    "print(f\"  Mean Boundary Error: {np.mean(boundary_errors):.6f}\")\n",
    "print(f\"  Final Training Loss: {train_losses['total'][-1]:.2e}\")\n",
    "\n",
    "# Improvement metrics\n",
    "l2_improvement = (np.mean(standard_l2_errors) - np.mean(l2_errors)) / np.mean(standard_l2_errors) * 100\n",
    "print(f\"\\nImprovement: {l2_improvement:.1f}% reduction in L2 error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjMviAG01UpH"
   },
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "1. **Physics-Informed DeepONet:** Successfully implemented PI-DeepONet for 1D Poisson equation\n",
    "2. **Multi-objective Training:** Balanced data fidelity, physics consistency, and boundary conditions\n",
    "3. **Comprehensive Evaluation:** Analyzed prediction accuracy, physics satisfaction, and boundary compliance\n",
    "4. **Comparative Analysis:** Demonstrated advantages over standard data-driven approach\n",
    "\n",
    "### Key Advantages of Physics-Informed DeepONet\n",
    "\n",
    "✅ **Enhanced Generalization:** Physics constraints help the model generalize better to unseen data\n",
    "\n",
    "✅ **Boundary Condition Enforcement:** Soft constraints ensure boundary conditions are satisfied\n",
    "\n",
    "✅ **Physical Consistency:** Solutions automatically satisfy the governing PDE\n",
    "\n",
    "✅ **Reduced Data Requirements:** Physics knowledge compensates for limited training data\n",
    "\n",
    "✅ **Interpretable Learning:** Physics loss components provide insight into learning dynamics\n",
    "\n",
    "### Architecture Highlights\n",
    "\n",
    "- **Automatic Differentiation:** PyTorch's autograd enables seamless derivative computation\n",
    "- **Soft Constraints:** Physics and boundary conditions are enforced through loss terms\n",
    "- **Multi-scale Training:** Different loss components learn at different scales\n",
    "- **Flexible Framework:** Easily adaptable to other PDEs and boundary conditions\n",
    "\n",
    "### Applications and Extensions\n",
    "\n",
    "**Immediate Extensions:**\n",
    "- **Different Boundary Conditions:** Neumann, Robin, or periodic boundaries\n",
    "- **Higher Dimensions:** 2D/3D Poisson equations\n",
    "- **Time-dependent PDEs:** Parabolic and hyperbolic equations\n",
    "- **Nonlinear PDEs:** Variable coefficients and nonlinear source terms\n",
    "\n",
    "**Advanced Applications:**\n",
    "- **Inverse Problems:** Learning unknown parameters from data\n",
    "- **Multi-physics:** Coupled PDE systems\n",
    "- **Uncertainty Quantification:** Bayesian neural operators\n",
    "- **Real-time Control:** Fast PDE-constrained optimization\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Loss Balancing:** Carefully tune λ_physics and λ_boundary weights\n",
    "2. **Sampling Strategy:** Use sufficient collocation points for physics loss\n",
    "3. **Architecture Design:** Match network capacity to problem complexity\n",
    "4. **Training Monitoring:** Track all loss components separately\n",
    "5. **Validation:** Always verify physics satisfaction on test data\n",
    "\n",
    "---\n",
    "\n",
    "**Physics-informed DeepONet represents a powerful paradigm that combines the flexibility of neural networks with the rigor of physical laws, opening new possibilities for scientific machine learning and computational physics.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sciml",
   "language": "python",
   "name": "sciml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
