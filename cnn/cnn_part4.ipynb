{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e805597",
   "metadata": {},
   "source": [
    "# CNN Part 4: Single Node MultiGPU Training with Torchrun "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470907f",
   "metadata": {},
   "source": [
    "## Using Torchrun \n",
    "Code below will modify the MNIST example to be run by torchrun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af9480",
   "metadata": {},
   "source": [
    "### Modify code for environment variables set by torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# A. Remove code that sets environment variables as this done for you automatically with torchrun.\n",
    "def init_distributed():\n",
    "\n",
    "    # B. Instead, use these environment variables set by pytorch and instead of explicitly defining them.\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    dist.init_process_group(\"nccl\",\n",
    "                            rank=local_rank,\n",
    "                            world_size=world_size)\n",
    "\n",
    "def main():\n",
    "    #####################################################################\n",
    "    # B. We also create the variable local_rank in our main function as well as call the new init_distributed()\n",
    "    # this will be used to assign the gpu where our model should reside as highlighted below \n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    init_distributed()\n",
    "    ################################################\n",
    "    # .....\n",
    "    # instantiate network and set to local_rank device\n",
    "    net = Net().to(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334192",
   "metadata": {},
   "source": [
    "### Add code for writing checkpoints and resuming training after failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee27c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    init_distributed()\n",
    "\n",
    "    train_dataloader = prepare_data()\n",
    "\n",
    "    ################################################                                                 \n",
    "    # A. Create location to store checkpoints\n",
    "\n",
    "    # Create directory for storing checkpointed model\n",
    "    model_folder_path = os.path.join(os.environ['SCRATCH'], \"cnn4_mnist_output_model\") # create variable for path to folder for checkpoints\n",
    "    os.makedirs(model_folder_path,exist_ok=True)                         # create directory for models if they do not exist\n",
    "    \n",
    "    # create file name for checkpoint \n",
    "    checkpoint_file = os.path.join(model_folder_path, \"best_model.pt\")   # create filename for model checkpoint\n",
    "    ################################################\n",
    "\n",
    "    net = Net().to(local_rank)\n",
    "\n",
    "    #################################################\n",
    "    # 2B. Read checkpoints if they exist \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)  # load previous checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # set model weights to be that of the last checkpoint\n",
    "        epoch_start = checkpoint['epoch']                      # set epoch where training should resume\n",
    "   \n",
    "    # otherwise we are starting training from the beginning at epoch 0\n",
    "    else:\n",
    "        epoch_start = 0\n",
    "    ################################################\n",
    "\n",
    "    model = DDP(net,\n",
    "            device_ids=[local_rank],                  # list of gpu that model lives on \n",
    "            output_device=local_rank,                 # where to output model\n",
    "        )\n",
    "\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    save_every = 1\n",
    "    epochs = 10\n",
    "    ###########################################################\n",
    "    # 2C. Resume training at epoch last checkpoint was written\n",
    "    for epoch in range(epoch_start, epochs):                  # note we start loop at epoch_start defined in code above\n",
    "    ###########################################################\n",
    "        train_loop(rank, train_dataloader, model, loss_fn, optimizer)\n",
    "        ###########################################################\n",
    "        # 2D. Write checkpoints periodically during training\n",
    "        if rank == 0 and epoch%save_every==0:\n",
    "            print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "            torch.save({                                     # save model's state_dict and current epoch periodically\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict':model.module.state_dict(),\n",
    "            }, checkpoint_file)\n",
    "            print(\"Finished saving model\\n\")\n",
    "        ###########################################################\n",
    "\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6887b5",
   "metadata": {},
   "source": [
    "### Launching jobs with Torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f44c33",
   "metadata": {},
   "source": [
    "Remove the model from $SCRATCH if you would like to start from epoch 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m torch.distributed.run --nproc-per-node=4 cnn_part4_mnist_torchrun.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e315261",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "\n",
    "Modify `cnn_part4_simple_linear_regression_parallel.py` to be able to use torchrun. \n",
    "\n",
    "Copy the file, modify it, and test if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m torch.distributed.run --nproc-per-node=4 <YOUR_FILE_HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea86fe9",
   "metadata": {},
   "source": [
    "## DesignSafe Classifier \n",
    "### Reused code from Part 1 and 2 \n",
    "Below are a set of functions and import statements that can be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to our data.\n",
    "# The datasets transformations are the same as the ones from part 2 of this tutorial.\n",
    "def load_datasets(train_path, val_path, test_path):\n",
    "    val_img_transform = transforms.Compose([transforms.Resize((244,244)),\n",
    "                                             transforms.ToTensor()])\n",
    "    train_img_transform = transforms.Compose([transforms.AutoAugment(),\n",
    "                                               transforms.Resize((244,244)),\n",
    "                                               transforms.ToTensor()])\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform=train_img_transform)\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=val_img_transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform=val_img_transform) if test_path is not None else None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Building the Neural Network\n",
    "def getResNet():\n",
    "    resnet = models.resnet34(weights='IMAGENET1K_V1')\n",
    "\n",
    "    # Fix the conv layers parameters\n",
    "    for conv_param in resnet.parameters():\n",
    "        conv_param.require_grad = False\n",
    "\n",
    "    # get the input dimension for this layer\n",
    "    num_ftrs = resnet.fc.in_features\n",
    "\n",
    "    # build the new final mlp layers of network\n",
    "    fc = nn.Sequential(\n",
    "          nn.Linear(num_ftrs, num_ftrs),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(num_ftrs, 3)\n",
    "        )\n",
    "   \n",
    "    # replace final fully connected layer\n",
    "    resnet.fc = fc\n",
    "    return resnet\n",
    "\n",
    "# Model evaluation.\n",
    "@torch.no_grad()\n",
    "def eval_model(data_loader, model, loss_fn, DEVICE):\n",
    "    model.train(False)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0.0, 0.0\n",
    "    n = len(data_loader)\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        x,y = data\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        pred = model(x)\n",
    "        loss += loss_fn(pred, y)/len(x)\n",
    "        pred_label = torch.argmax(pred, axis = 1)\n",
    "        accuracy += torch.sum(pred_label == y)/len(x)\n",
    "\n",
    "    return loss/n, accuracy/n\n",
    "\n",
    "# loading checkpoint\n",
    "def load_checkpoint(checkpoint_path, DEVICE):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    return checkpoint\n",
    "\n",
    "def load_model_fm_checkpoint(checkpoint, primitive_model):\n",
    "    primitive_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return primitive_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d665d11",
   "metadata": {},
   "source": [
    "### Setup Process Group (1 and 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f64ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_distributed():\n",
    "    '''\n",
    "    set up process group with torchrun's environment variables\n",
    "    '''\n",
    "    # 1, 6 use os to get rank and world size\n",
    "    dist_url = \"env://\"\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    dist.init_process_group(backend=\"nccl\", # \"nccl\" for using GPUs, \"gloo\" for using CPUs\n",
    "                          init_method=dist_url,\n",
    "                          world_size=world_size,\n",
    "                          rank=local_rank)\n",
    "    torch.cuda.set_device(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc61024",
   "metadata": {},
   "source": [
    "### Create Data DistributedSampler (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12044b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataloaders(train_set, val_set, test_set, batch_size, shuffle=True):\n",
    "    ##########################################################################################\n",
    "    # 2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
    "\n",
    "    # create distributedsampler for train, validation and test sets\n",
    "    train_sampler = DistributedSampler(dataset=train_set,shuffle=shuffle)\n",
    "    val_sampler = DistributedSampler(dataset=val_set, shuffle=False)\n",
    "    test_sampler = DistributedSampler(dataset=test_set, shuffle=False) if test_set is not None else None\n",
    "\n",
    "    # pass distributedsampler for train, validation and test sets into DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,sampler=train_sampler,num_workers=4,pin_memory=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set,batch_size=batch_size,sampler=val_sampler,num_workers=4)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_aset, batch_size, sampler=test_sampler,num_workers=4) if test_set is not None else None\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518ab26",
   "metadata": {},
   "source": [
    "### Write Checkpoints periodically during training and only from one device (4, 7C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, opt, scheduler, loss_fn, epochs, DEVICE, checkpoint_file, prev_best_val_acc):\n",
    "    n = len(train_loader)\n",
    "\n",
    "    best_val_acc = torch.tensor(0.0).to(DEVICE) if prev_best_val_acc is None else prev_best_val_acc\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train(True)\n",
    "\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        avg_loss, val_loss, val_acc, avg_acc  = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred,y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            avg_loss += loss.item()/len(x)\n",
    "            pred_label = torch.argmax(pred, axis=1)\n",
    "            avg_acc += torch.sum(pred_label == y)/len(x)\n",
    "\n",
    "        val_loss, val_acc = eval_model(val_loader, model, loss_fn, DEVICE)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        total_time = torch.tensor((end_time-start_time).seconds).to(DEVICE)\n",
    "\n",
    "        # Learning rate reducer takes action\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        avg_loss, avg_acc = avg_loss/n, avg_acc/n\n",
    "\n",
    "        ###############################################################################\n",
    "        # 4. Modify Training Loop to write model from one GPU     #####################\n",
    "        # 7C. Write checkpoints periodically throughout training. #####################\n",
    "        local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        # Only machine rank==0 (master machine) saves the model and prints the metrics    \n",
    "        if local_rank == 0:\n",
    "\n",
    "          # Save the best model that has the highest val accuracy\n",
    "            if val_acc.item() > best_val_acc.item():\n",
    "                print(f'lr for this epoch is {scheduler.get_last_lr()}')\n",
    "                print(f\"\\nPrev Best Val Acc: {best_val_acc} < Cur Val Acc: {val_acc}\")\n",
    "                print(\"Saving the new best model...\")\n",
    "                torch.save({\n",
    "                    'epoch':epoch,\n",
    "                    'machine':local_rank,\n",
    "                    'model_state_dict':model.module.state_dict(),\n",
    "                    'accuracy':val_acc,\n",
    "                    'loss':val_loss\n",
    "                }, checkpoint_file)\n",
    "                best_val_acc = val_acc\n",
    "                print(\"Finished saving model\\n\")\n",
    "\n",
    "            # Print the metrics (should be same on all machines)\n",
    "            print(f\"\\n(Epoch {epoch+1}/{epochs}) Time: {total_time}s\")\n",
    "            print(f\"(Epoch {epoch+1}/{epochs}) Average train loss: {avg_loss}, Average train accuracy: {avg_acc}\")\n",
    "            print(f\"(Epoch {epoch+1}/{epochs}) Val loss: {val_loss}, Val accuracy: {val_acc}\")\n",
    "            print(f\"(Epoch {epoch+1}/{epochs}) Current best val acc: {best_val_acc}\\n\")\n",
    "        ###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f95fb",
   "metadata": {},
   "source": [
    "### Create Clean Up Function (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    print(\"Cleaning up the distributed environment...\")\n",
    "    dist.destroy_process_group()\n",
    "    print(\"Distributed environment has been properly closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4a96c",
   "metadata": {},
   "source": [
    "### Wrap Model with DDP and put everything together in main function (3, 6B, 7A, 7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    hp = {\"lr\":1e-4, \"batch_size\":16, \"epochs\":5}\n",
    "    train_path = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Train/\")\n",
    "    val_path   = os.path.join(os.environ['SCRATCH'], \"Dataset_2/Validation/\")\n",
    "    test_path  = None\n",
    "\n",
    "    #################################################\n",
    "    # 6B. Use pytorch's enviornment variables.  #####\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    #################################################\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "    ###########################################################\n",
    "    # 7A. create location to store checkpoints if they did not exist. ##\n",
    "    \n",
    "    model_folder_path = os.path.join(os.environ['SCRATCH'], \"cnn4_damagelevel_output_model\") \n",
    "    os.makedirs(model_folder_path,exist_ok=True)\n",
    "    ###########################################################\n",
    "   \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).to(DEVICE)\n",
    "    train_set, val_set, test_set = load_datasets(train_path, val_path, test_path)\n",
    "    train_dataloader, val_dataloader, test_dataloader = construct_dataloaders(train_set, val_set, test_set, hp[\"batch_size\"], True)\n",
    "\n",
    "    model = getResNet().to(DEVICE)\n",
    "    \n",
    "    \n",
    "    ######################################################################################\n",
    "    # 7B, Read check point if it exists and pass to the train function to resume training##\n",
    "    prev_best_val_acc = None\n",
    "    checkpoint_file = os.path.join(model_folder_path, \"best_model.pt\")\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        prev_best_val_acc = checkpoint['accuracy']\n",
    "        model = load_model_fm_checkpoint(checkpoint,model)\n",
    "        epoch_start = checkpoint['epoch']\n",
    "        if rank == 0:\n",
    "            print(f\"resuming training from epoch {epoch_start}\")\n",
    "        else:\n",
    "            epoch_start = 0\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    ##########################################################################\n",
    "    # 3. Wrap model with DDP #################################################\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "    ##########################################################################\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=hp[\"lr\"])\n",
    "\n",
    "\n",
    "\n",
    "    # same learning rate scheduler as part 2\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',factor=0.1, patience=5, min_lr=1e-8)\n",
    "\n",
    "    train(train_dataloader, val_dataloader, model, opt, scheduler, loss_fn, hp[\"epochs\"], DEVICE, checkpoint_file, prev_best_val_acc)\n",
    "\n",
    "    # only the node with rank 0 does the loading, evaluation and printing to avoild duplicate \n",
    "    if local_rank == 0:\n",
    "        # store and print info on the best model at the end of training\n",
    "        primitive_model = getResNet().to(DEVICE)\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        best_model = load_model_fm_checkpoint(checkpoint,primitive_model)\n",
    "        loss, acc = eval_model(val_dataloader,best_model,loss_fn,DEVICE)\n",
    "        print(f\"\\nBest model (val loss: {loss}, val accuracy: {acc}) has been saved to {checkpoint_file}\\n\")\n",
    "        ###############################\n",
    "        # 5. close process group ######\n",
    "        cleanup()\n",
    "        ###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5846857",
   "metadata": {},
   "source": [
    "Download the dataset to `$SCRATCH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861757c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /work2/10000/zw427/data.tar.gz $SCRATCH\n",
    "! tar zxf $SCRATCH/data.tar.gz -C $SCRATCH\n",
    "! ls $SCRATCH/Dataset_2\n",
    "! rm $SCRATCH/data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66a08a",
   "metadata": {},
   "source": [
    "Launch the job with torchrun to train the designsafe classifier on a single node and 3 GPUs. Remove the model from $SCRATCH if you would like to start from epoch 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m torch.distributed.run --nproc-per-node=4 cnn_part4_torch_train_distributed.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
